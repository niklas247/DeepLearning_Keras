{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Regression Model in Keras by Niklas"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Import Pandas and Numpy"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Import the dataset and show the first rows"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  "
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data = pd.read_csv('https://cocl.us/concrete_data')\nconcrete_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Install Keras and Tensorflow"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting keras==2.2.5\n  Downloading Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 336 kB 11.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (1.0.8)\nRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (5.3.1)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (1.5.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (1.15.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (1.18.5)\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (2.10.0)\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras==2.2.5) (1.1.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.2.5\nCollecting tensorflow==1.15.0\n  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 412.3 MB 34 kB/s s eta 0:00:01\n\u001b[?25hCollecting tensorflow-estimator==1.15.1\n  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 503 kB 52.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.18.5)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.15.0)\nRequirement already satisfied: keras-applications>=1.0.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.0.8)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.8.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.1.0)\nCollecting tensorboard<1.16.0,>=1.15.0\n  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8 MB 52.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.34.2)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.9.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.12.1)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.27.2)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (1.1.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (3.1.0)\nRequirement already satisfied: gast==0.2.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (0.2.2)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15.0) (3.12.3)\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (47.3.1.post20200622)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.1)\nInstalling collected packages: tensorflow-estimator, tensorboard, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.1.0\n    Uninstalling tensorflow-estimator-2.1.0:\n      Successfully uninstalled tensorflow-estimator-2.1.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.1.0\n    Uninstalling tensorboard-2.1.0:\n      Successfully uninstalled tensorboard-2.1.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.1.0\n    Uninstalling tensorflow-2.1.0:\n      Successfully uninstalled tensorflow-2.1.0\nSuccessfully installed tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
                }
            ],
            "source": "!pip install keras==2.2.5 \n!pip install tensorflow==1.15.0"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Import Keras\n\nImport Keras and related models, layers, and utils"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Separate dataset\n\nDivide data into predictor variables and the target (Strength of concrete) and show the first five lines"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "concrete_data_columns = concrete_data.columns\n\npredictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\ntarget = concrete_data['Strength'] # Strength column"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  \n0            1040.0           676.0   28  \n1            1055.0           676.0   28  \n2             932.0           594.0  270  \n3             932.0           594.0  365  \n4             978.4           825.5  360  "
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "predictors.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Normalize the data\n\nNormalize the data to make sure the influence of the predictor variables is balanced"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>0.862735</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>1.055651</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.790075</td>\n      <td>0.678079</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.070492</td>\n      <td>0.647569</td>\n      <td>4.976069</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n\n   Coarse Aggregate  Fine Aggregate       Age  \n0          0.862735       -1.217079 -0.279597  \n1          1.055651       -1.217079 -0.279597  \n2         -0.526262       -2.239829  3.551340  \n3         -0.526262       -2.239829  5.055221  \n4          0.070492        0.647569  4.976069  "
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "predictors_norm = (predictors - predictors.mean()) / predictors.std()\npredictors_norm.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "n_cols = predictors_norm.shape[1] # number of predictors"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Import train_test_split from sklearn\n\nImporting the split to divide the dataset into a train and test set with respective predictor and target variables. The split is set on 30% being test-data - assigned randomly."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.30, random_state=42)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Regression Model\n\nDefinition of the regression model with the desired characteristics:\n- One Hidden Layer with density of 10 and the ReLu activation function\n- Optimizer = Adam, loss is defined with the MSE"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "# define regression model\ndef regression_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Building the model\n\nBuilding the model and already setting up a list of scores which will be used to collect the MSE of the individual iterations."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n"
                }
            ],
            "source": "# build the model\nmodel = regression_model()\nscores = []"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Running the iterations\n\nRunning 50 iterations which 50 epochs each and attach the respective MSE to the scores list."
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.3591 - val_loss: 41.2534\nEpoch 2/50\n - 0s - loss: 33.3875 - val_loss: 41.7188\nEpoch 3/50\n - 0s - loss: 33.5204 - val_loss: 41.6458\nEpoch 4/50\n - 0s - loss: 33.3876 - val_loss: 41.1034\nEpoch 5/50\n - 0s - loss: 33.4988 - val_loss: 41.2547\nEpoch 6/50\n - 0s - loss: 33.4877 - val_loss: 41.3652\nEpoch 7/50\n - 0s - loss: 33.4449 - val_loss: 41.3060\nEpoch 8/50\n - 0s - loss: 33.3606 - val_loss: 41.1836\nEpoch 9/50\n - 0s - loss: 33.3988 - val_loss: 41.4504\nEpoch 10/50\n - 0s - loss: 33.3871 - val_loss: 41.7829\nEpoch 11/50\n - 0s - loss: 33.4966 - val_loss: 41.5259\nEpoch 12/50\n - 0s - loss: 33.4081 - val_loss: 41.4354\nEpoch 13/50\n - 0s - loss: 33.4226 - val_loss: 41.0454\nEpoch 14/50\n - 0s - loss: 33.4060 - val_loss: 41.2279\nEpoch 15/50\n - 0s - loss: 33.4498 - val_loss: 41.4806\nEpoch 16/50\n - 0s - loss: 33.4978 - val_loss: 41.6822\nEpoch 17/50\n - 0s - loss: 33.4102 - val_loss: 41.5073\nEpoch 18/50\n - 0s - loss: 33.3629 - val_loss: 41.3001\nEpoch 19/50\n - 0s - loss: 33.3865 - val_loss: 41.3821\nEpoch 20/50\n - 0s - loss: 33.3831 - val_loss: 41.3552\nEpoch 21/50\n - 0s - loss: 33.4881 - val_loss: 41.3079\nEpoch 22/50\n - 0s - loss: 33.4015 - val_loss: 41.2273\nEpoch 23/50\n - 0s - loss: 33.3682 - val_loss: 41.5795\nEpoch 24/50\n - 0s - loss: 33.3736 - val_loss: 41.5413\nEpoch 25/50\n - 0s - loss: 33.5450 - val_loss: 41.6622\nEpoch 26/50\n - 0s - loss: 33.3655 - val_loss: 41.1779\nEpoch 27/50\n - 0s - loss: 33.3747 - val_loss: 41.3491\nEpoch 28/50\n - 0s - loss: 33.4103 - val_loss: 41.5780\nEpoch 29/50\n - 0s - loss: 33.3461 - val_loss: 41.6027\nEpoch 30/50\n - 0s - loss: 33.3703 - val_loss: 41.3337\nEpoch 31/50\n - 0s - loss: 33.3948 - val_loss: 41.2752\nEpoch 32/50\n - 0s - loss: 33.3670 - val_loss: 41.2361\nEpoch 33/50\n - 0s - loss: 33.4086 - val_loss: 41.5841\nEpoch 34/50\n - 0s - loss: 33.5694 - val_loss: 41.5488\nEpoch 35/50\n - 0s - loss: 33.3712 - val_loss: 41.3429\nEpoch 36/50\n - 0s - loss: 33.3949 - val_loss: 41.3414\nEpoch 37/50\n - 0s - loss: 33.3909 - val_loss: 41.3815\nEpoch 38/50\n - 0s - loss: 33.5696 - val_loss: 41.5996\nEpoch 39/50\n - 0s - loss: 33.3845 - val_loss: 41.2976\nEpoch 40/50\n - 0s - loss: 33.3900 - val_loss: 41.2791\nEpoch 41/50\n - 0s - loss: 33.4417 - val_loss: 41.2285\nEpoch 42/50\n - 0s - loss: 33.2977 - val_loss: 41.4089\nEpoch 43/50\n - 0s - loss: 33.3560 - val_loss: 41.5481\nEpoch 44/50\n - 0s - loss: 33.4288 - val_loss: 41.2626\nEpoch 45/50\n - 0s - loss: 33.4718 - val_loss: 41.8222\nEpoch 46/50\n - 0s - loss: 33.4927 - val_loss: 41.2884\nEpoch 47/50\n - 0s - loss: 33.3188 - val_loss: 41.3834\nEpoch 48/50\n - 0s - loss: 33.5172 - val_loss: 41.5325\nEpoch 49/50\n - 0s - loss: 33.5784 - val_loss: 41.3603\nEpoch 50/50\n - 0s - loss: 33.4527 - val_loss: 41.4776\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.3374 - val_loss: 41.4078\nEpoch 2/50\n - 0s - loss: 33.3976 - val_loss: 41.3832\nEpoch 3/50\n - 0s - loss: 33.3615 - val_loss: 41.6211\nEpoch 4/50\n - 0s - loss: 33.3347 - val_loss: 41.4841\nEpoch 5/50\n - 0s - loss: 33.3701 - val_loss: 41.3029\nEpoch 6/50\n - 0s - loss: 33.3892 - val_loss: 41.3812\nEpoch 7/50\n - 0s - loss: 33.4199 - val_loss: 41.3807\nEpoch 8/50\n - 0s - loss: 33.3405 - val_loss: 41.2787\nEpoch 9/50\n - 0s - loss: 33.4360 - val_loss: 41.1478\nEpoch 10/50\n - 0s - loss: 33.4957 - val_loss: 41.2731\nEpoch 11/50\n - 0s - loss: 33.3775 - val_loss: 41.5989\nEpoch 12/50\n - 0s - loss: 33.5050 - val_loss: 41.2082\nEpoch 13/50\n - 0s - loss: 33.3757 - val_loss: 41.3332\nEpoch 14/50\n - 0s - loss: 33.3238 - val_loss: 41.5302\nEpoch 15/50\n - 0s - loss: 33.3587 - val_loss: 41.4367\nEpoch 16/50\n - 0s - loss: 33.4137 - val_loss: 41.4035\nEpoch 17/50\n - 0s - loss: 33.3596 - val_loss: 41.4546\nEpoch 18/50\n - 0s - loss: 33.3398 - val_loss: 41.3531\nEpoch 19/50\n - 0s - loss: 33.3299 - val_loss: 41.5139\nEpoch 20/50\n - 0s - loss: 33.4193 - val_loss: 41.5624\nEpoch 21/50\n - 0s - loss: 33.4255 - val_loss: 41.3924\nEpoch 22/50\n - 0s - loss: 33.3751 - val_loss: 41.2213\nEpoch 23/50\n - 0s - loss: 33.3153 - val_loss: 41.5867\nEpoch 24/50\n - 0s - loss: 33.3497 - val_loss: 41.4946\nEpoch 25/50\n - 0s - loss: 33.3551 - val_loss: 41.3772\nEpoch 26/50\n - 0s - loss: 33.4278 - val_loss: 41.1679\nEpoch 27/50\n - 0s - loss: 33.3198 - val_loss: 41.5325\nEpoch 28/50\n - 0s - loss: 33.3345 - val_loss: 41.3406\nEpoch 29/50\n - 0s - loss: 33.3401 - val_loss: 41.2568\nEpoch 30/50\n - 0s - loss: 33.5481 - val_loss: 41.2591\nEpoch 31/50\n - 0s - loss: 33.2630 - val_loss: 41.6878\nEpoch 32/50\n - 0s - loss: 33.4452 - val_loss: 41.5792\nEpoch 33/50\n - 0s - loss: 33.4112 - val_loss: 41.1299\nEpoch 34/50\n - 0s - loss: 33.3865 - val_loss: 41.7376\nEpoch 35/50\n - 0s - loss: 33.4047 - val_loss: 41.2245\nEpoch 36/50\n - 0s - loss: 33.4869 - val_loss: 41.2300\nEpoch 37/50\n - 0s - loss: 33.4158 - val_loss: 41.4815\nEpoch 38/50\n - 0s - loss: 33.3464 - val_loss: 41.3713\nEpoch 39/50\n - 0s - loss: 33.3584 - val_loss: 41.4298\nEpoch 40/50\n - 0s - loss: 33.3631 - val_loss: 41.1247\nEpoch 41/50\n - 0s - loss: 33.3353 - val_loss: 41.2529\nEpoch 42/50\n - 0s - loss: 33.4248 - val_loss: 41.1277\nEpoch 43/50\n - 0s - loss: 33.3108 - val_loss: 41.4390\nEpoch 44/50\n - 0s - loss: 33.3847 - val_loss: 41.7148\nEpoch 45/50\n - 0s - loss: 33.3013 - val_loss: 41.4320\nEpoch 46/50\n - 0s - loss: 33.3830 - val_loss: 41.1871\nEpoch 47/50\n - 0s - loss: 33.4205 - val_loss: 41.1620\nEpoch 48/50\n - 0s - loss: 33.3239 - val_loss: 41.4211\nEpoch 49/50\n - 0s - loss: 33.3271 - val_loss: 41.5189\nEpoch 50/50\n - 0s - loss: 33.3135 - val_loss: 41.4516\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.3403 - val_loss: 41.1826\nEpoch 2/50\n - 0s - loss: 33.3818 - val_loss: 41.3144\nEpoch 3/50\n - 0s - loss: 33.3473 - val_loss: 41.1550\nEpoch 4/50\n - 0s - loss: 33.3738 - val_loss: 41.5179\nEpoch 5/50\n - 0s - loss: 33.4017 - val_loss: 41.1953\nEpoch 6/50\n - 0s - loss: 33.3611 - val_loss: 41.6451\nEpoch 7/50\n - 0s - loss: 33.4248 - val_loss: 41.6373\nEpoch 8/50\n - 0s - loss: 33.5578 - val_loss: 41.1149\nEpoch 9/50\n - 0s - loss: 33.3595 - val_loss: 41.5000\nEpoch 10/50\n - 0s - loss: 33.4225 - val_loss: 41.3940\nEpoch 11/50\n - 0s - loss: 33.3881 - val_loss: 40.9887\nEpoch 12/50\n - 0s - loss: 33.3365 - val_loss: 41.4290\nEpoch 13/50\n - 0s - loss: 33.5885 - val_loss: 41.9442\nEpoch 14/50\n - 0s - loss: 33.3978 - val_loss: 41.2865\nEpoch 15/50\n - 0s - loss: 33.3415 - val_loss: 41.2400\nEpoch 16/50\n - 0s - loss: 33.3681 - val_loss: 41.5758\nEpoch 17/50\n - 0s - loss: 33.3375 - val_loss: 41.4307\nEpoch 18/50\n - 0s - loss: 33.4172 - val_loss: 41.5833\nEpoch 19/50\n - 0s - loss: 33.3070 - val_loss: 41.3243\nEpoch 20/50\n - 0s - loss: 33.3840 - val_loss: 41.2945\nEpoch 21/50\n - 0s - loss: 33.4117 - val_loss: 41.6085\nEpoch 22/50\n - 0s - loss: 33.3056 - val_loss: 41.1643\nEpoch 23/50\n - 0s - loss: 33.3386 - val_loss: 41.3300\nEpoch 24/50\n - 0s - loss: 33.3180 - val_loss: 41.5023\nEpoch 25/50\n - 0s - loss: 33.3541 - val_loss: 41.4596\nEpoch 26/50\n - 0s - loss: 33.4447 - val_loss: 41.3251\nEpoch 27/50\n - 0s - loss: 33.3229 - val_loss: 41.6312\nEpoch 28/50\n - 0s - loss: 33.3750 - val_loss: 41.4631\nEpoch 29/50\n - 0s - loss: 33.4501 - val_loss: 41.6286\nEpoch 30/50\n - 0s - loss: 33.3242 - val_loss: 41.6155\nEpoch 31/50\n - 0s - loss: 33.3626 - val_loss: 41.2011\nEpoch 32/50\n - 0s - loss: 33.3657 - val_loss: 41.3780\nEpoch 33/50\n - 0s - loss: 33.3147 - val_loss: 41.4323\nEpoch 34/50\n - 0s - loss: 33.4314 - val_loss: 41.6251\nEpoch 35/50\n - 0s - loss: 33.3285 - val_loss: 41.2855\nEpoch 36/50\n - 0s - loss: 33.3549 - val_loss: 41.5122\nEpoch 37/50\n - 0s - loss: 33.3319 - val_loss: 41.5800\nEpoch 38/50\n - 0s - loss: 33.3562 - val_loss: 41.4577\nEpoch 39/50\n - 0s - loss: 33.4461 - val_loss: 41.3535\nEpoch 40/50\n - 0s - loss: 33.3570 - val_loss: 41.1769\nEpoch 41/50\n - 0s - loss: 33.4526 - val_loss: 41.2627\nEpoch 42/50\n - 0s - loss: 33.3620 - val_loss: 41.4665\nEpoch 43/50\n - 0s - loss: 33.4598 - val_loss: 41.9302\nEpoch 44/50\n - 0s - loss: 33.2632 - val_loss: 41.2988\nEpoch 45/50\n - 0s - loss: 33.4081 - val_loss: 41.4593\nEpoch 46/50\n - 0s - loss: 33.3845 - val_loss: 41.3032\nEpoch 47/50\n - 0s - loss: 33.4274 - val_loss: 41.4135\nEpoch 48/50\n - 0s - loss: 33.4399 - val_loss: 41.2793\nEpoch 49/50\n - 0s - loss: 33.4602 - val_loss: 41.4496\nEpoch 50/50\n - 0s - loss: 33.3295 - val_loss: 41.5057\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.4252 - val_loss: 41.4269\nEpoch 2/50\n - 0s - loss: 33.5239 - val_loss: 41.4642\nEpoch 3/50\n - 0s - loss: 33.2935 - val_loss: 41.4356\nEpoch 4/50\n - 0s - loss: 33.3062 - val_loss: 41.6265\nEpoch 5/50\n - 0s - loss: 33.3268 - val_loss: 41.3154\nEpoch 6/50\n - 0s - loss: 33.3241 - val_loss: 41.3419\nEpoch 7/50\n - 0s - loss: 33.3347 - val_loss: 41.5886\nEpoch 8/50\n - 0s - loss: 33.3308 - val_loss: 41.4302\nEpoch 9/50\n - 0s - loss: 33.3875 - val_loss: 41.6370\nEpoch 10/50\n - 0s - loss: 33.2731 - val_loss: 41.2756\nEpoch 11/50\n - 0s - loss: 33.2967 - val_loss: 41.4334\nEpoch 12/50\n - 0s - loss: 33.3239 - val_loss: 41.3251\nEpoch 13/50\n - 0s - loss: 33.2848 - val_loss: 41.2006\nEpoch 14/50\n - 0s - loss: 33.3348 - val_loss: 41.5716\nEpoch 15/50\n - 0s - loss: 33.3116 - val_loss: 41.5837\nEpoch 16/50\n - 0s - loss: 33.3998 - val_loss: 41.3707\nEpoch 17/50\n - 0s - loss: 33.2783 - val_loss: 41.4041\nEpoch 18/50\n - 0s - loss: 33.4098 - val_loss: 41.5268\nEpoch 19/50\n - 0s - loss: 33.4628 - val_loss: 41.3824\nEpoch 20/50\n - 0s - loss: 33.4319 - val_loss: 41.4580\nEpoch 21/50\n - 0s - loss: 33.4441 - val_loss: 41.3866\nEpoch 22/50\n - 0s - loss: 33.2735 - val_loss: 41.5812\nEpoch 23/50\n - 0s - loss: 33.3856 - val_loss: 41.3946\nEpoch 24/50\n - 0s - loss: 33.3297 - val_loss: 41.6172\nEpoch 25/50\n - 0s - loss: 33.3481 - val_loss: 41.2353\nEpoch 26/50\n - 0s - loss: 33.3452 - val_loss: 41.4382\nEpoch 27/50\n - 0s - loss: 33.3663 - val_loss: 41.3900\nEpoch 28/50\n - 0s - loss: 33.3307 - val_loss: 41.6190\nEpoch 29/50\n - 0s - loss: 33.4013 - val_loss: 41.4427\nEpoch 30/50\n - 0s - loss: 33.3289 - val_loss: 41.1614\nEpoch 31/50\n - 0s - loss: 33.2764 - val_loss: 41.4097\nEpoch 32/50\n - 0s - loss: 33.4455 - val_loss: 41.2837\nEpoch 33/50\n - 0s - loss: 33.3828 - val_loss: 41.3938\nEpoch 34/50\n - 0s - loss: 33.2944 - val_loss: 41.3784\nEpoch 35/50\n - 0s - loss: 33.2987 - val_loss: 41.5038\nEpoch 36/50\n - 0s - loss: 33.4033 - val_loss: 41.5141\nEpoch 37/50\n - 0s - loss: 33.2743 - val_loss: 41.5304\nEpoch 38/50\n - 0s - loss: 33.4079 - val_loss: 41.2958\nEpoch 39/50\n - 0s - loss: 33.3527 - val_loss: 41.1215\nEpoch 40/50\n - 0s - loss: 33.3460 - val_loss: 41.7073\nEpoch 41/50\n - 0s - loss: 33.3085 - val_loss: 41.4223\nEpoch 42/50\n - 0s - loss: 33.4007 - val_loss: 41.3729\nEpoch 43/50\n - 0s - loss: 33.3043 - val_loss: 41.7565\nEpoch 44/50\n - 0s - loss: 33.2765 - val_loss: 41.4114\nEpoch 45/50\n - 0s - loss: 33.3357 - val_loss: 41.1277\nEpoch 46/50\n - 0s - loss: 33.2766 - val_loss: 41.4163\nEpoch 47/50\n - 0s - loss: 33.3361 - val_loss: 41.5249\nEpoch 48/50\n - 0s - loss: 33.3303 - val_loss: 41.3778\nEpoch 49/50\n - 0s - loss: 33.2672 - val_loss: 41.3268\nEpoch 50/50\n - 0s - loss: 33.3821 - val_loss: 41.4758\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.3215 - val_loss: 41.4953\nEpoch 2/50\n - 0s - loss: 33.2737 - val_loss: 41.4333\nEpoch 3/50\n - 0s - loss: 33.3676 - val_loss: 41.2696\nEpoch 4/50\n - 0s - loss: 33.2672 - val_loss: 41.2974\nEpoch 5/50\n - 0s - loss: 33.2614 - val_loss: 41.4749\nEpoch 6/50\n - 0s - loss: 33.2617 - val_loss: 41.5330\nEpoch 7/50\n - 0s - loss: 33.3983 - val_loss: 41.2060\nEpoch 8/50\n - 0s - loss: 33.3116 - val_loss: 41.5809\nEpoch 9/50\n - 0s - loss: 33.3390 - val_loss: 41.6153\nEpoch 10/50\n - 0s - loss: 33.2076 - val_loss: 41.2688\nEpoch 11/50\n - 0s - loss: 33.3513 - val_loss: 41.4426\nEpoch 12/50\n - 0s - loss: 33.3114 - val_loss: 41.7026\nEpoch 13/50\n - 0s - loss: 33.2869 - val_loss: 41.4139\nEpoch 14/50\n - 0s - loss: 33.2946 - val_loss: 41.3286\nEpoch 15/50\n - 0s - loss: 33.3087 - val_loss: 41.3263\nEpoch 16/50\n - 0s - loss: 33.3227 - val_loss: 41.5181\nEpoch 17/50\n - 0s - loss: 33.2772 - val_loss: 41.2743\nEpoch 18/50\n - 0s - loss: 33.3178 - val_loss: 41.1841\nEpoch 19/50\n - 0s - loss: 33.2403 - val_loss: 41.6196\nEpoch 20/50\n - 0s - loss: 33.2817 - val_loss: 41.5885\nEpoch 21/50\n - 0s - loss: 33.2702 - val_loss: 41.4929\nEpoch 22/50\n - 0s - loss: 33.3588 - val_loss: 41.2584\nEpoch 23/50\n - 0s - loss: 33.4233 - val_loss: 41.6714\nEpoch 24/50\n - 0s - loss: 33.3110 - val_loss: 41.2984\nEpoch 25/50\n - 0s - loss: 33.3185 - val_loss: 41.4968\nEpoch 26/50\n - 0s - loss: 33.3985 - val_loss: 41.1874\nEpoch 27/50\n - 0s - loss: 33.3090 - val_loss: 41.4834\nEpoch 28/50\n - 0s - loss: 33.3468 - val_loss: 41.5441\nEpoch 29/50\n - 0s - loss: 33.3623 - val_loss: 41.3104\nEpoch 30/50\n - 0s - loss: 33.3192 - val_loss: 41.3220\nEpoch 31/50\n - 0s - loss: 33.2758 - val_loss: 41.3862\nEpoch 32/50\n - 0s - loss: 33.5670 - val_loss: 41.8707\nEpoch 33/50\n - 0s - loss: 33.3594 - val_loss: 41.1561\nEpoch 34/50\n - 0s - loss: 33.4011 - val_loss: 41.3036\nEpoch 35/50\n - 0s - loss: 33.2607 - val_loss: 41.3462\nEpoch 36/50\n - 0s - loss: 33.3205 - val_loss: 41.2557\nEpoch 37/50\n - 0s - loss: 33.3568 - val_loss: 41.3882\nEpoch 38/50\n - 0s - loss: 33.2361 - val_loss: 41.5846\nEpoch 39/50\n - 0s - loss: 33.4623 - val_loss: 41.3151\nEpoch 40/50\n - 0s - loss: 33.3226 - val_loss: 41.7431\nEpoch 41/50\n - 0s - loss: 33.2415 - val_loss: 41.5164\nEpoch 42/50\n - 0s - loss: 33.2748 - val_loss: 41.3236\nEpoch 43/50\n - 0s - loss: 33.2319 - val_loss: 41.2954\nEpoch 44/50\n - 0s - loss: 33.2816 - val_loss: 41.3230\nEpoch 45/50\n - 0s - loss: 33.2297 - val_loss: 41.4430\nEpoch 46/50\n - 0s - loss: 33.3645 - val_loss: 41.4401\nEpoch 47/50\n - 0s - loss: 33.3022 - val_loss: 41.3971\nEpoch 48/50\n - 0s - loss: 33.2967 - val_loss: 41.6221\nEpoch 49/50\n - 0s - loss: 33.2962 - val_loss: 41.3015\nEpoch 50/50\n - 0s - loss: 33.2922 - val_loss: 41.4402\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.2424 - val_loss: 41.3285\nEpoch 2/50\n - 0s - loss: 33.2449 - val_loss: 41.3545\nEpoch 3/50\n - 0s - loss: 33.3094 - val_loss: 41.3693\nEpoch 4/50\n - 0s - loss: 33.4008 - val_loss: 41.3501\nEpoch 5/50\n - 0s - loss: 33.2902 - val_loss: 41.4387\nEpoch 6/50\n - 0s - loss: 33.2445 - val_loss: 41.6442\nEpoch 7/50\n - 0s - loss: 33.2729 - val_loss: 41.6290\nEpoch 8/50\n - 0s - loss: 33.3005 - val_loss: 41.4151\nEpoch 9/50\n - 0s - loss: 33.3265 - val_loss: 41.3058\nEpoch 10/50\n - 0s - loss: 33.2591 - val_loss: 41.3177\nEpoch 11/50\n - 0s - loss: 33.2900 - val_loss: 41.5751\nEpoch 12/50\n - 0s - loss: 33.2643 - val_loss: 41.3231\nEpoch 13/50\n - 0s - loss: 33.3021 - val_loss: 41.4917\nEpoch 14/50\n - 0s - loss: 33.2978 - val_loss: 41.5070\nEpoch 15/50\n - 0s - loss: 33.3401 - val_loss: 41.2223\nEpoch 16/50\n - 0s - loss: 33.2806 - val_loss: 41.4001\nEpoch 17/50\n - 0s - loss: 33.3092 - val_loss: 41.4317\nEpoch 18/50\n - 0s - loss: 33.3410 - val_loss: 41.6143\nEpoch 19/50\n - 0s - loss: 33.2281 - val_loss: 41.3687\nEpoch 20/50\n - 0s - loss: 33.2301 - val_loss: 41.2377\nEpoch 21/50\n - 0s - loss: 33.2594 - val_loss: 41.3332\nEpoch 22/50\n - 0s - loss: 33.3655 - val_loss: 41.3053\nEpoch 23/50\n - 0s - loss: 33.2701 - val_loss: 41.5756\nEpoch 24/50\n - 0s - loss: 33.2701 - val_loss: 41.3033\nEpoch 25/50\n - 0s - loss: 33.1987 - val_loss: 41.4343\nEpoch 26/50\n - 0s - loss: 33.2814 - val_loss: 41.3081\nEpoch 27/50\n - 0s - loss: 33.4432 - val_loss: 41.6865\nEpoch 28/50\n - 0s - loss: 33.2700 - val_loss: 41.2550\nEpoch 29/50\n - 0s - loss: 33.2379 - val_loss: 41.4213\nEpoch 30/50\n - 0s - loss: 33.2416 - val_loss: 41.4867\nEpoch 31/50\n - 0s - loss: 33.2629 - val_loss: 41.2932\nEpoch 32/50\n - 0s - loss: 33.2761 - val_loss: 41.2557\nEpoch 33/50\n - 0s - loss: 33.2498 - val_loss: 41.2480\nEpoch 34/50\n - 0s - loss: 33.3130 - val_loss: 41.3508\nEpoch 35/50\n - 0s - loss: 33.2772 - val_loss: 41.3073\nEpoch 36/50\n - 0s - loss: 33.2887 - val_loss: 41.6618\nEpoch 37/50\n - 0s - loss: 33.2611 - val_loss: 41.4002\nEpoch 38/50\n - 0s - loss: 33.3221 - val_loss: 41.5018\nEpoch 39/50\n - 0s - loss: 33.4310 - val_loss: 41.5170\nEpoch 40/50\n - 0s - loss: 33.3262 - val_loss: 41.5472\nEpoch 41/50\n - 0s - loss: 33.2892 - val_loss: 41.4298\nEpoch 42/50\n - 0s - loss: 33.2972 - val_loss: 41.2173\nEpoch 43/50\n - 0s - loss: 33.3267 - val_loss: 41.3596\nEpoch 44/50\n - 0s - loss: 33.2698 - val_loss: 41.6778\nEpoch 45/50\n - 0s - loss: 33.2559 - val_loss: 41.5179\nEpoch 46/50\n - 0s - loss: 33.2654 - val_loss: 41.3429\nEpoch 47/50\n - 0s - loss: 33.3009 - val_loss: 41.6255\nEpoch 48/50\n - 0s - loss: 33.2575 - val_loss: 41.3872\nEpoch 49/50\n - 0s - loss: 33.4563 - val_loss: 41.2617\nEpoch 50/50\n - 0s - loss: 33.2187 - val_loss: 41.6803\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.2797 - val_loss: 41.4187\nEpoch 2/50\n - 0s - loss: 33.3162 - val_loss: 41.2852\nEpoch 3/50\n - 0s - loss: 33.2796 - val_loss: 41.3081\nEpoch 4/50\n - 0s - loss: 33.2508 - val_loss: 41.2089\nEpoch 5/50\n - 0s - loss: 33.4251 - val_loss: 41.8031\nEpoch 6/50\n - 0s - loss: 33.1640 - val_loss: 41.2103\nEpoch 7/50\n - 0s - loss: 33.2424 - val_loss: 41.3139\nEpoch 8/50\n - 0s - loss: 33.2885 - val_loss: 41.4410\nEpoch 9/50\n - 0s - loss: 33.2931 - val_loss: 41.5223\nEpoch 10/50\n - 0s - loss: 33.3171 - val_loss: 41.1742\nEpoch 11/50\n - 0s - loss: 33.2599 - val_loss: 41.3794\nEpoch 12/50\n - 0s - loss: 33.3329 - val_loss: 41.3441\nEpoch 13/50\n - 0s - loss: 33.2553 - val_loss: 41.3458\nEpoch 14/50\n - 0s - loss: 33.3396 - val_loss: 41.5783\nEpoch 15/50\n - 0s - loss: 33.2355 - val_loss: 41.2370\nEpoch 16/50\n - 0s - loss: 33.2213 - val_loss: 41.2054\nEpoch 17/50\n - 0s - loss: 33.2238 - val_loss: 41.4546\nEpoch 18/50\n - 0s - loss: 33.3322 - val_loss: 41.5620\nEpoch 19/50\n - 0s - loss: 33.2475 - val_loss: 41.6081\nEpoch 20/50\n - 0s - loss: 33.2560 - val_loss: 41.4013\nEpoch 21/50\n - 0s - loss: 33.2743 - val_loss: 41.3629\nEpoch 22/50\n - 0s - loss: 33.2111 - val_loss: 41.5427\nEpoch 23/50\n - 0s - loss: 33.2489 - val_loss: 41.3750\nEpoch 24/50\n - 0s - loss: 33.2545 - val_loss: 41.1863\nEpoch 25/50\n - 0s - loss: 33.2360 - val_loss: 41.6631\nEpoch 26/50\n - 0s - loss: 33.2130 - val_loss: 41.4725\nEpoch 27/50\n - 0s - loss: 33.2349 - val_loss: 41.3006\nEpoch 28/50\n - 0s - loss: 33.2301 - val_loss: 41.1268\nEpoch 29/50\n - 0s - loss: 33.2763 - val_loss: 41.4399\nEpoch 30/50\n - 0s - loss: 33.2208 - val_loss: 41.7569\nEpoch 31/50\n - 0s - loss: 33.2586 - val_loss: 41.5722\nEpoch 32/50\n - 0s - loss: 33.1873 - val_loss: 41.5331\nEpoch 33/50\n - 0s - loss: 33.2679 - val_loss: 41.1775\nEpoch 34/50\n - 0s - loss: 33.2509 - val_loss: 41.3606\nEpoch 35/50\n - 0s - loss: 33.2608 - val_loss: 41.6524\nEpoch 36/50\n - 0s - loss: 33.2693 - val_loss: 41.4203\nEpoch 37/50\n - 0s - loss: 33.2759 - val_loss: 41.5445\nEpoch 38/50\n - 0s - loss: 33.2524 - val_loss: 41.2685\nEpoch 39/50\n - 0s - loss: 33.2715 - val_loss: 41.5228\nEpoch 40/50\n - 0s - loss: 33.3132 - val_loss: 41.2175\nEpoch 41/50\n - 0s - loss: 33.2931 - val_loss: 41.6787\nEpoch 42/50\n - 0s - loss: 33.3855 - val_loss: 41.1499\nEpoch 43/50\n - 0s - loss: 33.2667 - val_loss: 41.3836\nEpoch 44/50\n - 0s - loss: 33.2745 - val_loss: 41.5929\nEpoch 45/50\n - 0s - loss: 33.2417 - val_loss: 41.3716\nEpoch 46/50\n - 0s - loss: 33.2786 - val_loss: 41.2820\nEpoch 47/50\n - 0s - loss: 33.2282 - val_loss: 41.3135\nEpoch 48/50\n - 0s - loss: 33.2512 - val_loss: 41.4539\nEpoch 49/50\n - 0s - loss: 33.2201 - val_loss: 41.2961\nEpoch 50/50\n - 0s - loss: 33.1903 - val_loss: 41.4171\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1958 - val_loss: 41.5191\nEpoch 2/50\n - 0s - loss: 33.2027 - val_loss: 41.4203\nEpoch 3/50\n - 0s - loss: 33.2734 - val_loss: 41.2491\nEpoch 4/50\n - 0s - loss: 33.2696 - val_loss: 41.6875\nEpoch 5/50\n - 0s - loss: 33.1912 - val_loss: 41.2340\nEpoch 6/50\n - 0s - loss: 33.3977 - val_loss: 41.0618\nEpoch 7/50\n - 0s - loss: 33.2237 - val_loss: 41.4764\nEpoch 8/50\n - 0s - loss: 33.2854 - val_loss: 41.3594\nEpoch 9/50\n - 0s - loss: 33.2521 - val_loss: 41.3329\nEpoch 10/50\n - 0s - loss: 33.2484 - val_loss: 41.6863\nEpoch 11/50\n - 0s - loss: 33.3341 - val_loss: 41.1464\nEpoch 12/50\n - 0s - loss: 33.2228 - val_loss: 41.6718\nEpoch 13/50\n - 0s - loss: 33.2446 - val_loss: 41.1247\nEpoch 14/50\n - 0s - loss: 33.2510 - val_loss: 41.7098\nEpoch 15/50\n - 0s - loss: 33.2148 - val_loss: 41.2844\nEpoch 16/50\n - 0s - loss: 33.3821 - val_loss: 41.5925\nEpoch 17/50\n - 0s - loss: 33.1658 - val_loss: 41.3276\nEpoch 18/50\n - 0s - loss: 33.2061 - val_loss: 41.2904\nEpoch 19/50\n - 0s - loss: 33.2164 - val_loss: 41.3768\nEpoch 20/50\n - 0s - loss: 33.2688 - val_loss: 41.3019\nEpoch 21/50\n - 0s - loss: 33.2751 - val_loss: 41.3159\nEpoch 22/50\n - 0s - loss: 33.2070 - val_loss: 41.1943\nEpoch 23/50\n - 0s - loss: 33.2559 - val_loss: 41.5758\nEpoch 24/50\n - 0s - loss: 33.2034 - val_loss: 41.3006\nEpoch 25/50\n - 0s - loss: 33.2484 - val_loss: 41.2472\nEpoch 26/50\n - 0s - loss: 33.4018 - val_loss: 41.3960\nEpoch 27/50\n - 0s - loss: 33.2114 - val_loss: 41.3478\nEpoch 28/50\n - 0s - loss: 33.2431 - val_loss: 41.1872\nEpoch 29/50\n - 0s - loss: 33.2538 - val_loss: 41.4171\nEpoch 30/50\n - 0s - loss: 33.2285 - val_loss: 41.1804\nEpoch 31/50\n - 0s - loss: 33.2654 - val_loss: 41.2981\nEpoch 32/50\n - 0s - loss: 33.1411 - val_loss: 41.4650\nEpoch 33/50\n - 0s - loss: 33.2224 - val_loss: 41.4309\nEpoch 34/50\n - 0s - loss: 33.2215 - val_loss: 41.4246\nEpoch 35/50\n - 0s - loss: 33.2661 - val_loss: 41.1476\nEpoch 36/50\n - 0s - loss: 33.2532 - val_loss: 41.1051\nEpoch 37/50\n - 0s - loss: 33.1968 - val_loss: 41.3993\nEpoch 38/50\n - 0s - loss: 33.2759 - val_loss: 41.4259\nEpoch 39/50\n - 0s - loss: 33.1782 - val_loss: 41.2119\nEpoch 40/50\n - 0s - loss: 33.1945 - val_loss: 41.2384\nEpoch 41/50\n - 0s - loss: 33.2783 - val_loss: 41.2776\nEpoch 42/50\n - 0s - loss: 33.1820 - val_loss: 41.4422\nEpoch 43/50\n - 0s - loss: 33.2257 - val_loss: 41.4186\nEpoch 44/50\n - 0s - loss: 33.2705 - val_loss: 41.1847\nEpoch 45/50\n - 0s - loss: 33.3818 - val_loss: 41.7119\nEpoch 46/50\n - 0s - loss: 33.2139 - val_loss: 41.3409\nEpoch 47/50\n - 0s - loss: 33.2615 - val_loss: 41.0593\nEpoch 48/50\n - 0s - loss: 33.3258 - val_loss: 41.2294\nEpoch 49/50\n - 0s - loss: 33.2176 - val_loss: 41.0588\nEpoch 50/50\n - 0s - loss: 33.2922 - val_loss: 41.4380\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1872 - val_loss: 41.1249\nEpoch 2/50\n - 0s - loss: 33.3841 - val_loss: 41.3629\nEpoch 3/50\n - 0s - loss: 33.3201 - val_loss: 41.0714\nEpoch 4/50\n - 0s - loss: 33.2529 - val_loss: 41.5385\nEpoch 5/50\n - 0s - loss: 33.2031 - val_loss: 41.0835\nEpoch 6/50\n - 0s - loss: 33.2937 - val_loss: 41.2828\nEpoch 7/50\n - 0s - loss: 33.1734 - val_loss: 41.5168\nEpoch 8/50\n - 0s - loss: 33.2507 - val_loss: 41.2577\nEpoch 9/50\n - 0s - loss: 33.3275 - val_loss: 41.7778\nEpoch 10/50\n - 0s - loss: 33.1544 - val_loss: 41.0983\nEpoch 11/50\n - 0s - loss: 33.2711 - val_loss: 41.3501\nEpoch 12/50\n - 0s - loss: 33.2483 - val_loss: 41.2661\nEpoch 13/50\n - 0s - loss: 33.2163 - val_loss: 41.2532\nEpoch 14/50\n - 0s - loss: 33.1532 - val_loss: 41.4614\nEpoch 15/50\n - 0s - loss: 33.2168 - val_loss: 41.1141\nEpoch 16/50\n - 0s - loss: 33.3048 - val_loss: 41.4298\nEpoch 17/50\n - 0s - loss: 33.2039 - val_loss: 41.3509\nEpoch 18/50\n - 0s - loss: 33.2356 - val_loss: 41.3261\nEpoch 19/50\n - 0s - loss: 33.3162 - val_loss: 41.1625\nEpoch 20/50\n - 0s - loss: 33.2775 - val_loss: 41.4670\nEpoch 21/50\n - 0s - loss: 33.2677 - val_loss: 41.2503\nEpoch 22/50\n - 0s - loss: 33.1903 - val_loss: 41.3324\nEpoch 23/50\n - 0s - loss: 33.2167 - val_loss: 41.3008\nEpoch 24/50\n - 0s - loss: 33.2546 - val_loss: 41.4933\nEpoch 25/50\n - 0s - loss: 33.1839 - val_loss: 41.3719\nEpoch 26/50\n - 0s - loss: 33.2582 - val_loss: 41.1166\nEpoch 27/50\n - 0s - loss: 33.1441 - val_loss: 41.5406\nEpoch 28/50\n - 0s - loss: 33.3295 - val_loss: 41.4974\nEpoch 29/50\n - 0s - loss: 33.1800 - val_loss: 41.1589\nEpoch 30/50\n - 0s - loss: 33.1697 - val_loss: 41.1848\nEpoch 31/50\n - 0s - loss: 33.2592 - val_loss: 41.1285\nEpoch 32/50\n - 0s - loss: 33.2446 - val_loss: 41.6549\nEpoch 33/50\n - 0s - loss: 33.2980 - val_loss: 41.5432\nEpoch 34/50\n - 0s - loss: 33.1944 - val_loss: 41.4355\nEpoch 35/50\n - 0s - loss: 33.1733 - val_loss: 41.4218\nEpoch 36/50\n - 0s - loss: 33.3242 - val_loss: 41.6594\nEpoch 37/50\n - 0s - loss: 33.2442 - val_loss: 41.0920\nEpoch 38/50\n - 0s - loss: 33.3079 - val_loss: 41.2487\nEpoch 39/50\n - 0s - loss: 33.1844 - val_loss: 41.1684\nEpoch 40/50\n - 0s - loss: 33.1908 - val_loss: 41.1484\nEpoch 41/50\n - 0s - loss: 33.1800 - val_loss: 41.3032\nEpoch 42/50\n - 0s - loss: 33.2380 - val_loss: 41.6268\nEpoch 43/50\n - 0s - loss: 33.2253 - val_loss: 41.2990\nEpoch 44/50\n - 0s - loss: 33.1542 - val_loss: 41.2639\nEpoch 45/50\n - 0s - loss: 33.2450 - val_loss: 41.3849\nEpoch 46/50\n - 0s - loss: 33.2846 - val_loss: 41.1870\nEpoch 47/50\n - 0s - loss: 33.1786 - val_loss: 41.3187\nEpoch 48/50\n - 0s - loss: 33.2106 - val_loss: 41.1676\nEpoch 49/50\n - 0s - loss: 33.2036 - val_loss: 41.4135\nEpoch 50/50\n - 0s - loss: 33.1663 - val_loss: 41.6325\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.3315 - val_loss: 41.2678\nEpoch 2/50\n - 0s - loss: 33.2859 - val_loss: 41.2551\nEpoch 3/50\n - 0s - loss: 33.2440 - val_loss: 41.1870\nEpoch 4/50\n - 0s - loss: 33.2365 - val_loss: 41.5968\nEpoch 5/50\n - 0s - loss: 33.1505 - val_loss: 41.3803\nEpoch 6/50\n - 0s - loss: 33.2492 - val_loss: 41.0555\nEpoch 7/50\n - 0s - loss: 33.1557 - val_loss: 41.4072\nEpoch 8/50\n - 0s - loss: 33.2501 - val_loss: 41.3974\nEpoch 9/50\n - 0s - loss: 33.2730 - val_loss: 41.1181\nEpoch 10/50\n - 0s - loss: 33.1850 - val_loss: 41.3794\nEpoch 11/50\n - 0s - loss: 33.1660 - val_loss: 41.3384\nEpoch 12/50\n - 0s - loss: 33.1846 - val_loss: 41.3656\nEpoch 13/50\n - 0s - loss: 33.1400 - val_loss: 41.3793\nEpoch 14/50\n - 0s - loss: 33.2481 - val_loss: 41.4266\nEpoch 15/50\n - 0s - loss: 33.2613 - val_loss: 41.3194\nEpoch 16/50\n - 0s - loss: 33.1829 - val_loss: 41.2741\nEpoch 17/50\n - 0s - loss: 33.2249 - val_loss: 41.4621\nEpoch 18/50\n - 0s - loss: 33.1890 - val_loss: 41.1789\nEpoch 19/50\n - 0s - loss: 33.2148 - val_loss: 41.1370\nEpoch 20/50\n - 0s - loss: 33.2397 - val_loss: 41.3368\nEpoch 21/50\n - 0s - loss: 33.1731 - val_loss: 41.2330\nEpoch 22/50\n - 0s - loss: 33.1500 - val_loss: 41.3985\nEpoch 23/50\n - 0s - loss: 33.2082 - val_loss: 41.2611\nEpoch 24/50\n - 0s - loss: 33.1769 - val_loss: 41.1871\nEpoch 25/50\n - 0s - loss: 33.2275 - val_loss: 41.2893\nEpoch 26/50\n - 0s - loss: 33.3287 - val_loss: 41.4032\nEpoch 27/50\n - 0s - loss: 33.3081 - val_loss: 41.3135\nEpoch 28/50\n - 0s - loss: 33.1746 - val_loss: 41.1707\nEpoch 29/50\n - 0s - loss: 33.2172 - val_loss: 41.3125\nEpoch 30/50\n - 0s - loss: 33.2121 - val_loss: 41.2609\nEpoch 31/50\n - 0s - loss: 33.1327 - val_loss: 41.2862\nEpoch 32/50\n - 0s - loss: 33.1344 - val_loss: 41.5632\nEpoch 33/50\n - 0s - loss: 33.2194 - val_loss: 41.0631\nEpoch 34/50\n - 0s - loss: 33.2056 - val_loss: 41.5031\nEpoch 35/50\n - 0s - loss: 33.2185 - val_loss: 41.4996\nEpoch 36/50\n - 0s - loss: 33.2205 - val_loss: 41.3907\nEpoch 37/50\n - 0s - loss: 33.2776 - val_loss: 41.1851\nEpoch 38/50\n - 0s - loss: 33.1390 - val_loss: 41.1779\nEpoch 39/50\n - 0s - loss: 33.1433 - val_loss: 41.2193\nEpoch 40/50\n - 0s - loss: 33.1872 - val_loss: 41.3208\nEpoch 41/50\n - 0s - loss: 33.1621 - val_loss: 41.3905\nEpoch 42/50\n - 0s - loss: 33.2391 - val_loss: 41.2177\nEpoch 43/50\n - 0s - loss: 33.1718 - val_loss: 41.1316\nEpoch 44/50\n - 0s - loss: 33.1691 - val_loss: 41.1103\nEpoch 45/50\n - 0s - loss: 33.2017 - val_loss: 41.2919\nEpoch 46/50\n - 0s - loss: 33.1939 - val_loss: 41.2220\nEpoch 47/50\n - 0s - loss: 33.1604 - val_loss: 41.3813\nEpoch 48/50\n - 0s - loss: 33.2947 - val_loss: 41.5706\nEpoch 49/50\n - 0s - loss: 33.1979 - val_loss: 41.4177\nEpoch 50/50\n - 0s - loss: 33.1554 - val_loss: 41.2625\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1865 - val_loss: 41.2564\nEpoch 2/50\n - 0s - loss: 33.2377 - val_loss: 41.3926\nEpoch 3/50\n - 0s - loss: 33.2008 - val_loss: 41.1395\nEpoch 4/50\n - 0s - loss: 33.2694 - val_loss: 41.3346\nEpoch 5/50\n - 0s - loss: 33.1799 - val_loss: 41.1971\nEpoch 6/50\n - 0s - loss: 33.1479 - val_loss: 41.3789\nEpoch 7/50\n - 0s - loss: 33.1822 - val_loss: 41.2812\nEpoch 8/50\n - 0s - loss: 33.2956 - val_loss: 41.5197\nEpoch 9/50\n - 0s - loss: 33.1272 - val_loss: 41.0372\nEpoch 10/50\n - 0s - loss: 33.1748 - val_loss: 41.2114\nEpoch 11/50\n - 0s - loss: 33.3447 - val_loss: 41.3784\nEpoch 12/50\n - 0s - loss: 33.1853 - val_loss: 41.1797\nEpoch 13/50\n - 0s - loss: 33.1800 - val_loss: 41.2859\nEpoch 14/50\n - 0s - loss: 33.1523 - val_loss: 41.5113\nEpoch 15/50\n - 0s - loss: 33.1787 - val_loss: 41.2698\nEpoch 16/50\n - 0s - loss: 33.1320 - val_loss: 41.1976\nEpoch 17/50\n - 0s - loss: 33.2337 - val_loss: 41.4984\nEpoch 18/50\n - 0s - loss: 33.2135 - val_loss: 41.5476\nEpoch 19/50\n - 0s - loss: 33.1011 - val_loss: 41.0561\nEpoch 20/50\n - 0s - loss: 33.1802 - val_loss: 41.2709\nEpoch 21/50\n - 0s - loss: 33.1408 - val_loss: 41.1505\nEpoch 22/50\n - 0s - loss: 33.2217 - val_loss: 41.6055\nEpoch 23/50\n - 0s - loss: 33.1819 - val_loss: 41.3677\nEpoch 24/50\n - 0s - loss: 33.1826 - val_loss: 41.4487\nEpoch 25/50\n - 0s - loss: 33.1673 - val_loss: 41.1781\nEpoch 26/50\n - 0s - loss: 33.2748 - val_loss: 41.5035\nEpoch 27/50\n - 0s - loss: 33.4567 - val_loss: 41.0716\nEpoch 28/50\n - 0s - loss: 33.1618 - val_loss: 41.6126\nEpoch 29/50\n - 0s - loss: 33.1512 - val_loss: 41.3414\nEpoch 30/50\n - 0s - loss: 33.2036 - val_loss: 41.1016\nEpoch 31/50\n - 0s - loss: 33.1241 - val_loss: 41.3055\nEpoch 32/50\n - 0s - loss: 33.1917 - val_loss: 41.5914\nEpoch 33/50\n - 0s - loss: 33.1119 - val_loss: 41.1204\nEpoch 34/50\n - 0s - loss: 33.1695 - val_loss: 41.3176\nEpoch 35/50\n - 0s - loss: 33.1326 - val_loss: 41.4610\nEpoch 36/50\n - 0s - loss: 33.1790 - val_loss: 41.0511\nEpoch 37/50\n - 0s - loss: 33.3353 - val_loss: 41.6644\nEpoch 38/50\n - 0s - loss: 33.4473 - val_loss: 41.0049\nEpoch 39/50\n - 0s - loss: 33.3799 - val_loss: 41.6345\nEpoch 40/50\n - 0s - loss: 33.1336 - val_loss: 41.3596\nEpoch 41/50\n - 0s - loss: 33.1885 - val_loss: 41.1468\nEpoch 42/50\n - 0s - loss: 33.1178 - val_loss: 41.4756\nEpoch 43/50\n - 0s - loss: 33.1636 - val_loss: 41.2314\nEpoch 44/50\n - 0s - loss: 33.2363 - val_loss: 41.2251\nEpoch 45/50\n - 0s - loss: 33.1861 - val_loss: 41.2561\nEpoch 46/50\n - 0s - loss: 33.2478 - val_loss: 41.6006\nEpoch 47/50\n - 0s - loss: 33.2328 - val_loss: 40.9911\nEpoch 48/50\n - 0s - loss: 33.2443 - val_loss: 41.5913\nEpoch 49/50\n - 0s - loss: 33.2162 - val_loss: 40.9877\nEpoch 50/50\n - 0s - loss: 33.1456 - val_loss: 41.1897\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1692 - val_loss: 41.5690\nEpoch 2/50\n - 0s - loss: 33.1183 - val_loss: 41.0456\nEpoch 3/50\n - 0s - loss: 33.1522 - val_loss: 41.4553\nEpoch 4/50\n - 0s - loss: 33.1341 - val_loss: 41.3096\nEpoch 5/50\n - 0s - loss: 33.1096 - val_loss: 41.5008\nEpoch 6/50\n - 0s - loss: 33.2459 - val_loss: 41.1050\nEpoch 7/50\n - 0s - loss: 33.1533 - val_loss: 41.6896\nEpoch 8/50\n - 0s - loss: 33.3953 - val_loss: 41.2191\nEpoch 9/50\n - 0s - loss: 33.0805 - val_loss: 41.5197\nEpoch 10/50\n - 0s - loss: 33.2477 - val_loss: 41.6707\nEpoch 11/50\n - 0s - loss: 33.1735 - val_loss: 41.2545\nEpoch 12/50\n - 0s - loss: 33.1230 - val_loss: 41.2765\nEpoch 13/50\n - 0s - loss: 33.1334 - val_loss: 41.5736\nEpoch 14/50\n - 0s - loss: 33.0745 - val_loss: 41.1183\nEpoch 15/50\n - 0s - loss: 33.1282 - val_loss: 41.3866\nEpoch 16/50\n - 0s - loss: 33.1525 - val_loss: 41.4306\nEpoch 17/50\n - 0s - loss: 33.1334 - val_loss: 41.4445\nEpoch 18/50\n - 0s - loss: 33.2076 - val_loss: 41.2192\nEpoch 19/50\n - 0s - loss: 33.1228 - val_loss: 41.0607\nEpoch 20/50\n - 0s - loss: 33.1031 - val_loss: 41.4282\nEpoch 21/50\n - 0s - loss: 33.2708 - val_loss: 41.2695\nEpoch 22/50\n - 0s - loss: 33.1768 - val_loss: 41.4974\nEpoch 23/50\n - 0s - loss: 33.1178 - val_loss: 41.3157\nEpoch 24/50\n - 0s - loss: 33.1322 - val_loss: 41.4790\nEpoch 25/50\n - 0s - loss: 33.1483 - val_loss: 41.3302\nEpoch 26/50\n - 0s - loss: 33.2039 - val_loss: 41.2436\nEpoch 27/50\n - 0s - loss: 33.1340 - val_loss: 41.4892\nEpoch 28/50\n - 0s - loss: 33.1141 - val_loss: 41.3073\nEpoch 29/50\n - 0s - loss: 33.1697 - val_loss: 41.0243\nEpoch 30/50\n - 0s - loss: 33.1937 - val_loss: 41.1625\nEpoch 31/50\n - 0s - loss: 33.1184 - val_loss: 41.5969\nEpoch 32/50\n - 0s - loss: 33.1337 - val_loss: 41.3890\nEpoch 33/50\n - 0s - loss: 33.1644 - val_loss: 41.2411\nEpoch 34/50\n - 0s - loss: 33.1776 - val_loss: 41.1767\nEpoch 35/50\n - 0s - loss: 33.1654 - val_loss: 41.6350\nEpoch 36/50\n - 0s - loss: 33.1268 - val_loss: 40.9917\nEpoch 37/50\n - 0s - loss: 33.0684 - val_loss: 41.1680\nEpoch 38/50\n - 0s - loss: 33.1938 - val_loss: 41.5390\nEpoch 39/50\n - 0s - loss: 33.1033 - val_loss: 41.1470\nEpoch 40/50\n - 0s - loss: 33.1817 - val_loss: 41.1848\nEpoch 41/50\n - 0s - loss: 33.1066 - val_loss: 41.2549\nEpoch 42/50\n - 0s - loss: 33.1243 - val_loss: 41.3308\nEpoch 43/50\n - 0s - loss: 33.1450 - val_loss: 41.3631\nEpoch 44/50\n - 0s - loss: 33.1433 - val_loss: 41.1938\nEpoch 45/50\n - 0s - loss: 33.2297 - val_loss: 41.5549\nEpoch 46/50\n - 0s - loss: 33.1456 - val_loss: 41.4975\nEpoch 47/50\n - 0s - loss: 33.1113 - val_loss: 41.1856\nEpoch 48/50\n - 0s - loss: 33.1272 - val_loss: 41.1538\nEpoch 49/50\n - 0s - loss: 33.1415 - val_loss: 41.4097\nEpoch 50/50\n - 0s - loss: 33.1511 - val_loss: 41.4979\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1418 - val_loss: 41.3087\nEpoch 2/50\n - 0s - loss: 33.1217 - val_loss: 41.1654\nEpoch 3/50\n - 0s - loss: 33.1340 - val_loss: 41.2775\nEpoch 4/50\n - 0s - loss: 33.1201 - val_loss: 41.1060\nEpoch 5/50\n - 0s - loss: 33.1490 - val_loss: 41.1144\nEpoch 6/50\n - 0s - loss: 33.1337 - val_loss: 41.3081\nEpoch 7/50\n - 0s - loss: 33.1115 - val_loss: 41.2499\nEpoch 8/50\n - 0s - loss: 33.1128 - val_loss: 41.2053\nEpoch 9/50\n - 0s - loss: 33.0744 - val_loss: 41.2007\nEpoch 10/50\n - 0s - loss: 33.1521 - val_loss: 41.3792\nEpoch 11/50\n - 0s - loss: 33.2104 - val_loss: 41.4736\nEpoch 12/50\n - 0s - loss: 33.2551 - val_loss: 41.1460\nEpoch 13/50\n - 0s - loss: 33.1407 - val_loss: 41.1846\nEpoch 14/50\n - 0s - loss: 33.1520 - val_loss: 41.2063\nEpoch 15/50\n - 0s - loss: 33.1514 - val_loss: 41.6201\nEpoch 16/50\n - 0s - loss: 33.1342 - val_loss: 41.1690\nEpoch 17/50\n - 0s - loss: 33.1488 - val_loss: 41.3438\nEpoch 18/50\n - 0s - loss: 33.2763 - val_loss: 41.2533\nEpoch 19/50\n - 0s - loss: 33.2129 - val_loss: 41.4829\nEpoch 20/50\n - 0s - loss: 33.0846 - val_loss: 41.1270\nEpoch 21/50\n - 0s - loss: 33.0915 - val_loss: 41.0833\nEpoch 22/50\n - 0s - loss: 33.1352 - val_loss: 41.4492\nEpoch 23/50\n - 0s - loss: 33.1234 - val_loss: 41.2991\nEpoch 24/50\n - 0s - loss: 33.1339 - val_loss: 40.9546\nEpoch 25/50\n - 0s - loss: 33.1157 - val_loss: 41.1028\nEpoch 26/50\n - 0s - loss: 33.1056 - val_loss: 41.1706\nEpoch 27/50\n - 0s - loss: 33.1671 - val_loss: 41.5819\nEpoch 28/50\n - 0s - loss: 33.1431 - val_loss: 41.0600\nEpoch 29/50\n - 0s - loss: 33.0832 - val_loss: 41.0557\nEpoch 30/50\n - 0s - loss: 33.3771 - val_loss: 41.6884\nEpoch 31/50\n - 0s - loss: 33.0280 - val_loss: 41.3768\nEpoch 32/50\n - 0s - loss: 33.1596 - val_loss: 41.0728\nEpoch 33/50\n - 0s - loss: 33.1353 - val_loss: 41.0718\nEpoch 34/50\n - 0s - loss: 33.2452 - val_loss: 41.5041\nEpoch 35/50\n - 0s - loss: 33.1883 - val_loss: 41.3786\nEpoch 36/50\n - 0s - loss: 33.1962 - val_loss: 41.2866\nEpoch 37/50\n - 0s - loss: 33.1187 - val_loss: 41.3169\nEpoch 38/50\n - 0s - loss: 33.1377 - val_loss: 41.2414\nEpoch 39/50\n - 0s - loss: 33.1202 - val_loss: 41.2867\nEpoch 40/50\n - 0s - loss: 33.1582 - val_loss: 41.1976\nEpoch 41/50\n - 0s - loss: 33.0926 - val_loss: 41.4366\nEpoch 42/50\n - 0s - loss: 33.1979 - val_loss: 41.5886\nEpoch 43/50\n - 0s - loss: 33.1573 - val_loss: 41.4318\nEpoch 44/50\n - 0s - loss: 33.1848 - val_loss: 41.3162\nEpoch 45/50\n - 0s - loss: 33.1086 - val_loss: 41.1875\nEpoch 46/50\n - 0s - loss: 33.1518 - val_loss: 41.3418\nEpoch 47/50\n - 0s - loss: 33.0609 - val_loss: 41.1808\nEpoch 48/50\n - 0s - loss: 33.0654 - val_loss: 41.4345\nEpoch 49/50\n - 0s - loss: 33.1183 - val_loss: 41.2522\nEpoch 50/50\n - 0s - loss: 33.0505 - val_loss: 41.1711\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1678 - val_loss: 41.2031\nEpoch 2/50\n - 0s - loss: 33.0715 - val_loss: 41.3510\nEpoch 3/50\n - 0s - loss: 33.1061 - val_loss: 41.2931\nEpoch 4/50\n - 0s - loss: 33.1659 - val_loss: 41.4429\nEpoch 5/50\n - 0s - loss: 33.0821 - val_loss: 41.4726\nEpoch 6/50\n - 0s - loss: 33.1767 - val_loss: 41.1619\nEpoch 7/50\n - 0s - loss: 33.2412 - val_loss: 41.2768\nEpoch 8/50\n - 0s - loss: 33.0779 - val_loss: 41.4453\nEpoch 9/50\n - 0s - loss: 33.0971 - val_loss: 41.3348\nEpoch 10/50\n - 0s - loss: 33.0715 - val_loss: 41.4419\nEpoch 11/50\n - 0s - loss: 33.0760 - val_loss: 41.3876\nEpoch 12/50\n - 0s - loss: 33.0986 - val_loss: 41.1688\nEpoch 13/50\n - 0s - loss: 33.1117 - val_loss: 41.3644\nEpoch 14/50\n - 0s - loss: 33.1247 - val_loss: 41.1101\nEpoch 15/50\n - 0s - loss: 33.0878 - val_loss: 41.4214\nEpoch 16/50\n - 0s - loss: 33.1365 - val_loss: 41.3711\nEpoch 17/50\n - 0s - loss: 33.1147 - val_loss: 41.4218\nEpoch 18/50\n - 0s - loss: 33.2146 - val_loss: 41.2861\nEpoch 19/50\n - 0s - loss: 33.1054 - val_loss: 41.4072\nEpoch 20/50\n - 0s - loss: 33.0430 - val_loss: 41.2373\nEpoch 21/50\n - 0s - loss: 33.3415 - val_loss: 41.3745\nEpoch 22/50\n - 0s - loss: 33.0809 - val_loss: 41.4976\nEpoch 23/50\n - 0s - loss: 33.1220 - val_loss: 41.1924\nEpoch 24/50\n - 0s - loss: 33.2025 - val_loss: 41.3262\nEpoch 25/50\n - 0s - loss: 33.1158 - val_loss: 41.1649\nEpoch 26/50\n - 0s - loss: 33.1236 - val_loss: 41.3159\nEpoch 27/50\n - 0s - loss: 33.1337 - val_loss: 41.5032\nEpoch 28/50\n - 0s - loss: 33.2242 - val_loss: 41.2397\nEpoch 29/50\n - 0s - loss: 33.1215 - val_loss: 41.1958\nEpoch 30/50\n - 0s - loss: 33.1458 - val_loss: 41.4518\nEpoch 31/50\n - 0s - loss: 33.1043 - val_loss: 41.4841\nEpoch 32/50\n - 0s - loss: 33.1276 - val_loss: 41.1506\nEpoch 33/50\n - 0s - loss: 33.1376 - val_loss: 41.3815\nEpoch 34/50\n - 0s - loss: 33.1821 - val_loss: 41.0480\nEpoch 35/50\n - 0s - loss: 33.0403 - val_loss: 41.3880\nEpoch 36/50\n - 0s - loss: 33.0983 - val_loss: 41.3973\nEpoch 37/50\n - 0s - loss: 33.0513 - val_loss: 41.3255\nEpoch 38/50\n - 0s - loss: 33.1236 - val_loss: 41.4923\nEpoch 39/50\n - 0s - loss: 33.2991 - val_loss: 41.5860\nEpoch 40/50\n - 0s - loss: 33.2269 - val_loss: 41.4158\nEpoch 41/50\n - 0s - loss: 33.1919 - val_loss: 41.3323\nEpoch 42/50\n - 0s - loss: 33.0708 - val_loss: 41.3766\nEpoch 43/50\n - 0s - loss: 33.1124 - val_loss: 41.5907\nEpoch 44/50\n - 0s - loss: 33.0991 - val_loss: 41.2586\nEpoch 45/50\n - 0s - loss: 33.2001 - val_loss: 41.2364\nEpoch 46/50\n - 0s - loss: 33.0665 - val_loss: 41.4337\nEpoch 47/50\n - 0s - loss: 33.0816 - val_loss: 41.1777\nEpoch 48/50\n - 0s - loss: 33.0609 - val_loss: 41.4021\nEpoch 49/50\n - 0s - loss: 33.0661 - val_loss: 41.3174\nEpoch 50/50\n - 0s - loss: 33.2214 - val_loss: 41.3442\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.1584 - val_loss: 41.3257\nEpoch 2/50\n - 0s - loss: 33.0777 - val_loss: 41.4210\nEpoch 3/50\n - 0s - loss: 33.0882 - val_loss: 41.1939\nEpoch 4/50\n - 0s - loss: 33.4177 - val_loss: 41.1845\nEpoch 5/50\n - 0s - loss: 32.9713 - val_loss: 41.3987\nEpoch 6/50\n - 0s - loss: 33.0879 - val_loss: 41.3935\nEpoch 7/50\n - 0s - loss: 33.0147 - val_loss: 41.2718\nEpoch 8/50\n - 0s - loss: 33.0551 - val_loss: 40.9957\nEpoch 9/50\n - 0s - loss: 33.1718 - val_loss: 41.6735\nEpoch 10/50\n - 0s - loss: 33.0529 - val_loss: 41.1190\nEpoch 11/50\n - 0s - loss: 33.2077 - val_loss: 41.2433\nEpoch 12/50\n - 0s - loss: 33.0807 - val_loss: 41.3423\nEpoch 13/50\n - 0s - loss: 33.0243 - val_loss: 41.2687\nEpoch 14/50\n - 0s - loss: 33.1665 - val_loss: 40.9333\nEpoch 15/50\n - 0s - loss: 33.1683 - val_loss: 41.2880\nEpoch 16/50\n - 0s - loss: 33.0621 - val_loss: 41.3481\nEpoch 17/50\n - 0s - loss: 33.0828 - val_loss: 41.3209\nEpoch 18/50\n - 0s - loss: 33.1755 - val_loss: 41.2577\nEpoch 19/50\n - 0s - loss: 33.1209 - val_loss: 41.1446\nEpoch 20/50\n - 0s - loss: 33.0496 - val_loss: 41.2890\nEpoch 21/50\n - 0s - loss: 33.1042 - val_loss: 41.2205\nEpoch 22/50\n - 0s - loss: 33.1123 - val_loss: 41.5548\nEpoch 23/50\n - 0s - loss: 33.0276 - val_loss: 41.3237\nEpoch 24/50\n - 0s - loss: 33.0962 - val_loss: 41.2677\nEpoch 25/50\n - 0s - loss: 33.0516 - val_loss: 41.4424\nEpoch 26/50\n - 0s - loss: 33.0822 - val_loss: 41.3370\nEpoch 27/50\n - 0s - loss: 33.0901 - val_loss: 41.4951\nEpoch 28/50\n - 0s - loss: 33.1493 - val_loss: 41.0900\nEpoch 29/50\n - 0s - loss: 33.0464 - val_loss: 41.2329\nEpoch 30/50\n - 0s - loss: 33.1255 - val_loss: 41.5703\nEpoch 31/50\n - 0s - loss: 33.2554 - val_loss: 41.3986\nEpoch 32/50\n - 0s - loss: 33.0639 - val_loss: 41.3138\nEpoch 33/50\n - 0s - loss: 33.1457 - val_loss: 41.2467\nEpoch 34/50\n - 0s - loss: 33.0777 - val_loss: 41.4243\nEpoch 35/50\n - 0s - loss: 33.0457 - val_loss: 41.3748\nEpoch 36/50\n - 0s - loss: 33.1114 - val_loss: 41.0097\nEpoch 37/50\n - 0s - loss: 33.1329 - val_loss: 41.4376\nEpoch 38/50\n - 0s - loss: 33.0050 - val_loss: 41.3892\nEpoch 39/50\n - 0s - loss: 33.0426 - val_loss: 41.3527\nEpoch 40/50\n - 0s - loss: 33.1401 - val_loss: 41.3646\nEpoch 41/50\n - 0s - loss: 33.0945 - val_loss: 41.3778\nEpoch 42/50\n - 0s - loss: 33.0435 - val_loss: 41.2204\nEpoch 43/50\n - 0s - loss: 33.0918 - val_loss: 41.3832\nEpoch 44/50\n - 0s - loss: 33.0595 - val_loss: 41.4320\nEpoch 45/50\n - 0s - loss: 33.0753 - val_loss: 41.1908\nEpoch 46/50\n - 0s - loss: 33.0272 - val_loss: 41.4479\nEpoch 47/50\n - 0s - loss: 33.1187 - val_loss: 41.1867\nEpoch 48/50\n - 0s - loss: 33.0527 - val_loss: 41.4365\nEpoch 49/50\n - 0s - loss: 33.0524 - val_loss: 41.4634\nEpoch 50/50\n - 0s - loss: 33.1036 - val_loss: 41.4186\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.0535 - val_loss: 41.2521\nEpoch 2/50\n - 0s - loss: 33.0953 - val_loss: 41.2808\nEpoch 3/50\n - 0s - loss: 33.1128 - val_loss: 41.5077\nEpoch 4/50\n - 0s - loss: 33.0711 - val_loss: 41.2268\nEpoch 5/50\n - 0s - loss: 33.0215 - val_loss: 41.4706\nEpoch 6/50\n - 0s - loss: 33.0659 - val_loss: 41.5450\nEpoch 7/50\n - 0s - loss: 33.0691 - val_loss: 41.3123\nEpoch 8/50\n - 0s - loss: 33.0588 - val_loss: 41.4328\nEpoch 9/50\n - 0s - loss: 33.0937 - val_loss: 41.2161\nEpoch 10/50\n - 0s - loss: 33.0611 - val_loss: 41.1893\nEpoch 11/50\n - 0s - loss: 33.0891 - val_loss: 41.0215\nEpoch 12/50\n - 0s - loss: 33.0834 - val_loss: 41.3218\nEpoch 13/50\n - 0s - loss: 33.0251 - val_loss: 41.6861\nEpoch 14/50\n - 0s - loss: 33.2189 - val_loss: 41.2538\nEpoch 15/50\n - 0s - loss: 33.0253 - val_loss: 41.2139\nEpoch 16/50\n - 0s - loss: 33.0591 - val_loss: 41.4527\nEpoch 17/50\n - 0s - loss: 33.0991 - val_loss: 41.4208\nEpoch 18/50\n - 0s - loss: 33.0470 - val_loss: 41.2873\nEpoch 19/50\n - 0s - loss: 33.0570 - val_loss: 41.3531\nEpoch 20/50\n - 0s - loss: 33.0289 - val_loss: 41.3781\nEpoch 21/50\n - 0s - loss: 33.0573 - val_loss: 41.4215\nEpoch 22/50\n - 0s - loss: 33.0779 - val_loss: 41.4281\nEpoch 23/50\n - 0s - loss: 33.0193 - val_loss: 41.2521\nEpoch 24/50\n - 0s - loss: 33.0855 - val_loss: 41.3296\nEpoch 25/50\n - 0s - loss: 33.0660 - val_loss: 41.4125\nEpoch 26/50\n - 0s - loss: 33.0323 - val_loss: 41.3854\nEpoch 27/50\n - 0s - loss: 33.0627 - val_loss: 41.4295\nEpoch 28/50\n - 0s - loss: 33.0449 - val_loss: 41.3659\nEpoch 29/50\n - 0s - loss: 33.0555 - val_loss: 41.3177\nEpoch 30/50\n - 0s - loss: 33.0955 - val_loss: 41.2215\nEpoch 31/50\n - 0s - loss: 33.0110 - val_loss: 41.4620\nEpoch 32/50\n - 0s - loss: 33.1317 - val_loss: 41.1563\nEpoch 33/50\n - 0s - loss: 33.1324 - val_loss: 41.4526\nEpoch 34/50\n - 0s - loss: 33.1148 - val_loss: 41.1100\nEpoch 35/50\n - 0s - loss: 33.0307 - val_loss: 41.5027\nEpoch 36/50\n - 0s - loss: 33.0421 - val_loss: 41.4430\nEpoch 37/50\n - 0s - loss: 33.0444 - val_loss: 41.4477\nEpoch 38/50\n - 0s - loss: 33.0677 - val_loss: 41.2932\nEpoch 39/50\n - 0s - loss: 33.0929 - val_loss: 41.3748\nEpoch 40/50\n - 0s - loss: 33.1161 - val_loss: 41.2751\nEpoch 41/50\n - 0s - loss: 33.0498 - val_loss: 41.2354\nEpoch 42/50\n - 0s - loss: 33.0300 - val_loss: 41.2567\nEpoch 43/50\n - 0s - loss: 32.9969 - val_loss: 41.3277\nEpoch 44/50\n - 0s - loss: 33.0715 - val_loss: 41.4953\nEpoch 45/50\n - 0s - loss: 33.1609 - val_loss: 41.4904\nEpoch 46/50\n - 0s - loss: 32.9860 - val_loss: 41.5044\nEpoch 47/50\n - 0s - loss: 33.0556 - val_loss: 41.1397\nEpoch 48/50\n - 0s - loss: 33.0775 - val_loss: 41.1384\nEpoch 49/50\n - 0s - loss: 33.0383 - val_loss: 41.1657\nEpoch 50/50\n - 0s - loss: 33.0309 - val_loss: 41.3494\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.0065 - val_loss: 41.2201\nEpoch 2/50\n - 0s - loss: 33.0338 - val_loss: 41.3696\nEpoch 3/50\n - 0s - loss: 33.0736 - val_loss: 41.6285\nEpoch 4/50\n - 0s - loss: 33.0564 - val_loss: 41.2733\nEpoch 5/50\n - 0s - loss: 33.0392 - val_loss: 41.2569\nEpoch 6/50\n - 0s - loss: 33.0583 - val_loss: 41.1997\nEpoch 7/50\n - 0s - loss: 33.0253 - val_loss: 41.3195\nEpoch 8/50\n - 0s - loss: 33.0113 - val_loss: 41.3420\nEpoch 9/50\n - 0s - loss: 33.1437 - val_loss: 41.4635\nEpoch 10/50\n - 0s - loss: 33.0241 - val_loss: 41.4052\nEpoch 11/50\n - 0s - loss: 33.0463 - val_loss: 41.3012\nEpoch 12/50\n - 0s - loss: 33.0633 - val_loss: 41.4116\nEpoch 13/50\n - 0s - loss: 33.0586 - val_loss: 41.4430\nEpoch 14/50\n - 0s - loss: 33.0467 - val_loss: 41.2445\nEpoch 15/50\n - 0s - loss: 33.0887 - val_loss: 41.5996\nEpoch 16/50\n - 0s - loss: 33.0555 - val_loss: 41.1828\nEpoch 17/50\n - 0s - loss: 33.1121 - val_loss: 41.1508\nEpoch 18/50\n - 0s - loss: 32.9819 - val_loss: 41.1553\nEpoch 19/50\n - 0s - loss: 33.2780 - val_loss: 41.4286\nEpoch 20/50\n - 0s - loss: 33.1015 - val_loss: 41.1798\nEpoch 21/50\n - 0s - loss: 33.0064 - val_loss: 41.4573\nEpoch 22/50\n - 0s - loss: 32.9917 - val_loss: 41.4478\nEpoch 23/50\n - 0s - loss: 33.0434 - val_loss: 41.4697\nEpoch 24/50\n - 0s - loss: 32.9673 - val_loss: 41.2973\nEpoch 25/50\n - 0s - loss: 33.0233 - val_loss: 41.2650\nEpoch 26/50\n - 0s - loss: 33.0525 - val_loss: 41.1449\nEpoch 27/50\n - 0s - loss: 33.0254 - val_loss: 41.5909\nEpoch 28/50\n - 0s - loss: 33.0733 - val_loss: 41.2334\nEpoch 29/50\n - 0s - loss: 33.0537 - val_loss: 41.4881\nEpoch 30/50\n - 0s - loss: 33.0554 - val_loss: 41.2231\nEpoch 31/50\n - 0s - loss: 33.0733 - val_loss: 41.1575\nEpoch 32/50\n - 0s - loss: 33.0557 - val_loss: 41.4789\nEpoch 33/50\n - 0s - loss: 33.1270 - val_loss: 41.2658\nEpoch 34/50\n - 0s - loss: 33.0371 - val_loss: 41.5997\nEpoch 35/50\n - 0s - loss: 33.0316 - val_loss: 41.2187\nEpoch 36/50\n - 0s - loss: 33.0442 - val_loss: 41.4190\nEpoch 37/50\n - 0s - loss: 33.0803 - val_loss: 41.1922\nEpoch 38/50\n - 0s - loss: 33.0447 - val_loss: 41.3541\nEpoch 39/50\n - 0s - loss: 33.0507 - val_loss: 41.1807\nEpoch 40/50\n - 0s - loss: 33.0194 - val_loss: 41.2688\nEpoch 41/50\n - 0s - loss: 32.9718 - val_loss: 41.5157\nEpoch 42/50\n - 0s - loss: 33.0829 - val_loss: 41.3699\nEpoch 43/50\n - 0s - loss: 33.1186 - val_loss: 41.1503\nEpoch 44/50\n - 0s - loss: 33.1144 - val_loss: 41.5005\nEpoch 45/50\n - 0s - loss: 33.0147 - val_loss: 41.3963\nEpoch 46/50\n - 0s - loss: 33.0242 - val_loss: 41.2764\nEpoch 47/50\n - 0s - loss: 32.9883 - val_loss: 41.1082\nEpoch 48/50\n - 0s - loss: 33.0001 - val_loss: 41.2253\nEpoch 49/50\n - 0s - loss: 33.0308 - val_loss: 41.1880\nEpoch 50/50\n - 0s - loss: 33.1206 - val_loss: 41.6035\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.0513 - val_loss: 41.2783\nEpoch 2/50\n - 0s - loss: 33.0324 - val_loss: 40.9966\nEpoch 3/50\n - 0s - loss: 32.9768 - val_loss: 41.4561\nEpoch 4/50\n - 0s - loss: 33.0401 - val_loss: 41.4510\nEpoch 5/50\n - 0s - loss: 33.0204 - val_loss: 41.2532\nEpoch 6/50\n - 0s - loss: 33.0010 - val_loss: 41.5581\nEpoch 7/50\n - 0s - loss: 33.0217 - val_loss: 41.4214\nEpoch 8/50\n - 0s - loss: 33.0746 - val_loss: 41.1707\nEpoch 9/50\n - 0s - loss: 33.0818 - val_loss: 41.3206\nEpoch 10/50\n - 0s - loss: 33.0398 - val_loss: 41.5159\nEpoch 11/50\n - 0s - loss: 32.9945 - val_loss: 41.1667\nEpoch 12/50\n - 0s - loss: 33.0326 - val_loss: 41.5980\nEpoch 13/50\n - 0s - loss: 33.0115 - val_loss: 41.3062\nEpoch 14/50\n - 0s - loss: 32.9514 - val_loss: 41.4476\nEpoch 15/50\n - 0s - loss: 33.1141 - val_loss: 41.1747\nEpoch 16/50\n - 0s - loss: 33.1546 - val_loss: 41.6160\nEpoch 17/50\n - 0s - loss: 33.1070 - val_loss: 41.0332\nEpoch 18/50\n - 0s - loss: 33.0949 - val_loss: 41.3153\nEpoch 19/50\n - 0s - loss: 33.0487 - val_loss: 41.3778\nEpoch 20/50\n - 0s - loss: 33.1272 - val_loss: 41.6571\nEpoch 21/50\n - 0s - loss: 33.0634 - val_loss: 41.2637\nEpoch 22/50\n - 0s - loss: 33.0046 - val_loss: 41.3798\nEpoch 23/50\n - 0s - loss: 33.0948 - val_loss: 41.2561\nEpoch 24/50\n - 0s - loss: 33.0383 - val_loss: 41.2827\nEpoch 25/50\n - 0s - loss: 33.0371 - val_loss: 41.3218\nEpoch 26/50\n - 0s - loss: 33.0042 - val_loss: 41.1986\nEpoch 27/50\n - 0s - loss: 32.9818 - val_loss: 41.4214\nEpoch 28/50\n - 0s - loss: 32.9807 - val_loss: 41.4337\nEpoch 29/50\n - 0s - loss: 33.0528 - val_loss: 41.0693\nEpoch 30/50\n - 0s - loss: 33.0250 - val_loss: 41.4623\nEpoch 31/50\n - 0s - loss: 32.9757 - val_loss: 41.2798\nEpoch 32/50\n - 0s - loss: 33.0829 - val_loss: 41.3630\nEpoch 33/50\n - 0s - loss: 33.0216 - val_loss: 41.2786\nEpoch 34/50\n - 0s - loss: 32.9906 - val_loss: 41.2374\nEpoch 35/50\n - 0s - loss: 32.9826 - val_loss: 41.2887\nEpoch 36/50\n - 0s - loss: 33.0576 - val_loss: 41.2117\nEpoch 37/50\n - 0s - loss: 32.9942 - val_loss: 41.2520\nEpoch 38/50\n - 0s - loss: 33.0287 - val_loss: 41.2901\nEpoch 39/50\n - 0s - loss: 33.0550 - val_loss: 41.2378\nEpoch 40/50\n - 0s - loss: 32.9795 - val_loss: 41.3091\nEpoch 41/50\n - 0s - loss: 32.9777 - val_loss: 41.5063\nEpoch 42/50\n - 0s - loss: 33.0601 - val_loss: 41.2102\nEpoch 43/50\n - 0s - loss: 33.0427 - val_loss: 41.0700\nEpoch 44/50\n - 0s - loss: 32.9651 - val_loss: 41.5489\nEpoch 45/50\n - 0s - loss: 32.9860 - val_loss: 41.1501\nEpoch 46/50\n - 0s - loss: 33.0846 - val_loss: 41.3823\nEpoch 47/50\n - 0s - loss: 33.0848 - val_loss: 41.4172\nEpoch 48/50\n - 0s - loss: 32.9992 - val_loss: 41.3123\nEpoch 49/50\n - 0s - loss: 32.9862 - val_loss: 41.1685\nEpoch 50/50\n - 0s - loss: 33.1670 - val_loss: 41.5282\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.0606 - val_loss: 41.0358\nEpoch 2/50\n - 0s - loss: 33.0088 - val_loss: 41.3300\nEpoch 3/50\n - 0s - loss: 32.9789 - val_loss: 41.3375\nEpoch 4/50\n - 0s - loss: 33.0919 - val_loss: 41.4376\nEpoch 5/50\n - 0s - loss: 33.0001 - val_loss: 41.4262\nEpoch 6/50\n - 0s - loss: 32.9844 - val_loss: 41.4448\nEpoch 7/50\n - 0s - loss: 33.1362 - val_loss: 41.2944\nEpoch 8/50\n - 0s - loss: 33.0346 - val_loss: 41.6594\nEpoch 9/50\n - 0s - loss: 33.0017 - val_loss: 41.4612\nEpoch 10/50\n - 0s - loss: 33.3643 - val_loss: 41.0363\nEpoch 11/50\n - 0s - loss: 33.1213 - val_loss: 41.6882\nEpoch 12/50\n - 0s - loss: 33.0898 - val_loss: 41.6215\nEpoch 13/50\n - 0s - loss: 33.0232 - val_loss: 41.4101\nEpoch 14/50\n - 0s - loss: 33.0052 - val_loss: 41.1521\nEpoch 15/50\n - 0s - loss: 32.9389 - val_loss: 41.5224\nEpoch 16/50\n - 0s - loss: 33.0511 - val_loss: 41.3885\nEpoch 17/50\n - 0s - loss: 32.9767 - val_loss: 41.3985\nEpoch 18/50\n - 0s - loss: 33.0245 - val_loss: 41.5277\nEpoch 19/50\n - 0s - loss: 33.0117 - val_loss: 41.1668\nEpoch 20/50\n - 0s - loss: 33.0043 - val_loss: 41.2903\nEpoch 21/50\n - 0s - loss: 33.0732 - val_loss: 41.4385\nEpoch 22/50\n - 0s - loss: 32.9998 - val_loss: 41.3247\nEpoch 23/50\n - 0s - loss: 33.0227 - val_loss: 41.2523\nEpoch 24/50\n - 0s - loss: 33.0650 - val_loss: 41.3013\nEpoch 25/50\n - 0s - loss: 32.9975 - val_loss: 40.9514\nEpoch 26/50\n - 0s - loss: 33.0451 - val_loss: 41.3777\nEpoch 27/50\n - 0s - loss: 33.0074 - val_loss: 41.2598\nEpoch 28/50\n - 0s - loss: 32.9805 - val_loss: 41.2977\nEpoch 29/50\n - 0s - loss: 33.0178 - val_loss: 41.6291\nEpoch 30/50\n - 0s - loss: 33.0236 - val_loss: 41.4289\nEpoch 31/50\n - 0s - loss: 32.9782 - val_loss: 41.3137\nEpoch 32/50\n - 0s - loss: 33.0012 - val_loss: 41.2137\nEpoch 33/50\n - 0s - loss: 33.0518 - val_loss: 41.3854\nEpoch 34/50\n - 0s - loss: 33.1128 - val_loss: 41.4042\nEpoch 35/50\n - 0s - loss: 33.0296 - val_loss: 41.0511\nEpoch 36/50\n - 0s - loss: 33.1028 - val_loss: 41.4761\nEpoch 37/50\n - 0s - loss: 33.1048 - val_loss: 41.7457\nEpoch 38/50\n - 0s - loss: 33.0275 - val_loss: 41.0435\nEpoch 39/50\n - 0s - loss: 33.0692 - val_loss: 41.1299\nEpoch 40/50\n - 0s - loss: 32.9363 - val_loss: 41.2993\nEpoch 41/50\n - 0s - loss: 32.9907 - val_loss: 41.2258\nEpoch 42/50\n - 0s - loss: 33.0256 - val_loss: 41.6974\nEpoch 43/50\n - 0s - loss: 33.0413 - val_loss: 41.4024\nEpoch 44/50\n - 0s - loss: 33.1077 - val_loss: 41.2493\nEpoch 45/50\n - 0s - loss: 32.9745 - val_loss: 41.2743\nEpoch 46/50\n - 0s - loss: 33.0123 - val_loss: 41.4769\nEpoch 47/50\n - 0s - loss: 33.0623 - val_loss: 41.0392\nEpoch 48/50\n - 0s - loss: 33.0081 - val_loss: 41.2697\nEpoch 49/50\n - 0s - loss: 33.0142 - val_loss: 41.3027\nEpoch 50/50\n - 0s - loss: 33.0702 - val_loss: 40.9095\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 33.0609 - val_loss: 41.7472\nEpoch 2/50\n - 0s - loss: 33.0492 - val_loss: 40.9278\nEpoch 3/50\n - 0s - loss: 33.0523 - val_loss: 41.4176\nEpoch 4/50\n - 0s - loss: 32.9551 - val_loss: 41.3309\nEpoch 5/50\n - 0s - loss: 32.9833 - val_loss: 41.4303\nEpoch 6/50\n - 0s - loss: 32.9376 - val_loss: 41.4144\nEpoch 7/50\n - 0s - loss: 33.0473 - val_loss: 41.2828\nEpoch 8/50\n - 0s - loss: 32.9167 - val_loss: 41.3761\nEpoch 9/50\n - 0s - loss: 32.9524 - val_loss: 41.4057\nEpoch 10/50\n - 0s - loss: 33.0765 - val_loss: 41.4218\nEpoch 11/50\n - 0s - loss: 32.9770 - val_loss: 41.2540\nEpoch 12/50\n - 0s - loss: 33.0924 - val_loss: 41.5258\nEpoch 13/50\n - 0s - loss: 33.0120 - val_loss: 41.2440\nEpoch 14/50\n - 0s - loss: 33.0660 - val_loss: 41.4336\nEpoch 15/50\n - 0s - loss: 33.0259 - val_loss: 41.5856\nEpoch 16/50\n - 0s - loss: 32.9980 - val_loss: 41.0965\nEpoch 17/50\n - 0s - loss: 33.0051 - val_loss: 41.4229\nEpoch 18/50\n - 0s - loss: 32.9522 - val_loss: 41.5703\nEpoch 19/50\n - 0s - loss: 33.0024 - val_loss: 41.2043\nEpoch 20/50\n - 0s - loss: 32.9374 - val_loss: 41.3248\nEpoch 21/50\n - 0s - loss: 32.9674 - val_loss: 41.2799\nEpoch 22/50\n - 0s - loss: 33.0186 - val_loss: 41.4157\nEpoch 23/50\n - 0s - loss: 32.9223 - val_loss: 41.3052\nEpoch 24/50\n - 0s - loss: 32.9865 - val_loss: 41.4203\nEpoch 25/50\n - 0s - loss: 32.9873 - val_loss: 41.3935\nEpoch 26/50\n - 0s - loss: 32.9409 - val_loss: 40.9827\nEpoch 27/50\n - 0s - loss: 32.9900 - val_loss: 41.1903\nEpoch 28/50\n - 0s - loss: 32.9378 - val_loss: 41.4600\nEpoch 29/50\n - 0s - loss: 33.0367 - val_loss: 41.2734\nEpoch 30/50\n - 0s - loss: 32.9502 - val_loss: 41.4296\nEpoch 31/50\n - 0s - loss: 32.9319 - val_loss: 41.3479\nEpoch 32/50\n - 0s - loss: 32.9451 - val_loss: 41.2899\nEpoch 33/50\n - 0s - loss: 33.0254 - val_loss: 41.6114\nEpoch 34/50\n - 0s - loss: 32.9860 - val_loss: 41.1042\nEpoch 35/50\n - 0s - loss: 33.0166 - val_loss: 41.2154\nEpoch 36/50\n - 0s - loss: 32.9634 - val_loss: 41.0853\nEpoch 37/50\n - 0s - loss: 32.9566 - val_loss: 41.4742\nEpoch 38/50\n - 0s - loss: 33.0914 - val_loss: 41.3274\nEpoch 39/50\n - 0s - loss: 33.0091 - val_loss: 41.4517\nEpoch 40/50\n - 0s - loss: 32.9914 - val_loss: 41.3454\nEpoch 41/50\n - 0s - loss: 33.0020 - val_loss: 41.4310\nEpoch 42/50\n - 0s - loss: 33.0146 - val_loss: 41.4107\nEpoch 43/50\n - 0s - loss: 32.9574 - val_loss: 41.3738\nEpoch 44/50\n - 0s - loss: 32.9330 - val_loss: 41.3350\nEpoch 45/50\n - 0s - loss: 32.9528 - val_loss: 41.3487\nEpoch 46/50\n - 0s - loss: 33.0013 - val_loss: 41.3494\nEpoch 47/50\n - 0s - loss: 33.0058 - val_loss: 41.2596\nEpoch 48/50\n - 0s - loss: 32.9814 - val_loss: 41.5245\nEpoch 49/50\n - 0s - loss: 32.9709 - val_loss: 41.2322\nEpoch 50/50\n - 0s - loss: 32.9476 - val_loss: 41.3752\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.9628 - val_loss: 41.1699\nEpoch 2/50\n - 0s - loss: 32.9487 - val_loss: 41.4696\nEpoch 3/50\n - 0s - loss: 33.0947 - val_loss: 41.3505\nEpoch 4/50\n - 0s - loss: 32.9990 - val_loss: 41.4607\nEpoch 5/50\n - 0s - loss: 32.9948 - val_loss: 41.3510\nEpoch 6/50\n - 0s - loss: 32.9451 - val_loss: 41.5917\nEpoch 7/50\n - 0s - loss: 32.9122 - val_loss: 41.3172\nEpoch 8/50\n - 0s - loss: 32.9998 - val_loss: 41.1847\nEpoch 9/50\n - 0s - loss: 33.0189 - val_loss: 41.7769\nEpoch 10/50\n - 0s - loss: 32.9066 - val_loss: 41.4343\nEpoch 11/50\n - 0s - loss: 32.9305 - val_loss: 41.4871\nEpoch 12/50\n - 0s - loss: 32.9395 - val_loss: 41.3202\nEpoch 13/50\n - 0s - loss: 32.9712 - val_loss: 41.4795\nEpoch 14/50\n - 0s - loss: 32.9719 - val_loss: 41.1224\nEpoch 15/50\n - 0s - loss: 32.9785 - val_loss: 41.5311\nEpoch 16/50\n - 0s - loss: 33.1111 - val_loss: 41.7283\nEpoch 17/50\n - 0s - loss: 33.2726 - val_loss: 41.1339\nEpoch 18/50\n - 0s - loss: 32.9873 - val_loss: 41.2821\nEpoch 19/50\n - 0s - loss: 32.9515 - val_loss: 41.3387\nEpoch 20/50\n - 0s - loss: 32.9305 - val_loss: 41.5691\nEpoch 21/50\n - 0s - loss: 33.2737 - val_loss: 41.1430\nEpoch 22/50\n - 0s - loss: 32.9807 - val_loss: 41.6535\nEpoch 23/50\n - 0s - loss: 33.0041 - val_loss: 41.4520\nEpoch 24/50\n - 0s - loss: 33.0442 - val_loss: 41.3468\nEpoch 25/50\n - 0s - loss: 32.9455 - val_loss: 41.2661\nEpoch 26/50\n - 0s - loss: 32.9732 - val_loss: 41.4417\nEpoch 27/50\n - 0s - loss: 32.9272 - val_loss: 41.4654\nEpoch 28/50\n - 0s - loss: 33.1324 - val_loss: 41.3482\nEpoch 29/50\n - 0s - loss: 33.0760 - val_loss: 41.5507\nEpoch 30/50\n - 0s - loss: 32.9295 - val_loss: 41.2933\nEpoch 31/50\n - 0s - loss: 33.0382 - val_loss: 41.4650\nEpoch 32/50\n - 0s - loss: 32.9287 - val_loss: 41.3315\nEpoch 33/50\n - 0s - loss: 33.0681 - val_loss: 41.4140\nEpoch 34/50\n - 0s - loss: 33.0688 - val_loss: 41.3395\nEpoch 35/50\n - 0s - loss: 33.0656 - val_loss: 41.6678\nEpoch 36/50\n - 0s - loss: 32.9354 - val_loss: 41.0988\nEpoch 37/50\n - 0s - loss: 32.9258 - val_loss: 41.4323\nEpoch 38/50\n - 0s - loss: 33.1224 - val_loss: 41.7146\nEpoch 39/50\n - 0s - loss: 33.1087 - val_loss: 41.2156\nEpoch 40/50\n - 0s - loss: 32.9695 - val_loss: 41.7411\nEpoch 41/50\n - 0s - loss: 32.9435 - val_loss: 41.3485\nEpoch 42/50\n - 0s - loss: 33.0367 - val_loss: 41.3780\nEpoch 43/50\n - 0s - loss: 32.9700 - val_loss: 41.2733\nEpoch 44/50\n - 0s - loss: 32.9202 - val_loss: 41.1523\nEpoch 45/50\n - 0s - loss: 32.9044 - val_loss: 41.4398\nEpoch 46/50\n - 0s - loss: 32.9480 - val_loss: 41.7054\nEpoch 47/50\n - 0s - loss: 32.9630 - val_loss: 41.4810\nEpoch 48/50\n - 0s - loss: 32.9332 - val_loss: 41.2744\nEpoch 49/50\n - 0s - loss: 33.0173 - val_loss: 41.4824\nEpoch 50/50\n - 0s - loss: 32.9815 - val_loss: 41.3438\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.9342 - val_loss: 41.4414\nEpoch 2/50\n - 0s - loss: 32.9149 - val_loss: 41.0770\nEpoch 3/50\n - 0s - loss: 33.0166 - val_loss: 41.6100\nEpoch 4/50\n - 0s - loss: 32.9016 - val_loss: 41.2089\nEpoch 5/50\n - 0s - loss: 32.9477 - val_loss: 41.3183\nEpoch 6/50\n - 0s - loss: 33.0410 - val_loss: 41.7754\nEpoch 7/50\n - 0s - loss: 32.9818 - val_loss: 41.1720\nEpoch 8/50\n - 0s - loss: 32.9111 - val_loss: 41.5872\nEpoch 9/50\n - 0s - loss: 32.9386 - val_loss: 41.3641\nEpoch 10/50\n - 0s - loss: 32.9775 - val_loss: 41.5482\nEpoch 11/50\n - 0s - loss: 32.9721 - val_loss: 41.4608\nEpoch 12/50\n - 0s - loss: 33.1258 - val_loss: 41.3868\nEpoch 13/50\n - 0s - loss: 32.9778 - val_loss: 41.3170\nEpoch 14/50\n - 0s - loss: 33.0191 - val_loss: 41.1423\nEpoch 15/50\n - 0s - loss: 33.0145 - val_loss: 41.4626\nEpoch 16/50\n - 0s - loss: 32.9007 - val_loss: 41.3137\nEpoch 17/50\n - 0s - loss: 33.0625 - val_loss: 41.1764\nEpoch 18/50\n - 0s - loss: 33.0065 - val_loss: 41.5069\nEpoch 19/50\n - 0s - loss: 32.9170 - val_loss: 41.2811\nEpoch 20/50\n - 0s - loss: 32.9516 - val_loss: 41.4775\nEpoch 21/50\n - 0s - loss: 32.9086 - val_loss: 41.2764\nEpoch 22/50\n - 0s - loss: 32.9266 - val_loss: 41.5082\nEpoch 23/50\n - 0s - loss: 32.8950 - val_loss: 41.5094\nEpoch 24/50\n - 0s - loss: 33.0526 - val_loss: 41.9318\nEpoch 25/50\n - 0s - loss: 32.9461 - val_loss: 41.2891\nEpoch 26/50\n - 0s - loss: 33.0117 - val_loss: 41.1526\nEpoch 27/50\n - 0s - loss: 32.9644 - val_loss: 41.6894\nEpoch 28/50\n - 0s - loss: 32.9265 - val_loss: 41.2477\nEpoch 29/50\n - 0s - loss: 32.9366 - val_loss: 41.6067\nEpoch 30/50\n - 0s - loss: 32.8910 - val_loss: 41.2942\nEpoch 31/50\n - 0s - loss: 33.1848 - val_loss: 41.6335\nEpoch 32/50\n - 0s - loss: 32.9319 - val_loss: 41.1671\nEpoch 33/50\n - 0s - loss: 32.9170 - val_loss: 41.3243\nEpoch 34/50\n - 0s - loss: 32.9622 - val_loss: 41.5877\nEpoch 35/50\n - 0s - loss: 32.9004 - val_loss: 41.4013\nEpoch 36/50\n - 0s - loss: 32.9458 - val_loss: 41.2332\nEpoch 37/50\n - 0s - loss: 33.0535 - val_loss: 41.5754\nEpoch 38/50\n - 0s - loss: 32.9941 - val_loss: 41.2703\nEpoch 39/50\n - 0s - loss: 32.9694 - val_loss: 41.2098\nEpoch 40/50\n - 0s - loss: 33.0338 - val_loss: 41.4648\nEpoch 41/50\n - 0s - loss: 33.0128 - val_loss: 41.4296\nEpoch 42/50\n - 0s - loss: 32.9381 - val_loss: 41.6529\nEpoch 43/50\n - 0s - loss: 32.9935 - val_loss: 41.4647\nEpoch 44/50\n - 0s - loss: 32.9538 - val_loss: 41.2922\nEpoch 45/50\n - 0s - loss: 33.0533 - val_loss: 41.4409\nEpoch 46/50\n - 0s - loss: 32.9245 - val_loss: 41.1397\nEpoch 47/50\n - 0s - loss: 33.1600 - val_loss: 41.4109\nEpoch 48/50\n - 0s - loss: 32.8985 - val_loss: 41.5769\nEpoch 49/50\n - 0s - loss: 32.9832 - val_loss: 41.4975\nEpoch 50/50\n - 0s - loss: 32.9681 - val_loss: 41.2305\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.9038 - val_loss: 41.4967\nEpoch 2/50\n - 0s - loss: 32.8814 - val_loss: 41.3777\nEpoch 3/50\n - 0s - loss: 32.9211 - val_loss: 41.5135\nEpoch 4/50\n - 0s - loss: 32.9423 - val_loss: 41.5487\nEpoch 5/50\n - 0s - loss: 32.9537 - val_loss: 41.5128\nEpoch 6/50\n - 0s - loss: 32.9686 - val_loss: 41.3973\nEpoch 7/50\n - 0s - loss: 33.0270 - val_loss: 41.1193\nEpoch 8/50\n - 0s - loss: 32.9118 - val_loss: 41.4288\nEpoch 9/50\n - 0s - loss: 32.9828 - val_loss: 41.3998\nEpoch 10/50\n - 0s - loss: 32.8752 - val_loss: 41.4345\nEpoch 11/50\n - 0s - loss: 32.8795 - val_loss: 41.3403\nEpoch 12/50\n - 0s - loss: 32.8840 - val_loss: 41.3443\nEpoch 13/50\n - 0s - loss: 32.8544 - val_loss: 41.5179\nEpoch 14/50\n - 0s - loss: 32.9256 - val_loss: 41.5052\nEpoch 15/50\n - 0s - loss: 32.8732 - val_loss: 41.3913\nEpoch 16/50\n - 0s - loss: 32.9279 - val_loss: 41.4648\nEpoch 17/50\n - 0s - loss: 32.9415 - val_loss: 41.4291\nEpoch 18/50\n - 0s - loss: 32.8605 - val_loss: 41.6608\nEpoch 19/50\n - 0s - loss: 33.0129 - val_loss: 41.4252\nEpoch 20/50\n - 0s - loss: 32.9515 - val_loss: 41.2755\nEpoch 21/50\n - 0s - loss: 32.8895 - val_loss: 41.3312\nEpoch 22/50\n - 0s - loss: 32.8624 - val_loss: 41.4544\nEpoch 23/50\n - 0s - loss: 32.9304 - val_loss: 41.6575\nEpoch 24/50\n - 0s - loss: 32.8725 - val_loss: 41.4512\nEpoch 25/50\n - 0s - loss: 32.8977 - val_loss: 41.1783\nEpoch 26/50\n - 0s - loss: 32.9156 - val_loss: 41.3563\nEpoch 27/50\n - 0s - loss: 32.8900 - val_loss: 41.1649\nEpoch 28/50\n - 0s - loss: 32.8754 - val_loss: 41.3218\nEpoch 29/50\n - 0s - loss: 32.8921 - val_loss: 41.6578\nEpoch 30/50\n - 0s - loss: 33.0401 - val_loss: 41.4537\nEpoch 31/50\n - 0s - loss: 33.0440 - val_loss: 41.3603\nEpoch 32/50\n - 0s - loss: 32.8620 - val_loss: 41.4876\nEpoch 33/50\n - 0s - loss: 32.9388 - val_loss: 41.5191\nEpoch 34/50\n - 0s - loss: 32.9079 - val_loss: 41.3532\nEpoch 35/50\n - 0s - loss: 32.8669 - val_loss: 41.2754\nEpoch 36/50\n - 0s - loss: 32.9152 - val_loss: 41.1077\nEpoch 37/50\n - 0s - loss: 32.9377 - val_loss: 41.7153\nEpoch 38/50\n - 0s - loss: 32.8887 - val_loss: 41.4978\nEpoch 39/50\n - 0s - loss: 32.8968 - val_loss: 41.3053\nEpoch 40/50\n - 0s - loss: 32.9281 - val_loss: 41.5491\nEpoch 41/50\n - 0s - loss: 32.8780 - val_loss: 41.5776\nEpoch 42/50\n - 0s - loss: 32.8698 - val_loss: 41.5310\nEpoch 43/50\n - 0s - loss: 32.8872 - val_loss: 41.5374\nEpoch 44/50\n - 0s - loss: 33.0265 - val_loss: 41.4055\nEpoch 45/50\n - 0s - loss: 32.8815 - val_loss: 41.4827\nEpoch 46/50\n - 0s - loss: 32.9928 - val_loss: 41.4177\nEpoch 47/50\n - 0s - loss: 32.8748 - val_loss: 41.4681\nEpoch 48/50\n - 0s - loss: 32.8924 - val_loss: 41.3557\nEpoch 49/50\n - 0s - loss: 33.0934 - val_loss: 41.3759\nEpoch 50/50\n - 0s - loss: 32.8649 - val_loss: 41.2622\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.9085 - val_loss: 41.4275\nEpoch 2/50\n - 0s - loss: 32.8790 - val_loss: 41.4834\nEpoch 3/50\n - 0s - loss: 32.9352 - val_loss: 41.1064\nEpoch 4/50\n - 0s - loss: 32.9617 - val_loss: 41.4478\nEpoch 5/50\n - 0s - loss: 32.9628 - val_loss: 41.6492\nEpoch 6/50\n - 0s - loss: 32.8700 - val_loss: 41.4013\nEpoch 7/50\n - 0s - loss: 32.8513 - val_loss: 41.5168\nEpoch 8/50\n - 0s - loss: 32.9104 - val_loss: 41.5743\nEpoch 9/50\n - 0s - loss: 32.8856 - val_loss: 41.2861\nEpoch 10/50\n - 0s - loss: 32.8844 - val_loss: 41.6176\nEpoch 11/50\n - 0s - loss: 32.8424 - val_loss: 41.2063\nEpoch 12/50\n - 0s - loss: 32.9061 - val_loss: 41.3928\nEpoch 13/50\n - 0s - loss: 32.8793 - val_loss: 41.3684\nEpoch 14/50\n - 0s - loss: 32.8696 - val_loss: 41.2537\nEpoch 15/50\n - 0s - loss: 32.8602 - val_loss: 41.4424\nEpoch 16/50\n - 0s - loss: 32.8763 - val_loss: 41.5540\nEpoch 17/50\n - 0s - loss: 32.8944 - val_loss: 41.6607\nEpoch 18/50\n - 0s - loss: 32.9057 - val_loss: 41.3199\nEpoch 19/50\n - 0s - loss: 32.8698 - val_loss: 41.6561\nEpoch 20/50\n - 0s - loss: 32.9618 - val_loss: 41.5004\nEpoch 21/50\n - 0s - loss: 32.8620 - val_loss: 41.2829\nEpoch 22/50\n - 0s - loss: 32.8612 - val_loss: 41.4329\nEpoch 23/50\n - 0s - loss: 32.9361 - val_loss: 41.6603\nEpoch 24/50\n - 0s - loss: 32.8739 - val_loss: 41.2610\nEpoch 25/50\n - 0s - loss: 32.8975 - val_loss: 41.3822\nEpoch 26/50\n - 0s - loss: 32.8452 - val_loss: 41.4804\nEpoch 27/50\n - 0s - loss: 32.9248 - val_loss: 41.2534\nEpoch 28/50\n - 0s - loss: 32.8794 - val_loss: 41.5906\nEpoch 29/50\n - 0s - loss: 32.8183 - val_loss: 41.5143\nEpoch 30/50\n - 0s - loss: 33.1106 - val_loss: 41.4254\nEpoch 31/50\n - 0s - loss: 32.8928 - val_loss: 41.2340\nEpoch 32/50\n - 0s - loss: 33.0272 - val_loss: 41.4467\nEpoch 33/50\n - 0s - loss: 32.9335 - val_loss: 41.2577\nEpoch 34/50\n - 0s - loss: 32.9822 - val_loss: 41.4517\nEpoch 35/50\n - 0s - loss: 32.8502 - val_loss: 41.4301\nEpoch 36/50\n - 0s - loss: 32.8922 - val_loss: 41.3629\nEpoch 37/50\n - 0s - loss: 32.9672 - val_loss: 41.5978\nEpoch 38/50\n - 0s - loss: 32.8923 - val_loss: 41.3223\nEpoch 39/50\n - 0s - loss: 32.9274 - val_loss: 41.6230\nEpoch 40/50\n - 0s - loss: 32.8349 - val_loss: 41.5643\nEpoch 41/50\n - 0s - loss: 32.8474 - val_loss: 41.3916\nEpoch 42/50\n - 0s - loss: 32.8706 - val_loss: 41.4296\nEpoch 43/50\n - 0s - loss: 32.9232 - val_loss: 41.4554\nEpoch 44/50\n - 0s - loss: 32.9128 - val_loss: 41.4246\nEpoch 45/50\n - 0s - loss: 33.1441 - val_loss: 41.2602\nEpoch 46/50\n - 0s - loss: 32.9591 - val_loss: 41.4656\nEpoch 47/50\n - 0s - loss: 32.8497 - val_loss: 41.4201\nEpoch 48/50\n - 0s - loss: 32.8676 - val_loss: 41.3550\nEpoch 49/50\n - 0s - loss: 32.9214 - val_loss: 41.6836\nEpoch 50/50\n - 0s - loss: 32.8498 - val_loss: 41.4412\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.8891 - val_loss: 41.3149\nEpoch 2/50\n - 0s - loss: 33.0085 - val_loss: 41.6093\nEpoch 3/50\n - 0s - loss: 32.8055 - val_loss: 41.2998\nEpoch 4/50\n - 0s - loss: 32.8450 - val_loss: 41.4930\nEpoch 5/50\n - 0s - loss: 32.8254 - val_loss: 41.3680\nEpoch 6/50\n - 0s - loss: 32.8474 - val_loss: 41.2466\nEpoch 7/50\n - 0s - loss: 32.8728 - val_loss: 41.6478\nEpoch 8/50\n - 0s - loss: 33.0280 - val_loss: 41.3750\nEpoch 9/50\n - 0s - loss: 32.8134 - val_loss: 41.6006\nEpoch 10/50\n - 0s - loss: 32.8623 - val_loss: 41.5686\nEpoch 11/50\n - 0s - loss: 32.9276 - val_loss: 41.2870\nEpoch 12/50\n - 0s - loss: 32.8331 - val_loss: 41.7159\nEpoch 13/50\n - 0s - loss: 32.8835 - val_loss: 41.5812\nEpoch 14/50\n - 0s - loss: 32.8835 - val_loss: 41.4337\nEpoch 15/50\n - 0s - loss: 32.9250 - val_loss: 41.5578\nEpoch 16/50\n - 0s - loss: 32.8474 - val_loss: 41.4215\nEpoch 17/50\n - 0s - loss: 32.9166 - val_loss: 41.5279\nEpoch 18/50\n - 0s - loss: 32.8622 - val_loss: 41.3795\nEpoch 19/50\n - 0s - loss: 32.9309 - val_loss: 41.4261\nEpoch 20/50\n - 0s - loss: 32.8482 - val_loss: 41.4694\nEpoch 21/50\n - 0s - loss: 32.8207 - val_loss: 41.6142\nEpoch 22/50\n - 0s - loss: 32.9691 - val_loss: 41.1380\nEpoch 23/50\n - 0s - loss: 32.8995 - val_loss: 41.6710\nEpoch 24/50\n - 0s - loss: 32.8806 - val_loss: 41.2044\nEpoch 25/50\n - 0s - loss: 32.9270 - val_loss: 41.4080\nEpoch 26/50\n - 0s - loss: 32.9001 - val_loss: 41.7149\nEpoch 27/50\n - 0s - loss: 32.9049 - val_loss: 41.4253\nEpoch 28/50\n - 0s - loss: 32.7880 - val_loss: 41.2002\nEpoch 29/50\n - 0s - loss: 32.8332 - val_loss: 41.1948\nEpoch 30/50\n - 0s - loss: 32.9853 - val_loss: 41.4369\nEpoch 31/50\n - 0s - loss: 32.8927 - val_loss: 41.6116\nEpoch 32/50\n - 0s - loss: 32.8467 - val_loss: 41.3923\nEpoch 33/50\n - 0s - loss: 32.9081 - val_loss: 41.0498\nEpoch 34/50\n - 0s - loss: 32.8043 - val_loss: 41.4419\nEpoch 35/50\n - 0s - loss: 32.8849 - val_loss: 41.5777\nEpoch 36/50\n - 0s - loss: 32.8747 - val_loss: 41.3182\nEpoch 37/50\n - 0s - loss: 32.9120 - val_loss: 41.5315\nEpoch 38/50\n - 0s - loss: 32.9224 - val_loss: 41.7228\nEpoch 39/50\n - 0s - loss: 32.8897 - val_loss: 41.6427\nEpoch 40/50\n - 0s - loss: 32.8215 - val_loss: 41.2348\nEpoch 41/50\n - 0s - loss: 32.8415 - val_loss: 41.3908\nEpoch 42/50\n - 0s - loss: 32.8542 - val_loss: 41.4713\nEpoch 43/50\n - 0s - loss: 32.8898 - val_loss: 41.5098\nEpoch 44/50\n - 0s - loss: 32.8757 - val_loss: 41.5888\nEpoch 45/50\n - 0s - loss: 32.8442 - val_loss: 41.1687\nEpoch 46/50\n - 0s - loss: 32.8295 - val_loss: 41.3212\nEpoch 47/50\n - 0s - loss: 32.9196 - val_loss: 41.4310\nEpoch 48/50\n - 0s - loss: 32.9099 - val_loss: 41.5295\nEpoch 49/50\n - 0s - loss: 32.8179 - val_loss: 41.5950\nEpoch 50/50\n - 0s - loss: 32.9314 - val_loss: 41.6537\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.8274 - val_loss: 41.3163\nEpoch 2/50\n - 0s - loss: 32.7963 - val_loss: 41.4281\nEpoch 3/50\n - 0s - loss: 32.8195 - val_loss: 41.6117\nEpoch 4/50\n - 0s - loss: 32.8397 - val_loss: 41.1270\nEpoch 5/50\n - 0s - loss: 32.8412 - val_loss: 41.4815\nEpoch 6/50\n - 0s - loss: 32.7780 - val_loss: 41.6387\nEpoch 7/50\n - 0s - loss: 32.8238 - val_loss: 41.5789\nEpoch 8/50\n - 0s - loss: 32.8436 - val_loss: 41.3449\nEpoch 9/50\n - 0s - loss: 32.8810 - val_loss: 41.3278\nEpoch 10/50\n - 0s - loss: 32.9357 - val_loss: 41.6263\nEpoch 11/50\n - 0s - loss: 32.8799 - val_loss: 41.4618\nEpoch 12/50\n - 0s - loss: 32.8636 - val_loss: 41.2557\nEpoch 13/50\n - 0s - loss: 32.8237 - val_loss: 41.5994\nEpoch 14/50\n - 0s - loss: 32.8156 - val_loss: 41.3700\nEpoch 15/50\n - 0s - loss: 32.8152 - val_loss: 41.3351\nEpoch 16/50\n - 0s - loss: 32.8139 - val_loss: 41.6230\nEpoch 17/50\n - 0s - loss: 32.8394 - val_loss: 41.5473\nEpoch 18/50\n - 0s - loss: 32.8590 - val_loss: 41.2661\nEpoch 19/50\n - 0s - loss: 32.9882 - val_loss: 41.6575\nEpoch 20/50\n - 0s - loss: 32.9299 - val_loss: 41.4901\nEpoch 21/50\n - 0s - loss: 32.7403 - val_loss: 41.5485\nEpoch 22/50\n - 0s - loss: 32.8911 - val_loss: 41.6226\nEpoch 23/50\n - 0s - loss: 32.8206 - val_loss: 41.2590\nEpoch 24/50\n - 0s - loss: 32.8527 - val_loss: 41.5466\nEpoch 25/50\n - 0s - loss: 32.8695 - val_loss: 41.4971\nEpoch 26/50\n - 0s - loss: 32.9129 - val_loss: 41.1533\nEpoch 27/50\n - 0s - loss: 32.8192 - val_loss: 41.6649\nEpoch 28/50\n - 0s - loss: 32.7906 - val_loss: 41.6185\nEpoch 29/50\n - 0s - loss: 32.8163 - val_loss: 41.5730\nEpoch 30/50\n - 0s - loss: 32.8389 - val_loss: 41.4660\nEpoch 31/50\n - 0s - loss: 32.8374 - val_loss: 41.3350\nEpoch 32/50\n - 0s - loss: 32.8528 - val_loss: 41.3513\nEpoch 33/50\n - 0s - loss: 32.7701 - val_loss: 41.1422\nEpoch 34/50\n - 0s - loss: 32.7776 - val_loss: 41.4663\nEpoch 35/50\n - 0s - loss: 32.7793 - val_loss: 41.5671\nEpoch 36/50\n - 0s - loss: 32.8581 - val_loss: 41.4676\nEpoch 37/50\n - 0s - loss: 32.8223 - val_loss: 41.4877\nEpoch 38/50\n - 0s - loss: 32.8193 - val_loss: 41.5855\nEpoch 39/50\n - 0s - loss: 32.8346 - val_loss: 41.7609\nEpoch 40/50\n - 0s - loss: 32.9134 - val_loss: 41.3311\nEpoch 41/50\n - 0s - loss: 32.7207 - val_loss: 41.4533\nEpoch 42/50\n - 0s - loss: 33.0125 - val_loss: 41.8148\nEpoch 43/50\n - 0s - loss: 32.8147 - val_loss: 41.5285\nEpoch 44/50\n - 0s - loss: 32.8901 - val_loss: 40.9261\nEpoch 45/50\n - 0s - loss: 32.8532 - val_loss: 41.5449\nEpoch 46/50\n - 0s - loss: 32.8672 - val_loss: 41.5478\nEpoch 47/50\n - 0s - loss: 32.7491 - val_loss: 41.6206\nEpoch 48/50\n - 0s - loss: 32.8337 - val_loss: 41.3818\nEpoch 49/50\n - 0s - loss: 32.7777 - val_loss: 41.4441\nEpoch 50/50\n - 0s - loss: 32.8155 - val_loss: 41.5666\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.7612 - val_loss: 41.5032\nEpoch 2/50\n - 0s - loss: 32.8021 - val_loss: 41.2798\nEpoch 3/50\n - 0s - loss: 32.7564 - val_loss: 41.6094\nEpoch 4/50\n - 0s - loss: 32.8057 - val_loss: 41.7832\nEpoch 5/50\n - 0s - loss: 32.8487 - val_loss: 41.5082\nEpoch 6/50\n - 0s - loss: 33.0908 - val_loss: 41.1363\nEpoch 7/50\n - 0s - loss: 32.8665 - val_loss: 41.7394\nEpoch 8/50\n - 0s - loss: 32.8344 - val_loss: 41.2867\nEpoch 9/50\n - 0s - loss: 32.8270 - val_loss: 41.8488\nEpoch 10/50\n - 0s - loss: 32.8072 - val_loss: 41.2680\nEpoch 11/50\n - 0s - loss: 32.8636 - val_loss: 41.4650\nEpoch 12/50\n - 0s - loss: 32.7588 - val_loss: 41.5998\nEpoch 13/50\n - 0s - loss: 32.8865 - val_loss: 41.3066\nEpoch 14/50\n - 0s - loss: 32.7956 - val_loss: 41.5241\nEpoch 15/50\n - 0s - loss: 32.7621 - val_loss: 41.2376\nEpoch 16/50\n - 0s - loss: 32.9332 - val_loss: 41.4136\nEpoch 17/50\n - 0s - loss: 32.9722 - val_loss: 41.1050\nEpoch 18/50\n - 0s - loss: 32.7698 - val_loss: 41.7171\nEpoch 19/50\n - 0s - loss: 32.8009 - val_loss: 41.5111\nEpoch 20/50\n - 0s - loss: 32.8546 - val_loss: 41.3150\nEpoch 21/50\n - 0s - loss: 32.8390 - val_loss: 41.1253\nEpoch 22/50\n - 0s - loss: 32.7648 - val_loss: 41.4128\nEpoch 23/50\n - 0s - loss: 32.8061 - val_loss: 41.3933\nEpoch 24/50\n - 0s - loss: 32.7631 - val_loss: 41.5271\nEpoch 25/50\n - 0s - loss: 32.7600 - val_loss: 41.7883\nEpoch 26/50\n - 0s - loss: 32.7920 - val_loss: 41.3315\nEpoch 27/50\n - 0s - loss: 32.7593 - val_loss: 41.4232\nEpoch 28/50\n - 0s - loss: 32.7644 - val_loss: 41.2805\nEpoch 29/50\n - 0s - loss: 32.7922 - val_loss: 41.5608\nEpoch 30/50\n - 0s - loss: 32.8049 - val_loss: 41.4699\nEpoch 31/50\n - 0s - loss: 32.7713 - val_loss: 41.3129\nEpoch 32/50\n - 0s - loss: 32.8516 - val_loss: 41.4273\nEpoch 33/50\n - 0s - loss: 32.7580 - val_loss: 41.3898\nEpoch 34/50\n - 0s - loss: 32.7816 - val_loss: 41.3528\nEpoch 35/50\n - 0s - loss: 32.7717 - val_loss: 41.3753\nEpoch 36/50\n - 0s - loss: 32.8611 - val_loss: 41.5910\nEpoch 37/50\n - 0s - loss: 32.7992 - val_loss: 41.4203\nEpoch 38/50\n - 0s - loss: 32.7262 - val_loss: 41.6683\nEpoch 39/50\n - 0s - loss: 32.8366 - val_loss: 41.4552\nEpoch 40/50\n - 0s - loss: 32.8950 - val_loss: 41.8117\nEpoch 41/50\n - 0s - loss: 32.7955 - val_loss: 41.3747\nEpoch 42/50\n - 0s - loss: 32.7685 - val_loss: 41.2557\nEpoch 43/50\n - 0s - loss: 32.7561 - val_loss: 41.3831\nEpoch 44/50\n - 0s - loss: 32.7551 - val_loss: 41.1741\nEpoch 45/50\n - 0s - loss: 32.8355 - val_loss: 41.5669\nEpoch 46/50\n - 0s - loss: 32.7893 - val_loss: 41.3626\nEpoch 47/50\n - 0s - loss: 32.7595 - val_loss: 41.4094\nEpoch 48/50\n - 0s - loss: 32.8179 - val_loss: 41.4294\nEpoch 49/50\n - 0s - loss: 32.9077 - val_loss: 41.4643\nEpoch 50/50\n - 0s - loss: 32.7810 - val_loss: 41.3890\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.9729 - val_loss: 41.3354\nEpoch 2/50\n - 0s - loss: 32.7932 - val_loss: 41.4215\nEpoch 3/50\n - 0s - loss: 32.7229 - val_loss: 41.5029\nEpoch 4/50\n - 0s - loss: 32.7867 - val_loss: 41.1977\nEpoch 5/50\n - 0s - loss: 32.7848 - val_loss: 41.5784\nEpoch 6/50\n - 0s - loss: 32.8067 - val_loss: 41.5723\nEpoch 7/50\n - 0s - loss: 32.9031 - val_loss: 41.2599\nEpoch 8/50\n - 0s - loss: 32.7330 - val_loss: 41.2552\nEpoch 9/50\n - 0s - loss: 32.7606 - val_loss: 41.3831\nEpoch 10/50\n - 0s - loss: 32.8337 - val_loss: 41.4706\nEpoch 11/50\n - 0s - loss: 32.8145 - val_loss: 41.5756\nEpoch 12/50\n - 0s - loss: 32.7290 - val_loss: 41.2330\nEpoch 13/50\n - 0s - loss: 32.7604 - val_loss: 41.6101\nEpoch 14/50\n - 0s - loss: 32.7388 - val_loss: 41.3809\nEpoch 15/50\n - 0s - loss: 32.7507 - val_loss: 41.2526\nEpoch 16/50\n - 0s - loss: 32.7779 - val_loss: 41.4980\nEpoch 17/50\n - 0s - loss: 32.7814 - val_loss: 41.6058\nEpoch 18/50\n - 0s - loss: 32.7242 - val_loss: 41.1060\nEpoch 19/50\n - 0s - loss: 32.7613 - val_loss: 41.2797\nEpoch 20/50\n - 0s - loss: 32.7607 - val_loss: 41.4958\nEpoch 21/50\n - 0s - loss: 32.7884 - val_loss: 41.3966\nEpoch 22/50\n - 0s - loss: 32.7760 - val_loss: 41.7662\nEpoch 23/50\n - 0s - loss: 32.7411 - val_loss: 41.3940\nEpoch 24/50\n - 0s - loss: 32.7771 - val_loss: 41.0796\nEpoch 25/50\n - 0s - loss: 32.8817 - val_loss: 41.4173\nEpoch 26/50\n - 0s - loss: 32.7483 - val_loss: 41.3491\nEpoch 27/50\n - 0s - loss: 32.8217 - val_loss: 41.5246\nEpoch 28/50\n - 0s - loss: 32.7431 - val_loss: 41.2941\nEpoch 29/50\n - 0s - loss: 32.7071 - val_loss: 41.2778\nEpoch 30/50\n - 0s - loss: 32.6962 - val_loss: 41.3591\nEpoch 31/50\n - 0s - loss: 32.7831 - val_loss: 41.5305\nEpoch 32/50\n - 0s - loss: 32.8941 - val_loss: 41.7534\nEpoch 33/50\n - 0s - loss: 32.8571 - val_loss: 41.1428\nEpoch 34/50\n - 0s - loss: 32.7177 - val_loss: 41.5420\nEpoch 35/50\n - 0s - loss: 32.7286 - val_loss: 41.4007\nEpoch 36/50\n - 0s - loss: 32.8034 - val_loss: 41.5110\nEpoch 37/50\n - 0s - loss: 32.7694 - val_loss: 41.2230\nEpoch 38/50\n - 0s - loss: 32.8226 - val_loss: 41.3579\nEpoch 39/50\n - 0s - loss: 32.8176 - val_loss: 41.5870\nEpoch 40/50\n - 0s - loss: 32.7637 - val_loss: 41.2304\nEpoch 41/50\n - 0s - loss: 32.7653 - val_loss: 41.3889\nEpoch 42/50\n - 0s - loss: 32.7197 - val_loss: 41.4431\nEpoch 43/50\n - 0s - loss: 32.7489 - val_loss: 41.5244\nEpoch 44/50\n - 0s - loss: 32.8607 - val_loss: 41.3532\nEpoch 45/50\n - 0s - loss: 32.7401 - val_loss: 41.1549\nEpoch 46/50\n - 0s - loss: 32.7306 - val_loss: 41.5164\nEpoch 47/50\n - 0s - loss: 32.7127 - val_loss: 41.3058\nEpoch 48/50\n - 0s - loss: 32.8435 - val_loss: 41.1851\nEpoch 49/50\n - 0s - loss: 32.7014 - val_loss: 41.5096\nEpoch 50/50\n - 0s - loss: 32.8493 - val_loss: 41.7699\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.8262 - val_loss: 41.3067\nEpoch 2/50\n - 0s - loss: 32.6878 - val_loss: 41.5360\nEpoch 3/50\n - 0s - loss: 32.8140 - val_loss: 41.3449\nEpoch 4/50\n - 0s - loss: 32.7647 - val_loss: 41.6142\nEpoch 5/50\n - 0s - loss: 32.7487 - val_loss: 41.3695\nEpoch 6/50\n - 0s - loss: 32.7297 - val_loss: 41.4249\nEpoch 7/50\n - 0s - loss: 32.7719 - val_loss: 41.5529\nEpoch 8/50\n - 0s - loss: 32.8066 - val_loss: 41.4109\nEpoch 9/50\n - 0s - loss: 32.6662 - val_loss: 41.4716\nEpoch 10/50\n - 0s - loss: 32.7638 - val_loss: 41.4214\nEpoch 11/50\n - 0s - loss: 32.8355 - val_loss: 41.6715\nEpoch 12/50\n - 0s - loss: 32.7750 - val_loss: 41.0498\nEpoch 13/50\n - 0s - loss: 32.7574 - val_loss: 41.3123\nEpoch 14/50\n - 0s - loss: 32.6899 - val_loss: 41.3972\nEpoch 15/50\n - 0s - loss: 32.7336 - val_loss: 41.4015\nEpoch 16/50\n - 0s - loss: 32.6867 - val_loss: 41.5634\nEpoch 17/50\n - 0s - loss: 32.7095 - val_loss: 41.3419\nEpoch 18/50\n - 0s - loss: 32.7171 - val_loss: 41.4527\nEpoch 19/50\n - 0s - loss: 32.7008 - val_loss: 41.2766\nEpoch 20/50\n - 0s - loss: 32.7556 - val_loss: 41.3912\nEpoch 21/50\n - 0s - loss: 32.7693 - val_loss: 41.4843\nEpoch 22/50\n - 0s - loss: 32.6875 - val_loss: 41.4489\nEpoch 23/50\n - 0s - loss: 32.7174 - val_loss: 41.2779\nEpoch 24/50\n - 0s - loss: 32.7045 - val_loss: 41.1795\nEpoch 25/50\n - 0s - loss: 32.7208 - val_loss: 41.3907\nEpoch 26/50\n - 0s - loss: 32.7536 - val_loss: 41.3777\nEpoch 27/50\n - 0s - loss: 32.7113 - val_loss: 41.4278\nEpoch 28/50\n - 0s - loss: 32.7171 - val_loss: 41.4959\nEpoch 29/50\n - 0s - loss: 32.7069 - val_loss: 41.4230\nEpoch 30/50\n - 0s - loss: 32.6612 - val_loss: 41.4197\nEpoch 31/50\n - 0s - loss: 32.7093 - val_loss: 41.2038\nEpoch 32/50\n - 0s - loss: 32.6642 - val_loss: 41.3537\nEpoch 33/50\n - 0s - loss: 32.7273 - val_loss: 41.2273\nEpoch 34/50\n - 0s - loss: 32.6915 - val_loss: 41.4954\nEpoch 35/50\n - 0s - loss: 32.6996 - val_loss: 41.7571\nEpoch 36/50\n - 0s - loss: 32.6976 - val_loss: 41.2755\nEpoch 37/50\n - 0s - loss: 32.7121 - val_loss: 41.1883\nEpoch 38/50\n - 0s - loss: 33.0113 - val_loss: 41.4414\nEpoch 39/50\n - 0s - loss: 32.6197 - val_loss: 41.3673\nEpoch 40/50\n - 0s - loss: 32.7670 - val_loss: 41.3321\nEpoch 41/50\n - 0s - loss: 32.7590 - val_loss: 41.4634\nEpoch 42/50\n - 0s - loss: 32.6453 - val_loss: 41.4245\nEpoch 43/50\n - 0s - loss: 32.7087 - val_loss: 41.2334\nEpoch 44/50\n - 0s - loss: 32.7007 - val_loss: 41.2823\nEpoch 45/50\n - 0s - loss: 32.7042 - val_loss: 41.6457\nEpoch 46/50\n - 0s - loss: 32.7141 - val_loss: 41.4015\nEpoch 47/50\n - 0s - loss: 32.7028 - val_loss: 41.1070\nEpoch 48/50\n - 0s - loss: 32.6363 - val_loss: 41.2789\nEpoch 49/50\n - 0s - loss: 32.7230 - val_loss: 41.2696\nEpoch 50/50\n - 0s - loss: 32.7640 - val_loss: 41.6370\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.6625 - val_loss: 41.3957\nEpoch 2/50\n - 0s - loss: 32.6659 - val_loss: 41.5953\nEpoch 3/50\n - 0s - loss: 32.6202 - val_loss: 41.1074\nEpoch 4/50\n - 0s - loss: 32.6808 - val_loss: 41.2080\nEpoch 5/50\n - 0s - loss: 32.7005 - val_loss: 41.3077\nEpoch 6/50\n - 0s - loss: 32.6910 - val_loss: 41.6391\nEpoch 7/50\n - 0s - loss: 32.6138 - val_loss: 41.3185\nEpoch 8/50\n - 0s - loss: 32.7388 - val_loss: 41.5248\nEpoch 9/50\n - 0s - loss: 32.6749 - val_loss: 41.2904\nEpoch 10/50\n - 0s - loss: 32.6890 - val_loss: 41.3348\nEpoch 11/50\n - 0s - loss: 32.6287 - val_loss: 41.2607\nEpoch 12/50\n - 0s - loss: 32.6417 - val_loss: 41.2149\nEpoch 13/50\n - 0s - loss: 32.6768 - val_loss: 41.1391\nEpoch 14/50\n - 0s - loss: 32.6833 - val_loss: 41.1757\nEpoch 15/50\n - 0s - loss: 32.6610 - val_loss: 41.5376\nEpoch 16/50\n - 0s - loss: 32.7382 - val_loss: 41.2854\nEpoch 17/50\n - 0s - loss: 32.7977 - val_loss: 41.3260\nEpoch 18/50\n - 0s - loss: 32.5943 - val_loss: 41.3376\nEpoch 19/50\n - 0s - loss: 32.6704 - val_loss: 41.5090\nEpoch 20/50\n - 0s - loss: 32.6773 - val_loss: 41.2793\nEpoch 21/50\n - 0s - loss: 32.7269 - val_loss: 41.2045\nEpoch 22/50\n - 0s - loss: 32.6640 - val_loss: 41.0188\nEpoch 23/50\n - 0s - loss: 32.6567 - val_loss: 41.3226\nEpoch 24/50\n - 0s - loss: 32.7156 - val_loss: 41.3423\nEpoch 25/50\n - 0s - loss: 32.7316 - val_loss: 41.3983\nEpoch 26/50\n - 0s - loss: 32.6260 - val_loss: 41.4296\nEpoch 27/50\n - 0s - loss: 32.6182 - val_loss: 41.2017\nEpoch 28/50\n - 0s - loss: 32.6400 - val_loss: 41.2244\nEpoch 29/50\n - 0s - loss: 32.7028 - val_loss: 41.1323\nEpoch 30/50\n - 0s - loss: 32.6810 - val_loss: 41.2896\nEpoch 31/50\n - 0s - loss: 32.7900 - val_loss: 41.7001\nEpoch 32/50\n - 0s - loss: 32.6499 - val_loss: 41.3684\nEpoch 33/50\n - 0s - loss: 32.7325 - val_loss: 41.2644\nEpoch 34/50\n - 0s - loss: 32.6650 - val_loss: 41.1492\nEpoch 35/50\n - 0s - loss: 32.6733 - val_loss: 41.5439\nEpoch 36/50\n - 0s - loss: 32.6899 - val_loss: 41.3001\nEpoch 37/50\n - 0s - loss: 32.6816 - val_loss: 41.5229\nEpoch 38/50\n - 0s - loss: 32.8880 - val_loss: 41.0694\nEpoch 39/50\n - 0s - loss: 32.6965 - val_loss: 41.6042\nEpoch 40/50\n - 0s - loss: 32.6752 - val_loss: 40.9631\nEpoch 41/50\n - 0s - loss: 32.7885 - val_loss: 41.7202\nEpoch 42/50\n - 0s - loss: 32.6549 - val_loss: 41.1859\nEpoch 43/50\n - 0s - loss: 32.6238 - val_loss: 41.4204\nEpoch 44/50\n - 0s - loss: 32.6195 - val_loss: 41.1203\nEpoch 45/50\n - 0s - loss: 32.6253 - val_loss: 41.2112\nEpoch 46/50\n - 0s - loss: 32.6555 - val_loss: 41.2070\nEpoch 47/50\n - 0s - loss: 32.7223 - val_loss: 41.2210\nEpoch 48/50\n - 0s - loss: 32.6222 - val_loss: 41.4419\nEpoch 49/50\n - 0s - loss: 32.6541 - val_loss: 41.1253\nEpoch 50/50\n - 0s - loss: 32.6475 - val_loss: 41.4681\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.6783 - val_loss: 41.2891\nEpoch 2/50\n - 0s - loss: 32.5886 - val_loss: 41.1422\nEpoch 3/50\n - 0s - loss: 32.6542 - val_loss: 41.1242\nEpoch 4/50\n - 0s - loss: 32.6687 - val_loss: 41.3271\nEpoch 5/50\n - 0s - loss: 32.6086 - val_loss: 41.2263\nEpoch 6/50\n - 0s - loss: 32.6331 - val_loss: 41.0010\nEpoch 7/50\n - 0s - loss: 32.5664 - val_loss: 41.3121\nEpoch 8/50\n - 0s - loss: 32.6752 - val_loss: 41.4943\nEpoch 9/50\n - 0s - loss: 32.6098 - val_loss: 41.2623\nEpoch 10/50\n - 0s - loss: 32.7600 - val_loss: 41.7443\nEpoch 11/50\n - 0s - loss: 32.7815 - val_loss: 41.0094\nEpoch 12/50\n - 0s - loss: 32.7418 - val_loss: 41.4737\nEpoch 13/50\n - 0s - loss: 32.6236 - val_loss: 41.4346\nEpoch 14/50\n - 0s - loss: 32.6645 - val_loss: 41.1952\nEpoch 15/50\n - 0s - loss: 32.5764 - val_loss: 41.2802\nEpoch 16/50\n - 0s - loss: 32.5980 - val_loss: 41.2128\nEpoch 17/50\n - 0s - loss: 32.6125 - val_loss: 41.3135\nEpoch 18/50\n - 0s - loss: 32.5754 - val_loss: 41.3202\nEpoch 19/50\n - 0s - loss: 32.6575 - val_loss: 41.2400\nEpoch 20/50\n - 0s - loss: 32.7038 - val_loss: 41.4055\nEpoch 21/50\n - 0s - loss: 32.6096 - val_loss: 41.1190\nEpoch 22/50\n - 0s - loss: 32.6502 - val_loss: 40.9888\nEpoch 23/50\n - 0s - loss: 32.6257 - val_loss: 41.3067\nEpoch 24/50\n - 0s - loss: 32.6651 - val_loss: 41.1167\nEpoch 25/50\n - 0s - loss: 32.7194 - val_loss: 41.1940\nEpoch 26/50\n - 0s - loss: 32.8085 - val_loss: 41.3629\nEpoch 27/50\n - 0s - loss: 32.6512 - val_loss: 40.9740\nEpoch 28/50\n - 0s - loss: 32.6094 - val_loss: 41.4014\nEpoch 29/50\n - 0s - loss: 32.5605 - val_loss: 41.1574\nEpoch 30/50\n - 0s - loss: 32.6170 - val_loss: 40.8813\nEpoch 31/50\n - 0s - loss: 32.6236 - val_loss: 41.4460\nEpoch 32/50\n - 0s - loss: 32.6425 - val_loss: 41.2954\nEpoch 33/50\n - 0s - loss: 32.6032 - val_loss: 41.3613\nEpoch 34/50\n - 0s - loss: 32.6630 - val_loss: 41.3135\nEpoch 35/50\n - 0s - loss: 32.6362 - val_loss: 41.2887\nEpoch 36/50\n - 0s - loss: 32.6468 - val_loss: 41.3643\nEpoch 37/50\n - 0s - loss: 32.6602 - val_loss: 41.1951\nEpoch 38/50\n - 0s - loss: 32.6015 - val_loss: 41.2000\nEpoch 39/50\n - 0s - loss: 32.6285 - val_loss: 41.3276\nEpoch 40/50\n - 0s - loss: 32.7651 - val_loss: 40.9613\nEpoch 41/50\n - 0s - loss: 32.5788 - val_loss: 41.4323\nEpoch 42/50\n - 0s - loss: 32.6374 - val_loss: 41.1713\nEpoch 43/50\n - 0s - loss: 32.6212 - val_loss: 41.2861\nEpoch 44/50\n - 0s - loss: 32.6516 - val_loss: 41.0386\nEpoch 45/50\n - 0s - loss: 32.6045 - val_loss: 41.3619\nEpoch 46/50\n - 0s - loss: 32.5493 - val_loss: 41.2068\nEpoch 47/50\n - 0s - loss: 32.5945 - val_loss: 41.0811\nEpoch 48/50\n - 0s - loss: 32.7979 - val_loss: 41.0142\nEpoch 49/50\n - 0s - loss: 32.6656 - val_loss: 41.4998\nEpoch 50/50\n - 0s - loss: 32.6567 - val_loss: 41.2854\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.5483 - val_loss: 41.0063\nEpoch 2/50\n - 0s - loss: 32.6046 - val_loss: 41.2303\nEpoch 3/50\n - 0s - loss: 32.5752 - val_loss: 41.0792\nEpoch 4/50\n - 0s - loss: 32.5901 - val_loss: 41.2005\nEpoch 5/50\n - 0s - loss: 32.5650 - val_loss: 41.3010\nEpoch 6/50\n - 0s - loss: 32.5406 - val_loss: 41.1803\nEpoch 7/50\n - 0s - loss: 32.6207 - val_loss: 40.9820\nEpoch 8/50\n - 0s - loss: 32.5113 - val_loss: 41.4465\nEpoch 9/50\n - 0s - loss: 32.7211 - val_loss: 41.2543\nEpoch 10/50\n - 0s - loss: 32.5546 - val_loss: 41.2091\nEpoch 11/50\n - 0s - loss: 32.5486 - val_loss: 41.2179\nEpoch 12/50\n - 0s - loss: 32.5957 - val_loss: 41.3910\nEpoch 13/50\n - 0s - loss: 32.6020 - val_loss: 41.1265\nEpoch 14/50\n - 0s - loss: 32.5688 - val_loss: 41.2019\nEpoch 15/50\n - 0s - loss: 32.5453 - val_loss: 41.1229\nEpoch 16/50\n - 0s - loss: 32.6670 - val_loss: 41.4347\nEpoch 17/50\n - 0s - loss: 32.5332 - val_loss: 41.0440\nEpoch 18/50\n - 0s - loss: 32.7121 - val_loss: 41.1082\nEpoch 19/50\n - 0s - loss: 32.6932 - val_loss: 41.2791\nEpoch 20/50\n - 0s - loss: 32.6025 - val_loss: 41.0227\nEpoch 21/50\n - 0s - loss: 32.5828 - val_loss: 40.9792\nEpoch 22/50\n - 0s - loss: 32.6634 - val_loss: 41.2130\nEpoch 23/50\n - 0s - loss: 32.7163 - val_loss: 41.3298\nEpoch 24/50\n - 0s - loss: 32.6397 - val_loss: 41.3891\nEpoch 25/50\n - 0s - loss: 32.4951 - val_loss: 40.8760\nEpoch 26/50\n - 0s - loss: 32.5434 - val_loss: 40.9858\nEpoch 27/50\n - 0s - loss: 32.5079 - val_loss: 41.1781\nEpoch 28/50\n - 0s - loss: 32.7044 - val_loss: 41.2429\nEpoch 29/50\n - 0s - loss: 32.6095 - val_loss: 41.5070\nEpoch 30/50\n - 0s - loss: 32.5765 - val_loss: 40.9577\nEpoch 31/50\n - 0s - loss: 32.6594 - val_loss: 41.3814\nEpoch 32/50\n - 0s - loss: 32.5564 - val_loss: 41.2163\nEpoch 33/50\n - 0s - loss: 32.5254 - val_loss: 41.2332\nEpoch 34/50\n - 0s - loss: 32.5512 - val_loss: 41.2187\nEpoch 35/50\n - 0s - loss: 32.5099 - val_loss: 41.2402\nEpoch 36/50\n - 0s - loss: 32.5306 - val_loss: 41.1315\nEpoch 37/50\n - 0s - loss: 32.5451 - val_loss: 41.1495\nEpoch 38/50\n - 0s - loss: 32.6225 - val_loss: 41.4649\nEpoch 39/50\n - 0s - loss: 32.7596 - val_loss: 41.0993\nEpoch 40/50\n - 0s - loss: 32.6059 - val_loss: 41.0538\nEpoch 41/50\n - 0s - loss: 32.6650 - val_loss: 41.0650\nEpoch 42/50\n - 0s - loss: 32.7288 - val_loss: 41.1756\nEpoch 43/50\n - 0s - loss: 32.5280 - val_loss: 41.2840\nEpoch 44/50\n - 0s - loss: 32.5341 - val_loss: 41.0987\nEpoch 45/50\n - 0s - loss: 32.5419 - val_loss: 41.2459\nEpoch 46/50\n - 0s - loss: 32.5249 - val_loss: 41.0792\nEpoch 47/50\n - 0s - loss: 32.5618 - val_loss: 41.0302\nEpoch 48/50\n - 0s - loss: 32.5815 - val_loss: 41.5345\nEpoch 49/50\n - 0s - loss: 32.6400 - val_loss: 41.3049\nEpoch 50/50\n - 0s - loss: 32.5174 - val_loss: 40.9267\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.5882 - val_loss: 41.2342\nEpoch 2/50\n - 0s - loss: 32.5277 - val_loss: 41.3482\nEpoch 3/50\n - 0s - loss: 32.5294 - val_loss: 41.1873\nEpoch 4/50\n - 0s - loss: 32.5417 - val_loss: 41.1004\nEpoch 5/50\n - 0s - loss: 32.5484 - val_loss: 41.1713\nEpoch 6/50\n - 0s - loss: 32.5519 - val_loss: 41.3455\nEpoch 7/50\n - 0s - loss: 32.5418 - val_loss: 41.6387\nEpoch 8/50\n - 0s - loss: 32.5397 - val_loss: 41.2248\nEpoch 9/50\n - 0s - loss: 32.5536 - val_loss: 41.0076\nEpoch 10/50\n - 0s - loss: 32.5721 - val_loss: 41.0521\nEpoch 11/50\n - 0s - loss: 32.6053 - val_loss: 41.3657\nEpoch 12/50\n - 0s - loss: 32.4967 - val_loss: 40.9568\nEpoch 13/50\n - 0s - loss: 32.5675 - val_loss: 41.1586\nEpoch 14/50\n - 0s - loss: 32.4766 - val_loss: 41.1252\nEpoch 15/50\n - 0s - loss: 32.5640 - val_loss: 41.3299\nEpoch 16/50\n - 0s - loss: 32.4687 - val_loss: 41.0711\nEpoch 17/50\n - 0s - loss: 32.4797 - val_loss: 41.0849\nEpoch 18/50\n - 0s - loss: 32.4669 - val_loss: 41.0820\nEpoch 19/50\n - 0s - loss: 32.4770 - val_loss: 41.1738\nEpoch 20/50\n - 0s - loss: 32.5309 - val_loss: 41.2347\nEpoch 21/50\n - 0s - loss: 32.5800 - val_loss: 41.2083\nEpoch 22/50\n - 0s - loss: 32.5223 - val_loss: 41.1643\nEpoch 23/50\n - 0s - loss: 32.4953 - val_loss: 41.1102\nEpoch 24/50\n - 0s - loss: 32.5163 - val_loss: 41.1535\nEpoch 25/50\n - 0s - loss: 32.5485 - val_loss: 41.3602\nEpoch 26/50\n - 0s - loss: 32.5450 - val_loss: 41.0068\nEpoch 27/50\n - 0s - loss: 32.5866 - val_loss: 41.1509\nEpoch 28/50\n - 0s - loss: 32.4788 - val_loss: 41.3284\nEpoch 29/50\n - 0s - loss: 32.5736 - val_loss: 41.2889\nEpoch 30/50\n - 0s - loss: 32.4984 - val_loss: 41.0177\nEpoch 31/50\n - 0s - loss: 32.5008 - val_loss: 41.0083\nEpoch 32/50\n - 0s - loss: 32.4983 - val_loss: 41.3622\nEpoch 33/50\n - 0s - loss: 32.4757 - val_loss: 41.2165\nEpoch 34/50\n - 0s - loss: 32.5818 - val_loss: 40.9011\nEpoch 35/50\n - 0s - loss: 32.5183 - val_loss: 41.0593\nEpoch 36/50\n - 0s - loss: 32.5338 - val_loss: 41.2324\nEpoch 37/50\n - 0s - loss: 32.5057 - val_loss: 41.4148\nEpoch 38/50\n - 0s - loss: 32.5261 - val_loss: 40.9387\nEpoch 39/50\n - 0s - loss: 32.6171 - val_loss: 41.4580\nEpoch 40/50\n - 0s - loss: 32.5888 - val_loss: 41.1548\nEpoch 41/50\n - 0s - loss: 32.4865 - val_loss: 40.8981\nEpoch 42/50\n - 0s - loss: 32.5067 - val_loss: 41.1174\nEpoch 43/50\n - 0s - loss: 32.6564 - val_loss: 41.0685\nEpoch 44/50\n - 0s - loss: 32.5440 - val_loss: 41.2894\nEpoch 45/50\n - 0s - loss: 32.6030 - val_loss: 41.1188\nEpoch 46/50\n - 0s - loss: 32.5066 - val_loss: 41.3272\nEpoch 47/50\n - 0s - loss: 32.5106 - val_loss: 41.3145\nEpoch 48/50\n - 0s - loss: 32.5355 - val_loss: 41.3465\nEpoch 49/50\n - 0s - loss: 32.4914 - val_loss: 41.2747\nEpoch 50/50\n - 0s - loss: 32.4972 - val_loss: 41.0684\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.5331 - val_loss: 41.1611\nEpoch 2/50\n - 0s - loss: 32.5244 - val_loss: 41.2411\nEpoch 3/50\n - 0s - loss: 32.5456 - val_loss: 41.2667\nEpoch 4/50\n - 0s - loss: 32.6405 - val_loss: 40.8484\nEpoch 5/50\n - 0s - loss: 32.4768 - val_loss: 41.4332\nEpoch 6/50\n - 0s - loss: 32.5466 - val_loss: 41.1761\nEpoch 7/50\n - 0s - loss: 32.5266 - val_loss: 41.1467\nEpoch 8/50\n - 0s - loss: 32.5556 - val_loss: 41.0237\nEpoch 9/50\n - 0s - loss: 32.5383 - val_loss: 41.1720\nEpoch 10/50\n - 0s - loss: 32.4982 - val_loss: 41.0726\nEpoch 11/50\n - 0s - loss: 32.5819 - val_loss: 41.4082\nEpoch 12/50\n - 0s - loss: 32.5552 - val_loss: 41.0143\nEpoch 13/50\n - 0s - loss: 32.5815 - val_loss: 41.4245\nEpoch 14/50\n - 0s - loss: 32.4347 - val_loss: 41.3000\nEpoch 15/50\n - 0s - loss: 32.5660 - val_loss: 41.3164\nEpoch 16/50\n - 0s - loss: 32.4526 - val_loss: 41.1504\nEpoch 17/50\n - 0s - loss: 32.4631 - val_loss: 41.1997\nEpoch 18/50\n - 0s - loss: 32.4494 - val_loss: 41.0411\nEpoch 19/50\n - 0s - loss: 32.5540 - val_loss: 41.3594\nEpoch 20/50\n - 0s - loss: 32.4714 - val_loss: 41.1184\nEpoch 21/50\n - 0s - loss: 32.7242 - val_loss: 41.1710\nEpoch 22/50\n - 0s - loss: 32.4873 - val_loss: 41.4363\nEpoch 23/50\n - 0s - loss: 32.4457 - val_loss: 41.2224\nEpoch 24/50\n - 0s - loss: 32.4794 - val_loss: 41.0326\nEpoch 25/50\n - 0s - loss: 32.6159 - val_loss: 40.9938\nEpoch 26/50\n - 0s - loss: 32.4471 - val_loss: 41.3005\nEpoch 27/50\n - 0s - loss: 32.5237 - val_loss: 41.3602\nEpoch 28/50\n - 0s - loss: 32.4721 - val_loss: 41.2405\nEpoch 29/50\n - 0s - loss: 32.6069 - val_loss: 41.0484\nEpoch 30/50\n - 0s - loss: 32.4266 - val_loss: 41.3199\nEpoch 31/50\n - 0s - loss: 32.5526 - val_loss: 41.6343\nEpoch 32/50\n - 0s - loss: 32.6080 - val_loss: 40.8714\nEpoch 33/50\n - 0s - loss: 32.5137 - val_loss: 41.2768\nEpoch 34/50\n - 0s - loss: 32.5808 - val_loss: 41.0475\nEpoch 35/50\n - 0s - loss: 32.4460 - val_loss: 41.3874\nEpoch 36/50\n - 0s - loss: 32.4690 - val_loss: 41.3978\nEpoch 37/50\n - 0s - loss: 32.4716 - val_loss: 41.1857\nEpoch 38/50\n - 0s - loss: 32.4976 - val_loss: 40.9787\nEpoch 39/50\n - 0s - loss: 32.4400 - val_loss: 41.1516\nEpoch 40/50\n - 0s - loss: 32.4808 - val_loss: 41.1816\nEpoch 41/50\n - 0s - loss: 32.5191 - val_loss: 41.1487\nEpoch 42/50\n - 0s - loss: 32.6962 - val_loss: 41.6838\nEpoch 43/50\n - 0s - loss: 32.5507 - val_loss: 41.0764\nEpoch 44/50\n - 0s - loss: 32.4609 - val_loss: 41.0652\nEpoch 45/50\n - 0s - loss: 32.4735 - val_loss: 41.3207\nEpoch 46/50\n - 0s - loss: 32.6129 - val_loss: 41.1675\nEpoch 47/50\n - 0s - loss: 32.4752 - val_loss: 41.3800\nEpoch 48/50\n - 0s - loss: 32.5269 - val_loss: 41.0747\nEpoch 49/50\n - 0s - loss: 32.6038 - val_loss: 41.2867\nEpoch 50/50\n - 0s - loss: 32.5809 - val_loss: 41.2012\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.4385 - val_loss: 41.0840\nEpoch 2/50\n - 0s - loss: 32.5253 - val_loss: 41.2142\nEpoch 3/50\n - 0s - loss: 32.4070 - val_loss: 41.1302\nEpoch 4/50\n - 0s - loss: 32.5141 - val_loss: 41.3678\nEpoch 5/50\n - 0s - loss: 32.4773 - val_loss: 40.8967\nEpoch 6/50\n - 0s - loss: 32.5428 - val_loss: 41.1514\nEpoch 7/50\n - 0s - loss: 32.6007 - val_loss: 40.7269\nEpoch 8/50\n - 0s - loss: 32.4236 - val_loss: 41.1339\nEpoch 9/50\n - 0s - loss: 32.4322 - val_loss: 41.2719\nEpoch 10/50\n - 0s - loss: 32.4798 - val_loss: 40.9954\nEpoch 11/50\n - 0s - loss: 32.5109 - val_loss: 41.2077\nEpoch 12/50\n - 0s - loss: 32.5336 - val_loss: 41.2986\nEpoch 13/50\n - 0s - loss: 32.4453 - val_loss: 41.2499\nEpoch 14/50\n - 0s - loss: 32.5440 - val_loss: 41.3752\nEpoch 15/50\n - 0s - loss: 32.4550 - val_loss: 41.0991\nEpoch 16/50\n - 0s - loss: 32.5271 - val_loss: 40.9362\nEpoch 17/50\n - 0s - loss: 32.5742 - val_loss: 41.5042\nEpoch 18/50\n - 0s - loss: 32.4541 - val_loss: 41.1675\nEpoch 19/50\n - 0s - loss: 32.5058 - val_loss: 41.4701\nEpoch 20/50\n - 0s - loss: 32.4124 - val_loss: 41.1828\nEpoch 21/50\n - 0s - loss: 32.4517 - val_loss: 40.9819\nEpoch 22/50\n - 0s - loss: 32.4725 - val_loss: 41.1579\nEpoch 23/50\n - 0s - loss: 32.4727 - val_loss: 41.1793\nEpoch 24/50\n - 0s - loss: 32.4474 - val_loss: 41.2989\nEpoch 25/50\n - 0s - loss: 32.6474 - val_loss: 41.0617\nEpoch 26/50\n - 0s - loss: 32.4181 - val_loss: 41.3215\nEpoch 27/50\n - 0s - loss: 32.5876 - val_loss: 41.0063\nEpoch 28/50\n - 0s - loss: 32.4590 - val_loss: 41.3134\nEpoch 29/50\n - 0s - loss: 32.5352 - val_loss: 41.1214\nEpoch 30/50\n - 0s - loss: 32.4231 - val_loss: 41.1669\nEpoch 31/50\n - 0s - loss: 32.4581 - val_loss: 41.2226\nEpoch 32/50\n - 0s - loss: 32.4466 - val_loss: 41.1277\nEpoch 33/50\n - 0s - loss: 32.4540 - val_loss: 41.1031\nEpoch 34/50\n - 0s - loss: 32.4437 - val_loss: 40.9450\nEpoch 35/50\n - 0s - loss: 32.4358 - val_loss: 41.2065\nEpoch 36/50\n - 0s - loss: 32.4401 - val_loss: 41.1039\nEpoch 37/50\n - 0s - loss: 32.4491 - val_loss: 41.0573\nEpoch 38/50\n - 0s - loss: 32.6704 - val_loss: 41.3392\nEpoch 39/50\n - 0s - loss: 32.2884 - val_loss: 40.9246\nEpoch 40/50\n - 0s - loss: 32.5695 - val_loss: 41.0165\nEpoch 41/50\n - 0s - loss: 32.4187 - val_loss: 41.0819\nEpoch 42/50\n - 0s - loss: 32.4030 - val_loss: 41.4644\nEpoch 43/50\n - 0s - loss: 32.4597 - val_loss: 41.3339\nEpoch 44/50\n - 0s - loss: 32.3893 - val_loss: 41.0951\nEpoch 45/50\n - 0s - loss: 32.4203 - val_loss: 41.1711\nEpoch 46/50\n - 0s - loss: 32.4417 - val_loss: 41.1493\nEpoch 47/50\n - 0s - loss: 32.4943 - val_loss: 40.9111\nEpoch 48/50\n - 0s - loss: 32.4515 - val_loss: 41.3335\nEpoch 49/50\n - 0s - loss: 32.4447 - val_loss: 41.1259\nEpoch 50/50\n - 0s - loss: 32.4921 - val_loss: 41.1355\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.4552 - val_loss: 41.0263\nEpoch 2/50\n - 0s - loss: 32.5017 - val_loss: 41.0244\nEpoch 3/50\n - 0s - loss: 32.4700 - val_loss: 41.4591\nEpoch 4/50\n - 0s - loss: 32.5967 - val_loss: 41.1376\nEpoch 5/50\n - 0s - loss: 32.4769 - val_loss: 41.0679\nEpoch 6/50\n - 0s - loss: 32.4446 - val_loss: 41.2989\nEpoch 7/50\n - 0s - loss: 32.5501 - val_loss: 41.1936\nEpoch 8/50\n - 0s - loss: 32.4355 - val_loss: 41.3139\nEpoch 9/50\n - 0s - loss: 32.4404 - val_loss: 40.9858\nEpoch 10/50\n - 0s - loss: 32.6226 - val_loss: 41.2977\nEpoch 11/50\n - 0s - loss: 32.5490 - val_loss: 41.0166\nEpoch 12/50\n - 0s - loss: 32.4212 - val_loss: 41.2054\nEpoch 13/50\n - 0s - loss: 32.4406 - val_loss: 41.1017\nEpoch 14/50\n - 0s - loss: 32.4440 - val_loss: 41.4082\nEpoch 15/50\n - 0s - loss: 32.5257 - val_loss: 40.9601\nEpoch 16/50\n - 0s - loss: 32.4447 - val_loss: 41.2317\nEpoch 17/50\n - 0s - loss: 32.5220 - val_loss: 41.4044\nEpoch 18/50\n - 0s - loss: 32.4481 - val_loss: 41.2702\nEpoch 19/50\n - 0s - loss: 32.5112 - val_loss: 41.1737\nEpoch 20/50\n - 0s - loss: 32.4442 - val_loss: 40.9350\nEpoch 21/50\n - 0s - loss: 32.4030 - val_loss: 41.3433\nEpoch 22/50\n - 0s - loss: 32.5585 - val_loss: 41.2126\nEpoch 23/50\n - 0s - loss: 32.4668 - val_loss: 41.1812\nEpoch 24/50\n - 0s - loss: 32.5260 - val_loss: 41.2394\nEpoch 25/50\n - 0s - loss: 32.4666 - val_loss: 40.9091\nEpoch 26/50\n - 0s - loss: 32.4427 - val_loss: 41.2242\nEpoch 27/50\n - 0s - loss: 32.5033 - val_loss: 41.2914\nEpoch 28/50\n - 0s - loss: 32.4288 - val_loss: 41.2086\nEpoch 29/50\n - 0s - loss: 32.4690 - val_loss: 41.1699\nEpoch 30/50\n - 0s - loss: 32.3956 - val_loss: 40.8667\nEpoch 31/50\n - 0s - loss: 32.4642 - val_loss: 41.1108\nEpoch 32/50\n - 0s - loss: 32.5092 - val_loss: 41.1988\nEpoch 33/50\n - 0s - loss: 32.5009 - val_loss: 41.0053\nEpoch 34/50\n - 0s - loss: 32.4688 - val_loss: 41.1122\nEpoch 35/50\n - 0s - loss: 32.4975 - val_loss: 41.1562\nEpoch 36/50\n - 0s - loss: 32.5007 - val_loss: 41.3391\nEpoch 37/50\n - 0s - loss: 32.5570 - val_loss: 40.9291\nEpoch 38/50\n - 0s - loss: 32.5836 - val_loss: 41.4617\nEpoch 39/50\n - 0s - loss: 32.4558 - val_loss: 40.9878\nEpoch 40/50\n - 0s - loss: 32.6453 - val_loss: 41.1692\nEpoch 41/50\n - 0s - loss: 32.4719 - val_loss: 41.4032\nEpoch 42/50\n - 0s - loss: 32.4041 - val_loss: 41.0562\nEpoch 43/50\n - 0s - loss: 32.4872 - val_loss: 41.2746\nEpoch 44/50\n - 0s - loss: 32.4186 - val_loss: 41.0569\nEpoch 45/50\n - 0s - loss: 32.4319 - val_loss: 41.2980\nEpoch 46/50\n - 0s - loss: 32.4508 - val_loss: 41.1061\nEpoch 47/50\n - 0s - loss: 32.5196 - val_loss: 41.4055\nEpoch 48/50\n - 0s - loss: 32.5082 - val_loss: 41.0591\nEpoch 49/50\n - 0s - loss: 32.5206 - val_loss: 41.6546\nEpoch 50/50\n - 0s - loss: 32.4438 - val_loss: 41.0410\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.4649 - val_loss: 41.0646\nEpoch 2/50\n - 0s - loss: 32.4051 - val_loss: 41.1405\nEpoch 3/50\n - 0s - loss: 32.4810 - val_loss: 41.5477\nEpoch 4/50\n - 0s - loss: 32.4667 - val_loss: 41.0054\nEpoch 5/50\n - 0s - loss: 32.4073 - val_loss: 40.9747\nEpoch 6/50\n - 0s - loss: 32.4496 - val_loss: 41.0177\nEpoch 7/50\n - 0s - loss: 32.4216 - val_loss: 41.0247\nEpoch 8/50\n - 0s - loss: 32.3733 - val_loss: 41.4379\nEpoch 9/50\n - 0s - loss: 32.4535 - val_loss: 41.2792\nEpoch 10/50\n - 0s - loss: 32.3920 - val_loss: 41.3039\nEpoch 11/50\n - 0s - loss: 32.5161 - val_loss: 41.2603\nEpoch 12/50\n - 0s - loss: 32.4024 - val_loss: 41.2758\nEpoch 13/50\n - 0s - loss: 32.4590 - val_loss: 41.1104\nEpoch 14/50\n - 0s - loss: 32.5102 - val_loss: 41.1429\nEpoch 15/50\n - 0s - loss: 32.4316 - val_loss: 41.0162\nEpoch 16/50\n - 0s - loss: 32.4228 - val_loss: 41.1648\nEpoch 17/50\n - 0s - loss: 32.5461 - val_loss: 41.2251\nEpoch 18/50\n - 0s - loss: 32.4268 - val_loss: 41.0478\nEpoch 19/50\n - 0s - loss: 32.4675 - val_loss: 41.1512\nEpoch 20/50\n - 0s - loss: 32.4059 - val_loss: 40.9662\nEpoch 21/50\n - 0s - loss: 32.5534 - val_loss: 40.8976\nEpoch 22/50\n - 0s - loss: 32.4067 - val_loss: 41.2397\nEpoch 23/50\n - 0s - loss: 32.4101 - val_loss: 41.2568\nEpoch 24/50\n - 0s - loss: 32.4114 - val_loss: 41.1098\nEpoch 25/50\n - 0s - loss: 32.4461 - val_loss: 41.2983\nEpoch 26/50\n - 0s - loss: 32.6016 - val_loss: 41.2271\nEpoch 27/50\n - 0s - loss: 32.4070 - val_loss: 40.8314\nEpoch 28/50\n - 0s - loss: 32.4312 - val_loss: 41.1554\nEpoch 29/50\n - 0s - loss: 32.5390 - val_loss: 41.3215\nEpoch 30/50\n - 0s - loss: 32.4939 - val_loss: 41.1189\nEpoch 31/50\n - 0s - loss: 32.6310 - val_loss: 41.1772\nEpoch 32/50\n - 0s - loss: 32.4815 - val_loss: 41.2755\nEpoch 33/50\n - 0s - loss: 32.4562 - val_loss: 41.1277\nEpoch 34/50\n - 0s - loss: 32.4842 - val_loss: 41.4375\nEpoch 35/50\n - 0s - loss: 32.4718 - val_loss: 41.3727\nEpoch 36/50\n - 0s - loss: 32.5132 - val_loss: 41.1366\nEpoch 37/50\n - 0s - loss: 32.3805 - val_loss: 41.2893\nEpoch 38/50\n - 0s - loss: 32.4884 - val_loss: 41.1441\nEpoch 39/50\n - 0s - loss: 32.4110 - val_loss: 41.1501\nEpoch 40/50\n - 0s - loss: 32.4270 - val_loss: 41.0092\nEpoch 41/50\n - 0s - loss: 32.4779 - val_loss: 41.1078\nEpoch 42/50\n - 0s - loss: 32.4919 - val_loss: 41.7112\nEpoch 43/50\n - 0s - loss: 32.4241 - val_loss: 41.0419\nEpoch 44/50\n - 0s - loss: 32.5291 - val_loss: 41.1016\nEpoch 45/50\n - 0s - loss: 32.4019 - val_loss: 41.3049\nEpoch 46/50\n - 0s - loss: 32.5023 - val_loss: 41.1461\nEpoch 47/50\n - 0s - loss: 32.3719 - val_loss: 41.2548\nEpoch 48/50\n - 0s - loss: 32.4915 - val_loss: 41.1184\nEpoch 49/50\n - 0s - loss: 32.4037 - val_loss: 41.2688\nEpoch 50/50\n - 0s - loss: 32.6337 - val_loss: 41.4105\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.4647 - val_loss: 41.0261\nEpoch 2/50\n - 0s - loss: 32.3740 - val_loss: 41.0489\nEpoch 3/50\n - 0s - loss: 32.4077 - val_loss: 41.1802\nEpoch 4/50\n - 0s - loss: 32.4247 - val_loss: 40.9660\nEpoch 5/50\n - 0s - loss: 32.3768 - val_loss: 41.2496\nEpoch 6/50\n - 0s - loss: 32.4882 - val_loss: 41.1476\nEpoch 7/50\n - 0s - loss: 32.3964 - val_loss: 41.3459\nEpoch 8/50\n - 0s - loss: 32.4024 - val_loss: 41.1108\nEpoch 9/50\n - 0s - loss: 32.5155 - val_loss: 41.1374\nEpoch 10/50\n - 0s - loss: 32.3660 - val_loss: 41.4471\nEpoch 11/50\n - 0s - loss: 32.4909 - val_loss: 41.1356\nEpoch 12/50\n - 0s - loss: 32.3673 - val_loss: 41.2642\nEpoch 13/50\n - 0s - loss: 32.4025 - val_loss: 41.4138\nEpoch 14/50\n - 0s - loss: 32.4845 - val_loss: 41.3169\nEpoch 15/50\n - 0s - loss: 32.4878 - val_loss: 40.8918\nEpoch 16/50\n - 0s - loss: 32.4068 - val_loss: 41.5315\nEpoch 17/50\n - 0s - loss: 32.4247 - val_loss: 41.1400\nEpoch 18/50\n - 0s - loss: 32.4178 - val_loss: 41.2534\nEpoch 19/50\n - 0s - loss: 32.4264 - val_loss: 41.2055\nEpoch 20/50\n - 0s - loss: 32.4235 - val_loss: 41.1429\nEpoch 21/50\n - 0s - loss: 32.5098 - val_loss: 41.0291\nEpoch 22/50\n - 0s - loss: 32.3697 - val_loss: 41.2362\nEpoch 23/50\n - 0s - loss: 32.4336 - val_loss: 41.5644\nEpoch 24/50\n - 0s - loss: 32.3993 - val_loss: 41.3472\nEpoch 25/50\n - 0s - loss: 32.4201 - val_loss: 41.1660\nEpoch 26/50\n - 0s - loss: 32.3896 - val_loss: 41.1939\nEpoch 27/50\n - 0s - loss: 32.6595 - val_loss: 41.2082\nEpoch 28/50\n - 0s - loss: 32.5643 - val_loss: 40.9875\nEpoch 29/50\n - 0s - loss: 32.3895 - val_loss: 41.3172\nEpoch 30/50\n - 0s - loss: 32.3840 - val_loss: 41.3025\nEpoch 31/50\n - 0s - loss: 32.3837 - val_loss: 41.2540\nEpoch 32/50\n - 0s - loss: 32.5232 - val_loss: 40.9022\nEpoch 33/50\n - 0s - loss: 32.6181 - val_loss: 41.0318\nEpoch 34/50\n - 0s - loss: 32.4182 - val_loss: 41.3991\nEpoch 35/50\n - 0s - loss: 32.4081 - val_loss: 41.3060\nEpoch 36/50\n - 0s - loss: 32.4241 - val_loss: 41.1510\nEpoch 37/50\n - 0s - loss: 32.4764 - val_loss: 41.3953\nEpoch 38/50\n - 0s - loss: 32.5406 - val_loss: 40.9991\nEpoch 39/50\n - 0s - loss: 32.4713 - val_loss: 41.0811\nEpoch 40/50\n - 0s - loss: 32.3977 - val_loss: 41.4046\nEpoch 41/50\n - 0s - loss: 32.4157 - val_loss: 41.0686\nEpoch 42/50\n - 0s - loss: 32.4116 - val_loss: 41.3208\nEpoch 43/50\n - 0s - loss: 32.4535 - val_loss: 41.0826\nEpoch 44/50\n - 0s - loss: 32.3932 - val_loss: 41.1429\nEpoch 45/50\n - 0s - loss: 32.4373 - val_loss: 41.2612\nEpoch 46/50\n - 0s - loss: 32.4045 - val_loss: 41.3081\nEpoch 47/50\n - 0s - loss: 32.3920 - val_loss: 41.1329\nEpoch 48/50\n - 0s - loss: 32.6661 - val_loss: 41.0759\nEpoch 49/50\n - 0s - loss: 32.3783 - val_loss: 41.3146\nEpoch 50/50\n - 0s - loss: 32.3543 - val_loss: 41.3283\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3877 - val_loss: 41.3361\nEpoch 2/50\n - 0s - loss: 32.4145 - val_loss: 41.1933\nEpoch 3/50\n - 0s - loss: 32.4719 - val_loss: 41.0522\nEpoch 4/50\n - 0s - loss: 32.4160 - val_loss: 41.2833\nEpoch 5/50\n - 0s - loss: 32.4014 - val_loss: 41.0771\nEpoch 6/50\n - 0s - loss: 32.4128 - val_loss: 41.1791\nEpoch 7/50\n - 0s - loss: 32.3812 - val_loss: 41.1128\nEpoch 8/50\n - 0s - loss: 32.3744 - val_loss: 41.2259\nEpoch 9/50\n - 0s - loss: 32.3886 - val_loss: 41.2633\nEpoch 10/50\n - 0s - loss: 32.3831 - val_loss: 41.3493\nEpoch 11/50\n - 0s - loss: 32.4941 - val_loss: 41.1382\nEpoch 12/50\n - 0s - loss: 32.3702 - val_loss: 41.0710\nEpoch 13/50\n - 0s - loss: 32.4244 - val_loss: 41.3657\nEpoch 14/50\n - 0s - loss: 32.4302 - val_loss: 41.1589\nEpoch 15/50\n - 0s - loss: 32.4634 - val_loss: 41.2633\nEpoch 16/50\n - 0s - loss: 32.4489 - val_loss: 41.0498\nEpoch 17/50\n - 0s - loss: 32.4391 - val_loss: 41.2355\nEpoch 18/50\n - 0s - loss: 32.4182 - val_loss: 41.1266\nEpoch 19/50\n - 0s - loss: 32.6081 - val_loss: 40.9288\nEpoch 20/50\n - 0s - loss: 32.4230 - val_loss: 41.6347\nEpoch 21/50\n - 0s - loss: 32.3922 - val_loss: 41.2423\nEpoch 22/50\n - 0s - loss: 32.4322 - val_loss: 41.4011\nEpoch 23/50\n - 0s - loss: 32.3851 - val_loss: 40.9872\nEpoch 24/50\n - 0s - loss: 32.4547 - val_loss: 41.2548\nEpoch 25/50\n - 0s - loss: 32.4008 - val_loss: 41.0336\nEpoch 26/50\n - 0s - loss: 32.3612 - val_loss: 41.2193\nEpoch 27/50\n - 0s - loss: 32.7610 - val_loss: 41.0535\nEpoch 28/50\n - 0s - loss: 32.4320 - val_loss: 41.3516\nEpoch 29/50\n - 0s - loss: 32.4259 - val_loss: 41.2633\nEpoch 30/50\n - 0s - loss: 32.4125 - val_loss: 41.1188\nEpoch 31/50\n - 0s - loss: 32.4513 - val_loss: 41.2494\nEpoch 32/50\n - 0s - loss: 32.4262 - val_loss: 41.0230\nEpoch 33/50\n - 0s - loss: 32.5484 - val_loss: 41.2518\nEpoch 34/50\n - 0s - loss: 32.5371 - val_loss: 41.6070\nEpoch 35/50\n - 0s - loss: 32.7058 - val_loss: 41.1716\nEpoch 36/50\n - 0s - loss: 32.3986 - val_loss: 41.2469\nEpoch 37/50\n - 0s - loss: 32.4333 - val_loss: 41.5336\nEpoch 38/50\n - 0s - loss: 32.4590 - val_loss: 41.0485\nEpoch 39/50\n - 0s - loss: 32.3678 - val_loss: 41.0678\nEpoch 40/50\n - 0s - loss: 32.4286 - val_loss: 41.2045\nEpoch 41/50\n - 0s - loss: 32.5125 - val_loss: 41.0945\nEpoch 42/50\n - 0s - loss: 32.4125 - val_loss: 41.0702\nEpoch 43/50\n - 0s - loss: 32.3638 - val_loss: 41.5023\nEpoch 44/50\n - 0s - loss: 32.4967 - val_loss: 41.4408\nEpoch 45/50\n - 0s - loss: 32.4016 - val_loss: 41.2998\nEpoch 46/50\n - 0s - loss: 32.3560 - val_loss: 41.0444\nEpoch 47/50\n - 0s - loss: 32.3832 - val_loss: 41.2286\nEpoch 48/50\n - 0s - loss: 32.4289 - val_loss: 41.2393\nEpoch 49/50\n - 0s - loss: 32.3632 - val_loss: 41.1818\nEpoch 50/50\n - 0s - loss: 32.4454 - val_loss: 40.9667\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.4475 - val_loss: 41.0272\nEpoch 2/50\n - 0s - loss: 32.4091 - val_loss: 41.4657\nEpoch 3/50\n - 0s - loss: 32.4265 - val_loss: 41.2344\nEpoch 4/50\n - 0s - loss: 32.5364 - val_loss: 41.0897\nEpoch 5/50\n - 0s - loss: 32.3588 - val_loss: 41.2304\nEpoch 6/50\n - 0s - loss: 32.4257 - val_loss: 41.1251\nEpoch 7/50\n - 0s - loss: 32.4382 - val_loss: 41.1043\nEpoch 8/50\n - 0s - loss: 32.5344 - val_loss: 41.3397\nEpoch 9/50\n - 0s - loss: 32.3337 - val_loss: 41.1856\nEpoch 10/50\n - 0s - loss: 32.4056 - val_loss: 41.0743\nEpoch 11/50\n - 0s - loss: 32.3473 - val_loss: 41.0825\nEpoch 12/50\n - 0s - loss: 32.3492 - val_loss: 41.3025\nEpoch 13/50\n - 0s - loss: 32.4827 - val_loss: 41.3267\nEpoch 14/50\n - 0s - loss: 32.3987 - val_loss: 41.3170\nEpoch 15/50\n - 0s - loss: 32.4112 - val_loss: 41.1156\nEpoch 16/50\n - 0s - loss: 32.3900 - val_loss: 41.1019\nEpoch 17/50\n - 0s - loss: 32.3521 - val_loss: 41.4439\nEpoch 18/50\n - 0s - loss: 32.4044 - val_loss: 41.2080\nEpoch 19/50\n - 0s - loss: 32.3674 - val_loss: 41.2240\nEpoch 20/50\n - 0s - loss: 32.3968 - val_loss: 41.1346\nEpoch 21/50\n - 0s - loss: 32.3084 - val_loss: 41.3896\nEpoch 22/50\n - 0s - loss: 32.6044 - val_loss: 41.3891\nEpoch 23/50\n - 0s - loss: 32.3497 - val_loss: 41.1532\nEpoch 24/50\n - 0s - loss: 32.4014 - val_loss: 41.3547\nEpoch 25/50\n - 0s - loss: 32.4957 - val_loss: 41.0773\nEpoch 26/50\n - 0s - loss: 32.3881 - val_loss: 41.0435\nEpoch 27/50\n - 0s - loss: 32.4134 - val_loss: 41.4350\nEpoch 28/50\n - 0s - loss: 32.3428 - val_loss: 41.1572\nEpoch 29/50\n - 0s - loss: 32.4000 - val_loss: 41.4217\nEpoch 30/50\n - 0s - loss: 32.3474 - val_loss: 41.2537\nEpoch 31/50\n - 0s - loss: 32.4527 - val_loss: 41.1694\nEpoch 32/50\n - 0s - loss: 32.4568 - val_loss: 40.9988\nEpoch 33/50\n - 0s - loss: 32.4494 - val_loss: 41.4726\nEpoch 34/50\n - 0s - loss: 32.4549 - val_loss: 41.2643\nEpoch 35/50\n - 0s - loss: 32.3524 - val_loss: 41.1930\nEpoch 36/50\n - 0s - loss: 32.4064 - val_loss: 41.2845\nEpoch 37/50\n - 0s - loss: 32.4463 - val_loss: 41.2120\nEpoch 38/50\n - 0s - loss: 32.4324 - val_loss: 41.3472\nEpoch 39/50\n - 0s - loss: 32.4681 - val_loss: 41.1986\nEpoch 40/50\n - 0s - loss: 32.3623 - val_loss: 41.1670\nEpoch 41/50\n - 0s - loss: 32.4156 - val_loss: 41.1968\nEpoch 42/50\n - 0s - loss: 32.4957 - val_loss: 41.1523\nEpoch 43/50\n - 0s - loss: 32.3587 - val_loss: 41.3409\nEpoch 44/50\n - 0s - loss: 32.4734 - val_loss: 41.1510\nEpoch 45/50\n - 0s - loss: 32.4294 - val_loss: 41.3502\nEpoch 46/50\n - 0s - loss: 32.3857 - val_loss: 41.4314\nEpoch 47/50\n - 0s - loss: 32.3929 - val_loss: 40.9703\nEpoch 48/50\n - 0s - loss: 32.5771 - val_loss: 41.2914\nEpoch 49/50\n - 0s - loss: 32.4024 - val_loss: 41.3430\nEpoch 50/50\n - 0s - loss: 32.5160 - val_loss: 41.1021\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3748 - val_loss: 41.0531\nEpoch 2/50\n - 0s - loss: 32.4508 - val_loss: 41.2605\nEpoch 3/50\n - 0s - loss: 32.4191 - val_loss: 41.2764\nEpoch 4/50\n - 0s - loss: 32.4426 - val_loss: 41.3570\nEpoch 5/50\n - 0s - loss: 32.5131 - val_loss: 41.1673\nEpoch 6/50\n - 0s - loss: 32.3993 - val_loss: 41.3306\nEpoch 7/50\n - 0s - loss: 32.3527 - val_loss: 41.3171\nEpoch 8/50\n - 0s - loss: 32.4898 - val_loss: 41.3059\nEpoch 9/50\n - 0s - loss: 32.4702 - val_loss: 41.1553\nEpoch 10/50\n - 0s - loss: 32.3119 - val_loss: 41.1154\nEpoch 11/50\n - 0s - loss: 32.4246 - val_loss: 41.2812\nEpoch 12/50\n - 0s - loss: 32.3812 - val_loss: 41.1904\nEpoch 13/50\n - 0s - loss: 32.4161 - val_loss: 41.3188\nEpoch 14/50\n - 0s - loss: 32.5626 - val_loss: 41.0181\nEpoch 15/50\n - 0s - loss: 32.3725 - val_loss: 41.4792\nEpoch 16/50\n - 0s - loss: 32.3585 - val_loss: 41.1869\nEpoch 17/50\n - 0s - loss: 32.4149 - val_loss: 41.4544\nEpoch 18/50\n - 0s - loss: 32.3821 - val_loss: 40.9936\nEpoch 19/50\n - 0s - loss: 32.3599 - val_loss: 41.1398\nEpoch 20/50\n - 0s - loss: 32.3408 - val_loss: 41.1355\nEpoch 21/50\n - 0s - loss: 32.3554 - val_loss: 41.1966\nEpoch 22/50\n - 0s - loss: 32.3592 - val_loss: 41.2089\nEpoch 23/50\n - 0s - loss: 32.3692 - val_loss: 41.0978\nEpoch 24/50\n - 0s - loss: 32.4390 - val_loss: 41.1287\nEpoch 25/50\n - 0s - loss: 32.4258 - val_loss: 41.2507\nEpoch 26/50\n - 0s - loss: 32.4381 - val_loss: 41.2045\nEpoch 27/50\n - 0s - loss: 32.3656 - val_loss: 41.0503\nEpoch 28/50\n - 0s - loss: 32.3908 - val_loss: 41.1972\nEpoch 29/50\n - 0s - loss: 32.3827 - val_loss: 41.1833\nEpoch 30/50\n - 0s - loss: 32.3781 - val_loss: 41.4818\nEpoch 31/50\n - 0s - loss: 32.3862 - val_loss: 41.2624\nEpoch 32/50\n - 0s - loss: 32.3916 - val_loss: 40.9911\nEpoch 33/50\n - 0s - loss: 32.4997 - val_loss: 40.9910\nEpoch 34/50\n - 0s - loss: 32.3891 - val_loss: 41.4861\nEpoch 35/50\n - 0s - loss: 32.3174 - val_loss: 41.1457\nEpoch 36/50\n - 0s - loss: 32.3652 - val_loss: 41.2298\nEpoch 37/50\n - 0s - loss: 32.5471 - val_loss: 41.1413\nEpoch 38/50\n - 0s - loss: 32.5396 - val_loss: 41.0139\nEpoch 39/50\n - 0s - loss: 32.3873 - val_loss: 41.5681\nEpoch 40/50\n - 0s - loss: 32.3817 - val_loss: 41.1500\nEpoch 41/50\n - 0s - loss: 32.3543 - val_loss: 41.2789\nEpoch 42/50\n - 0s - loss: 32.3692 - val_loss: 41.2662\nEpoch 43/50\n - 0s - loss: 32.3461 - val_loss: 41.1122\nEpoch 44/50\n - 0s - loss: 32.3564 - val_loss: 41.2691\nEpoch 45/50\n - 0s - loss: 32.3047 - val_loss: 41.0206\nEpoch 46/50\n - 0s - loss: 32.3569 - val_loss: 41.1656\nEpoch 47/50\n - 0s - loss: 32.3449 - val_loss: 41.1582\nEpoch 48/50\n - 0s - loss: 32.3399 - val_loss: 41.0752\nEpoch 49/50\n - 0s - loss: 32.4022 - val_loss: 40.9689\nEpoch 50/50\n - 0s - loss: 32.3953 - val_loss: 41.1099\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3456 - val_loss: 41.2808\nEpoch 2/50\n - 0s - loss: 32.3791 - val_loss: 41.0559\nEpoch 3/50\n - 0s - loss: 32.3657 - val_loss: 41.2460\nEpoch 4/50\n - 0s - loss: 32.4021 - val_loss: 41.0166\nEpoch 5/50\n - 0s - loss: 32.4017 - val_loss: 41.3792\nEpoch 6/50\n - 0s - loss: 32.3974 - val_loss: 41.4099\nEpoch 7/50\n - 0s - loss: 32.3981 - val_loss: 41.2380\nEpoch 8/50\n - 0s - loss: 32.4325 - val_loss: 41.4918\nEpoch 9/50\n - 0s - loss: 32.3270 - val_loss: 41.1521\nEpoch 10/50\n - 0s - loss: 32.5486 - val_loss: 40.9035\nEpoch 11/50\n - 0s - loss: 32.3804 - val_loss: 41.2038\nEpoch 12/50\n - 0s - loss: 32.3771 - val_loss: 41.2873\nEpoch 13/50\n - 0s - loss: 32.4501 - val_loss: 41.1778\nEpoch 14/50\n - 0s - loss: 32.3471 - val_loss: 41.2839\nEpoch 15/50\n - 0s - loss: 32.3440 - val_loss: 41.0758\nEpoch 16/50\n - 0s - loss: 32.4066 - val_loss: 41.5362\nEpoch 17/50\n - 0s - loss: 32.4051 - val_loss: 40.9345\nEpoch 18/50\n - 0s - loss: 32.3095 - val_loss: 41.3374\nEpoch 19/50\n - 0s - loss: 32.3238 - val_loss: 41.2788\nEpoch 20/50\n - 0s - loss: 32.3693 - val_loss: 41.1168\nEpoch 21/50\n - 0s - loss: 32.4510 - val_loss: 41.1479\nEpoch 22/50\n - 0s - loss: 32.4408 - val_loss: 41.1006\nEpoch 23/50\n - 0s - loss: 32.3417 - val_loss: 41.3939\nEpoch 24/50\n - 0s - loss: 32.3071 - val_loss: 41.2339\nEpoch 25/50\n - 0s - loss: 32.5242 - val_loss: 41.2066\nEpoch 26/50\n - 0s - loss: 32.4614 - val_loss: 41.5837\nEpoch 27/50\n - 0s - loss: 32.3057 - val_loss: 40.9600\nEpoch 28/50\n - 0s - loss: 32.4384 - val_loss: 41.2159\nEpoch 29/50\n - 0s - loss: 32.3553 - val_loss: 41.2113\nEpoch 30/50\n - 0s - loss: 32.3671 - val_loss: 41.2293\nEpoch 31/50\n - 0s - loss: 32.3431 - val_loss: 41.3494\nEpoch 32/50\n - 0s - loss: 32.3007 - val_loss: 41.3087\nEpoch 33/50\n - 0s - loss: 32.2937 - val_loss: 41.2716\nEpoch 34/50\n - 0s - loss: 32.3068 - val_loss: 41.3263\nEpoch 35/50\n - 0s - loss: 32.3616 - val_loss: 41.1077\nEpoch 36/50\n - 0s - loss: 32.3652 - val_loss: 41.1222\nEpoch 37/50\n - 0s - loss: 32.3938 - val_loss: 41.4120\nEpoch 38/50\n - 0s - loss: 32.3001 - val_loss: 41.2875\nEpoch 39/50\n - 0s - loss: 32.4890 - val_loss: 41.3553\nEpoch 40/50\n - 0s - loss: 32.3404 - val_loss: 41.1009\nEpoch 41/50\n - 0s - loss: 32.3671 - val_loss: 41.2053\nEpoch 42/50\n - 0s - loss: 32.3760 - val_loss: 41.4120\nEpoch 43/50\n - 0s - loss: 32.3911 - val_loss: 41.0592\nEpoch 44/50\n - 0s - loss: 32.3856 - val_loss: 41.3578\nEpoch 45/50\n - 0s - loss: 32.3326 - val_loss: 41.1936\nEpoch 46/50\n - 0s - loss: 32.3866 - val_loss: 41.2883\nEpoch 47/50\n - 0s - loss: 32.4249 - val_loss: 40.9242\nEpoch 48/50\n - 0s - loss: 32.2962 - val_loss: 41.4940\nEpoch 49/50\n - 0s - loss: 32.3096 - val_loss: 41.3574\nEpoch 50/50\n - 0s - loss: 32.5466 - val_loss: 41.6165\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3674 - val_loss: 41.2351\nEpoch 2/50\n - 0s - loss: 32.3128 - val_loss: 41.2087\nEpoch 3/50\n - 0s - loss: 32.4528 - val_loss: 41.3266\nEpoch 4/50\n - 0s - loss: 32.2924 - val_loss: 41.1646\nEpoch 5/50\n - 0s - loss: 32.3817 - val_loss: 41.0304\nEpoch 6/50\n - 0s - loss: 32.4975 - val_loss: 41.4723\nEpoch 7/50\n - 0s - loss: 32.4117 - val_loss: 41.0259\nEpoch 8/50\n - 0s - loss: 32.4011 - val_loss: 41.2605\nEpoch 9/50\n - 0s - loss: 32.3665 - val_loss: 41.0832\nEpoch 10/50\n - 0s - loss: 32.3167 - val_loss: 41.3278\nEpoch 11/50\n - 0s - loss: 32.4444 - val_loss: 41.1204\nEpoch 12/50\n - 0s - loss: 32.3627 - val_loss: 41.2761\nEpoch 13/50\n - 0s - loss: 32.6260 - val_loss: 41.1244\nEpoch 14/50\n - 0s - loss: 32.3857 - val_loss: 41.4906\nEpoch 15/50\n - 0s - loss: 32.3983 - val_loss: 41.0824\nEpoch 16/50\n - 0s - loss: 32.3317 - val_loss: 41.2465\nEpoch 17/50\n - 0s - loss: 32.3788 - val_loss: 41.1509\nEpoch 18/50\n - 0s - loss: 32.3538 - val_loss: 41.1451\nEpoch 19/50\n - 0s - loss: 32.3707 - val_loss: 41.0903\nEpoch 20/50\n - 0s - loss: 32.3779 - val_loss: 41.2911\nEpoch 21/50\n - 0s - loss: 32.3603 - val_loss: 41.3001\nEpoch 22/50\n - 0s - loss: 32.4574 - val_loss: 41.0220\nEpoch 23/50\n - 0s - loss: 32.3575 - val_loss: 41.2812\nEpoch 24/50\n - 0s - loss: 32.3037 - val_loss: 41.0543\nEpoch 25/50\n - 0s - loss: 32.3287 - val_loss: 41.1943\nEpoch 26/50\n - 0s - loss: 32.3293 - val_loss: 41.1966\nEpoch 27/50\n - 0s - loss: 32.3529 - val_loss: 41.3524\nEpoch 28/50\n - 0s - loss: 32.3160 - val_loss: 41.2091\nEpoch 29/50\n - 0s - loss: 32.4009 - val_loss: 41.0864\nEpoch 30/50\n - 0s - loss: 32.3797 - val_loss: 41.0840\nEpoch 31/50\n - 0s - loss: 32.3265 - val_loss: 41.2663\nEpoch 32/50\n - 0s - loss: 32.3553 - val_loss: 41.0935\nEpoch 33/50\n - 0s - loss: 32.3346 - val_loss: 41.2780\nEpoch 34/50\n - 0s - loss: 32.2977 - val_loss: 41.0563\nEpoch 35/50\n - 0s - loss: 32.3229 - val_loss: 41.0198\nEpoch 36/50\n - 0s - loss: 32.4104 - val_loss: 41.3485\nEpoch 37/50\n - 0s - loss: 32.2678 - val_loss: 41.0986\nEpoch 38/50\n - 0s - loss: 32.3501 - val_loss: 41.0775\nEpoch 39/50\n - 0s - loss: 32.4465 - val_loss: 41.3123\nEpoch 40/50\n - 0s - loss: 32.3618 - val_loss: 40.9125\nEpoch 41/50\n - 0s - loss: 32.3999 - val_loss: 41.1923\nEpoch 42/50\n - 0s - loss: 32.3349 - val_loss: 41.4351\nEpoch 43/50\n - 0s - loss: 32.3742 - val_loss: 40.9923\nEpoch 44/50\n - 0s - loss: 32.3322 - val_loss: 41.1442\nEpoch 45/50\n - 0s - loss: 32.4107 - val_loss: 41.4301\nEpoch 46/50\n - 0s - loss: 32.3390 - val_loss: 40.8903\nEpoch 47/50\n - 0s - loss: 32.2952 - val_loss: 41.0908\nEpoch 48/50\n - 0s - loss: 32.3428 - val_loss: 41.2160\nEpoch 49/50\n - 0s - loss: 32.3578 - val_loss: 41.5198\nEpoch 50/50\n - 0s - loss: 32.2674 - val_loss: 41.2255\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3433 - val_loss: 41.1547\nEpoch 2/50\n - 0s - loss: 32.5380 - val_loss: 41.5024\nEpoch 3/50\n - 0s - loss: 32.4243 - val_loss: 41.3230\nEpoch 4/50\n - 0s - loss: 32.3507 - val_loss: 41.0285\nEpoch 5/50\n - 0s - loss: 32.4213 - val_loss: 41.1843\nEpoch 6/50\n - 0s - loss: 32.4103 - val_loss: 41.4663\nEpoch 7/50\n - 0s - loss: 32.3538 - val_loss: 41.1365\nEpoch 8/50\n - 0s - loss: 32.4658 - val_loss: 41.1994\nEpoch 9/50\n - 0s - loss: 32.3477 - val_loss: 40.9847\nEpoch 10/50\n - 0s - loss: 32.2807 - val_loss: 41.2074\nEpoch 11/50\n - 0s - loss: 32.3133 - val_loss: 41.4024\nEpoch 12/50\n - 0s - loss: 32.3524 - val_loss: 41.2668\nEpoch 13/50\n - 0s - loss: 32.3215 - val_loss: 41.0585\nEpoch 14/50\n - 0s - loss: 32.2865 - val_loss: 40.9777\nEpoch 15/50\n - 0s - loss: 32.2718 - val_loss: 41.3071\nEpoch 16/50\n - 0s - loss: 32.2845 - val_loss: 41.1047\nEpoch 17/50\n - 0s - loss: 32.3161 - val_loss: 40.8961\nEpoch 18/50\n - 0s - loss: 32.4916 - val_loss: 41.1994\nEpoch 19/50\n - 0s - loss: 32.3195 - val_loss: 41.2301\nEpoch 20/50\n - 0s - loss: 32.3721 - val_loss: 41.1214\nEpoch 21/50\n - 0s - loss: 32.4617 - val_loss: 41.3679\nEpoch 22/50\n - 0s - loss: 32.2889 - val_loss: 41.1642\nEpoch 23/50\n - 0s - loss: 32.5992 - val_loss: 41.0169\nEpoch 24/50\n - 0s - loss: 32.3045 - val_loss: 41.7520\nEpoch 25/50\n - 0s - loss: 32.4115 - val_loss: 41.1811\nEpoch 26/50\n - 0s - loss: 32.3298 - val_loss: 41.1955\nEpoch 27/50\n - 0s - loss: 32.3319 - val_loss: 41.1352\nEpoch 28/50\n - 0s - loss: 32.3463 - val_loss: 41.3140\nEpoch 29/50\n - 0s - loss: 32.3214 - val_loss: 40.9228\nEpoch 30/50\n - 0s - loss: 32.3590 - val_loss: 41.1989\nEpoch 31/50\n - 0s - loss: 32.3678 - val_loss: 41.2493\nEpoch 32/50\n - 0s - loss: 32.3818 - val_loss: 41.3195\nEpoch 33/50\n - 0s - loss: 32.3368 - val_loss: 41.0209\nEpoch 34/50\n - 0s - loss: 32.4325 - val_loss: 41.6103\nEpoch 35/50\n - 0s - loss: 32.3205 - val_loss: 41.1981\nEpoch 36/50\n - 0s - loss: 32.3320 - val_loss: 40.9013\nEpoch 37/50\n - 0s - loss: 32.3133 - val_loss: 41.1036\nEpoch 38/50\n - 0s - loss: 32.3289 - val_loss: 41.3459\nEpoch 39/50\n - 0s - loss: 32.3817 - val_loss: 41.0214\nEpoch 40/50\n - 0s - loss: 32.3885 - val_loss: 41.3718\nEpoch 41/50\n - 0s - loss: 32.3154 - val_loss: 41.0950\nEpoch 42/50\n - 0s - loss: 32.2765 - val_loss: 41.1496\nEpoch 43/50\n - 0s - loss: 32.3523 - val_loss: 41.0543\nEpoch 44/50\n - 0s - loss: 32.3469 - val_loss: 41.4589\nEpoch 45/50\n - 0s - loss: 32.3892 - val_loss: 41.2959\nEpoch 46/50\n - 0s - loss: 32.2854 - val_loss: 41.3266\nEpoch 47/50\n - 0s - loss: 32.4043 - val_loss: 41.1990\nEpoch 48/50\n - 0s - loss: 32.3351 - val_loss: 41.2509\nEpoch 49/50\n - 0s - loss: 32.2949 - val_loss: 41.3047\nEpoch 50/50\n - 0s - loss: 32.3659 - val_loss: 41.2003\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3185 - val_loss: 41.1470\nEpoch 2/50\n - 0s - loss: 32.2917 - val_loss: 41.2011\nEpoch 3/50\n - 0s - loss: 32.3538 - val_loss: 41.1417\nEpoch 4/50\n - 0s - loss: 32.2754 - val_loss: 41.0717\nEpoch 5/50\n - 0s - loss: 32.3412 - val_loss: 41.3642\nEpoch 6/50\n - 0s - loss: 32.4125 - val_loss: 41.0619\nEpoch 7/50\n - 0s - loss: 32.4448 - val_loss: 41.2046\nEpoch 8/50\n - 0s - loss: 32.4779 - val_loss: 41.1795\nEpoch 9/50\n - 0s - loss: 32.2962 - val_loss: 41.2818\nEpoch 10/50\n - 0s - loss: 32.2629 - val_loss: 41.2841\nEpoch 11/50\n - 0s - loss: 32.2995 - val_loss: 41.2217\nEpoch 12/50\n - 0s - loss: 32.3004 - val_loss: 41.1894\nEpoch 13/50\n - 0s - loss: 32.2808 - val_loss: 41.1327\nEpoch 14/50\n - 0s - loss: 32.3311 - val_loss: 40.9569\nEpoch 15/50\n - 0s - loss: 32.3473 - val_loss: 41.3859\nEpoch 16/50\n - 0s - loss: 32.3433 - val_loss: 41.0499\nEpoch 17/50\n - 0s - loss: 32.4729 - val_loss: 41.5603\nEpoch 18/50\n - 0s - loss: 32.3402 - val_loss: 41.2885\nEpoch 19/50\n - 0s - loss: 32.2712 - val_loss: 40.9461\nEpoch 20/50\n - 0s - loss: 32.3188 - val_loss: 41.2317\nEpoch 21/50\n - 0s - loss: 32.3591 - val_loss: 41.1070\nEpoch 22/50\n - 0s - loss: 32.3906 - val_loss: 41.4895\nEpoch 23/50\n - 0s - loss: 32.3511 - val_loss: 41.1798\nEpoch 24/50\n - 0s - loss: 32.3650 - val_loss: 41.2631\nEpoch 25/50\n - 0s - loss: 32.3151 - val_loss: 41.0827\nEpoch 26/50\n - 0s - loss: 32.3441 - val_loss: 41.1565\nEpoch 27/50\n - 0s - loss: 32.2873 - val_loss: 41.1991\nEpoch 28/50\n - 0s - loss: 32.3537 - val_loss: 41.1966\nEpoch 29/50\n - 0s - loss: 32.3127 - val_loss: 40.9869\nEpoch 30/50\n - 0s - loss: 32.4490 - val_loss: 41.2540\nEpoch 31/50\n - 0s - loss: 32.4401 - val_loss: 40.9847\nEpoch 32/50\n - 0s - loss: 32.2445 - val_loss: 41.4288\nEpoch 33/50\n - 0s - loss: 32.3872 - val_loss: 41.4547\nEpoch 34/50\n - 0s - loss: 32.3932 - val_loss: 41.2405\nEpoch 35/50\n - 0s - loss: 32.3808 - val_loss: 41.1722\nEpoch 36/50\n - 0s - loss: 32.3611 - val_loss: 41.4451\nEpoch 37/50\n - 0s - loss: 32.5347 - val_loss: 41.2404\nEpoch 38/50\n - 0s - loss: 32.4405 - val_loss: 41.2764\nEpoch 39/50\n - 0s - loss: 32.3658 - val_loss: 41.3281\nEpoch 40/50\n - 0s - loss: 32.4237 - val_loss: 41.0960\nEpoch 41/50\n - 0s - loss: 32.4304 - val_loss: 41.2894\nEpoch 42/50\n - 0s - loss: 32.3737 - val_loss: 41.2184\nEpoch 43/50\n - 0s - loss: 32.3694 - val_loss: 41.4569\nEpoch 44/50\n - 0s - loss: 32.3905 - val_loss: 41.0561\nEpoch 45/50\n - 0s - loss: 32.3222 - val_loss: 41.1819\nEpoch 46/50\n - 0s - loss: 32.3858 - val_loss: 41.5523\nEpoch 47/50\n - 0s - loss: 32.4689 - val_loss: 41.1019\nEpoch 48/50\n - 0s - loss: 32.3406 - val_loss: 41.4991\nEpoch 49/50\n - 0s - loss: 32.3134 - val_loss: 41.4600\nEpoch 50/50\n - 0s - loss: 32.4394 - val_loss: 41.1037\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3028 - val_loss: 41.4211\nEpoch 2/50\n - 0s - loss: 32.4710 - val_loss: 41.2854\nEpoch 3/50\n - 0s - loss: 32.3618 - val_loss: 41.2304\nEpoch 4/50\n - 0s - loss: 32.4164 - val_loss: 41.1262\nEpoch 5/50\n - 0s - loss: 32.2969 - val_loss: 41.4031\nEpoch 6/50\n - 0s - loss: 32.3504 - val_loss: 41.5034\nEpoch 7/50\n - 0s - loss: 32.3110 - val_loss: 41.2058\nEpoch 8/50\n - 0s - loss: 32.3706 - val_loss: 41.2565\nEpoch 9/50\n - 0s - loss: 32.2997 - val_loss: 41.2366\nEpoch 10/50\n - 0s - loss: 32.3454 - val_loss: 41.2750\nEpoch 11/50\n - 0s - loss: 32.4446 - val_loss: 41.2132\nEpoch 12/50\n - 0s - loss: 32.3637 - val_loss: 40.9992\nEpoch 13/50\n - 0s - loss: 32.2570 - val_loss: 41.3317\nEpoch 14/50\n - 0s - loss: 32.3317 - val_loss: 41.3549\nEpoch 15/50\n - 0s - loss: 32.2665 - val_loss: 41.3812\nEpoch 16/50\n - 0s - loss: 32.3046 - val_loss: 40.9862\nEpoch 17/50\n - 0s - loss: 32.3618 - val_loss: 41.2027\nEpoch 18/50\n - 0s - loss: 32.3593 - val_loss: 41.3748\nEpoch 19/50\n - 0s - loss: 32.3741 - val_loss: 41.0375\nEpoch 20/50\n - 0s - loss: 32.3626 - val_loss: 41.7126\nEpoch 21/50\n - 0s - loss: 32.2950 - val_loss: 41.2309\nEpoch 22/50\n - 0s - loss: 32.3215 - val_loss: 41.0668\nEpoch 23/50\n - 0s - loss: 32.6408 - val_loss: 41.4358\nEpoch 24/50\n - 0s - loss: 32.3807 - val_loss: 41.0912\nEpoch 25/50\n - 0s - loss: 32.4658 - val_loss: 41.2434\nEpoch 26/50\n - 0s - loss: 32.3360 - val_loss: 41.0432\nEpoch 27/50\n - 0s - loss: 32.3712 - val_loss: 41.1744\nEpoch 28/50\n - 0s - loss: 32.3683 - val_loss: 41.2700\nEpoch 29/50\n - 0s - loss: 32.3129 - val_loss: 41.1444\nEpoch 30/50\n - 0s - loss: 32.3730 - val_loss: 41.3244\nEpoch 31/50\n - 0s - loss: 32.3024 - val_loss: 41.0223\nEpoch 32/50\n - 0s - loss: 32.2868 - val_loss: 41.0275\nEpoch 33/50\n - 0s - loss: 32.2957 - val_loss: 41.2010\nEpoch 34/50\n - 0s - loss: 32.3208 - val_loss: 41.2278\nEpoch 35/50\n - 0s - loss: 32.4297 - val_loss: 41.5272\nEpoch 36/50\n - 0s - loss: 32.2615 - val_loss: 41.1791\nEpoch 37/50\n - 0s - loss: 32.3424 - val_loss: 40.9674\nEpoch 38/50\n - 0s - loss: 32.3105 - val_loss: 41.0066\nEpoch 39/50\n - 0s - loss: 32.2956 - val_loss: 41.4489\nEpoch 40/50\n - 0s - loss: 32.3583 - val_loss: 41.1151\nEpoch 41/50\n - 0s - loss: 32.3069 - val_loss: 41.1952\nEpoch 42/50\n - 0s - loss: 32.3081 - val_loss: 41.2561\nEpoch 43/50\n - 0s - loss: 32.3141 - val_loss: 41.3136\nEpoch 44/50\n - 0s - loss: 32.4427 - val_loss: 41.5059\nEpoch 45/50\n - 0s - loss: 32.3231 - val_loss: 40.8391\nEpoch 46/50\n - 0s - loss: 32.6338 - val_loss: 41.1767\nEpoch 47/50\n - 0s - loss: 32.2842 - val_loss: 40.9969\nEpoch 48/50\n - 0s - loss: 32.4245 - val_loss: 41.2609\nEpoch 49/50\n - 0s - loss: 32.3246 - val_loss: 41.0410\nEpoch 50/50\n - 0s - loss: 32.3787 - val_loss: 41.3610\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3076 - val_loss: 41.4505\nEpoch 2/50\n - 0s - loss: 32.2473 - val_loss: 41.3217\nEpoch 3/50\n - 0s - loss: 32.3183 - val_loss: 41.5429\nEpoch 4/50\n - 0s - loss: 32.2609 - val_loss: 41.2947\nEpoch 5/50\n - 0s - loss: 32.3744 - val_loss: 40.9547\nEpoch 6/50\n - 0s - loss: 32.4389 - val_loss: 41.2434\nEpoch 7/50\n - 0s - loss: 32.2724 - val_loss: 41.2308\nEpoch 8/50\n - 0s - loss: 32.3944 - val_loss: 40.8930\nEpoch 9/50\n - 0s - loss: 32.3186 - val_loss: 41.4202\nEpoch 10/50\n - 0s - loss: 32.3137 - val_loss: 41.4019\nEpoch 11/50\n - 0s - loss: 32.3202 - val_loss: 41.0507\nEpoch 12/50\n - 0s - loss: 32.3161 - val_loss: 41.2152\nEpoch 13/50\n - 0s - loss: 32.3346 - val_loss: 41.1087\nEpoch 14/50\n - 0s - loss: 32.3640 - val_loss: 41.4547\nEpoch 15/50\n - 0s - loss: 32.2752 - val_loss: 41.0680\nEpoch 16/50\n - 0s - loss: 32.4626 - val_loss: 41.5031\nEpoch 17/50\n - 0s - loss: 32.2751 - val_loss: 41.2364\nEpoch 18/50\n - 0s - loss: 32.4851 - val_loss: 41.1472\nEpoch 19/50\n - 0s - loss: 32.2935 - val_loss: 41.2212\nEpoch 20/50\n - 0s - loss: 32.3006 - val_loss: 41.2902\nEpoch 21/50\n - 0s - loss: 32.2941 - val_loss: 41.2451\nEpoch 22/50\n - 0s - loss: 32.3615 - val_loss: 41.3179\nEpoch 23/50\n - 0s - loss: 32.3495 - val_loss: 41.0027\nEpoch 24/50\n - 0s - loss: 32.2738 - val_loss: 41.2828\nEpoch 25/50\n - 0s - loss: 32.4437 - val_loss: 41.1999\nEpoch 26/50\n - 0s - loss: 32.2929 - val_loss: 41.2920\nEpoch 27/50\n - 0s - loss: 32.3084 - val_loss: 41.2702\nEpoch 28/50\n - 0s - loss: 32.3436 - val_loss: 41.0950\nEpoch 29/50\n - 0s - loss: 32.3741 - val_loss: 41.0811\nEpoch 30/50\n - 0s - loss: 32.3204 - val_loss: 40.9643\nEpoch 31/50\n - 0s - loss: 32.3986 - val_loss: 41.3989\nEpoch 32/50\n - 0s - loss: 32.2737 - val_loss: 41.1396\nEpoch 33/50\n - 0s - loss: 32.3879 - val_loss: 40.8282\nEpoch 34/50\n - 0s - loss: 32.2688 - val_loss: 41.3210\nEpoch 35/50\n - 0s - loss: 32.3085 - val_loss: 41.2539\nEpoch 36/50\n - 0s - loss: 32.3909 - val_loss: 41.1342\nEpoch 37/50\n - 0s - loss: 32.3743 - val_loss: 41.0297\nEpoch 38/50\n - 0s - loss: 32.3322 - val_loss: 41.2485\nEpoch 39/50\n - 0s - loss: 32.3423 - val_loss: 41.0762\nEpoch 40/50\n - 0s - loss: 32.3243 - val_loss: 41.3570\nEpoch 41/50\n - 0s - loss: 32.4475 - val_loss: 41.4052\nEpoch 42/50\n - 0s - loss: 32.3502 - val_loss: 41.0822\nEpoch 43/50\n - 0s - loss: 32.4305 - val_loss: 40.9745\nEpoch 44/50\n - 0s - loss: 32.3604 - val_loss: 41.3085\nEpoch 45/50\n - 0s - loss: 32.3468 - val_loss: 41.2991\nEpoch 46/50\n - 0s - loss: 32.3059 - val_loss: 40.9997\nEpoch 47/50\n - 0s - loss: 32.3930 - val_loss: 41.0443\nEpoch 48/50\n - 0s - loss: 32.3261 - val_loss: 41.5773\nEpoch 49/50\n - 0s - loss: 32.4236 - val_loss: 41.2988\nEpoch 50/50\n - 0s - loss: 32.3011 - val_loss: 41.0020\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3363 - val_loss: 41.2815\nEpoch 2/50\n - 0s - loss: 32.3146 - val_loss: 41.2299\nEpoch 3/50\n - 0s - loss: 32.2934 - val_loss: 41.0040\nEpoch 4/50\n - 0s - loss: 32.3597 - val_loss: 41.2767\nEpoch 5/50\n - 0s - loss: 32.3808 - val_loss: 40.8633\nEpoch 6/50\n - 0s - loss: 32.2707 - val_loss: 41.3602\nEpoch 7/50\n - 0s - loss: 32.2824 - val_loss: 41.2507\nEpoch 8/50\n - 0s - loss: 32.2688 - val_loss: 41.2486\nEpoch 9/50\n - 0s - loss: 32.3234 - val_loss: 41.3443\nEpoch 10/50\n - 0s - loss: 32.3316 - val_loss: 41.3247\nEpoch 11/50\n - 0s - loss: 32.3528 - val_loss: 41.1903\nEpoch 12/50\n - 0s - loss: 32.3076 - val_loss: 41.2024\nEpoch 13/50\n - 0s - loss: 32.3706 - val_loss: 41.3832\nEpoch 14/50\n - 0s - loss: 32.3697 - val_loss: 41.4916\nEpoch 15/50\n - 0s - loss: 32.3528 - val_loss: 41.2511\nEpoch 16/50\n - 0s - loss: 32.3022 - val_loss: 41.0417\nEpoch 17/50\n - 0s - loss: 32.3847 - val_loss: 41.2772\nEpoch 18/50\n - 0s - loss: 32.3181 - val_loss: 41.3646\nEpoch 19/50\n - 0s - loss: 32.3434 - val_loss: 41.0323\nEpoch 20/50\n - 0s - loss: 32.3277 - val_loss: 41.2659\nEpoch 21/50\n - 0s - loss: 32.3236 - val_loss: 41.4163\nEpoch 22/50\n - 0s - loss: 32.2905 - val_loss: 41.4947\nEpoch 23/50\n - 0s - loss: 32.2734 - val_loss: 41.1677\nEpoch 24/50\n - 0s - loss: 32.3238 - val_loss: 41.2057\nEpoch 25/50\n - 0s - loss: 32.3982 - val_loss: 41.1516\nEpoch 26/50\n - 0s - loss: 32.4849 - val_loss: 41.0130\nEpoch 27/50\n - 0s - loss: 32.3495 - val_loss: 41.3807\nEpoch 28/50\n - 0s - loss: 32.6672 - val_loss: 41.0958\nEpoch 29/50\n - 0s - loss: 32.3037 - val_loss: 41.3867\nEpoch 30/50\n - 0s - loss: 32.4131 - val_loss: 41.2493\nEpoch 31/50\n - 0s - loss: 32.3881 - val_loss: 41.2568\nEpoch 32/50\n - 0s - loss: 32.2860 - val_loss: 41.1744\nEpoch 33/50\n - 0s - loss: 32.3018 - val_loss: 41.0874\nEpoch 34/50\n - 0s - loss: 32.3070 - val_loss: 41.1868\nEpoch 35/50\n - 0s - loss: 32.3148 - val_loss: 41.6477\nEpoch 36/50\n - 0s - loss: 32.3203 - val_loss: 41.1670\nEpoch 37/50\n - 0s - loss: 32.2953 - val_loss: 41.3629\nEpoch 38/50\n - 0s - loss: 32.3034 - val_loss: 41.2938\nEpoch 39/50\n - 0s - loss: 32.2961 - val_loss: 41.1595\nEpoch 40/50\n - 0s - loss: 32.4145 - val_loss: 41.0384\nEpoch 41/50\n - 0s - loss: 32.5387 - val_loss: 41.0880\nEpoch 42/50\n - 0s - loss: 32.4175 - val_loss: 41.1933\nEpoch 43/50\n - 0s - loss: 32.5026 - val_loss: 41.2629\nEpoch 44/50\n - 0s - loss: 32.2867 - val_loss: 41.4151\nEpoch 45/50\n - 0s - loss: 32.3890 - val_loss: 41.2610\nEpoch 46/50\n - 0s - loss: 32.2433 - val_loss: 41.1654\nEpoch 47/50\n - 0s - loss: 32.2876 - val_loss: 41.4942\nEpoch 48/50\n - 0s - loss: 32.2981 - val_loss: 41.1992\nEpoch 49/50\n - 0s - loss: 32.3172 - val_loss: 41.2976\nEpoch 50/50\n - 0s - loss: 32.3073 - val_loss: 41.0783\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.3061 - val_loss: 41.1352\nEpoch 2/50\n - 0s - loss: 32.2886 - val_loss: 41.1292\nEpoch 3/50\n - 0s - loss: 32.3896 - val_loss: 41.2682\nEpoch 4/50\n - 0s - loss: 32.2760 - val_loss: 41.3064\nEpoch 5/50\n - 0s - loss: 32.2749 - val_loss: 41.0920\nEpoch 6/50\n - 0s - loss: 32.3059 - val_loss: 41.1583\nEpoch 7/50\n - 0s - loss: 32.3210 - val_loss: 41.1294\nEpoch 8/50\n - 0s - loss: 32.2336 - val_loss: 41.2622\nEpoch 9/50\n - 0s - loss: 32.3866 - val_loss: 41.1806\nEpoch 10/50\n - 0s - loss: 32.2711 - val_loss: 41.1592\nEpoch 11/50\n - 0s - loss: 32.4289 - val_loss: 40.9880\nEpoch 12/50\n - 0s - loss: 32.2759 - val_loss: 41.3381\nEpoch 13/50\n - 0s - loss: 32.2997 - val_loss: 41.4049\nEpoch 14/50\n - 0s - loss: 32.3339 - val_loss: 41.2171\nEpoch 15/50\n - 0s - loss: 32.3189 - val_loss: 41.1655\nEpoch 16/50\n - 0s - loss: 32.2732 - val_loss: 41.2721\nEpoch 17/50\n - 0s - loss: 32.3735 - val_loss: 41.3996\nEpoch 18/50\n - 0s - loss: 32.3335 - val_loss: 41.0941\nEpoch 19/50\n - 0s - loss: 32.2623 - val_loss: 41.2385\nEpoch 20/50\n - 0s - loss: 32.3336 - val_loss: 41.2140\nEpoch 21/50\n - 0s - loss: 32.4684 - val_loss: 41.1693\nEpoch 22/50\n - 0s - loss: 32.3586 - val_loss: 41.1805\nEpoch 23/50\n - 0s - loss: 32.2713 - val_loss: 41.2267\nEpoch 24/50\n - 0s - loss: 32.2568 - val_loss: 41.1189\nEpoch 25/50\n - 0s - loss: 32.2992 - val_loss: 41.3802\nEpoch 26/50\n - 0s - loss: 32.3352 - val_loss: 41.1339\nEpoch 27/50\n - 0s - loss: 32.2888 - val_loss: 41.4188\nEpoch 28/50\n - 0s - loss: 32.4065 - val_loss: 41.4446\nEpoch 29/50\n - 0s - loss: 32.3055 - val_loss: 41.3144\nEpoch 30/50\n - 0s - loss: 32.4106 - val_loss: 41.2111\nEpoch 31/50\n - 0s - loss: 32.2848 - val_loss: 41.0640\nEpoch 32/50\n - 0s - loss: 32.2674 - val_loss: 41.1445\nEpoch 33/50\n - 0s - loss: 32.2433 - val_loss: 41.2312\nEpoch 34/50\n - 0s - loss: 32.3064 - val_loss: 41.1904\nEpoch 35/50\n - 0s - loss: 32.3006 - val_loss: 41.2598\nEpoch 36/50\n - 0s - loss: 32.3203 - val_loss: 41.3313\nEpoch 37/50\n - 0s - loss: 32.2354 - val_loss: 41.6448\nEpoch 38/50\n - 0s - loss: 32.2760 - val_loss: 41.3286\nEpoch 39/50\n - 0s - loss: 32.2512 - val_loss: 40.9378\nEpoch 40/50\n - 0s - loss: 32.3076 - val_loss: 41.1666\nEpoch 41/50\n - 0s - loss: 32.2812 - val_loss: 41.1855\nEpoch 42/50\n - 0s - loss: 32.3378 - val_loss: 41.1624\nEpoch 43/50\n - 0s - loss: 32.5123 - val_loss: 41.1546\nEpoch 44/50\n - 0s - loss: 32.5827 - val_loss: 41.4971\nEpoch 45/50\n - 0s - loss: 32.3196 - val_loss: 40.9443\nEpoch 46/50\n - 0s - loss: 32.3236 - val_loss: 41.1425\nEpoch 47/50\n - 0s - loss: 32.2066 - val_loss: 41.2715\nEpoch 48/50\n - 0s - loss: 32.5271 - val_loss: 41.2748\nEpoch 49/50\n - 0s - loss: 32.2910 - val_loss: 41.3492\nEpoch 50/50\n - 0s - loss: 32.3888 - val_loss: 41.3307\nTrain on 721 samples, validate on 309 samples\nEpoch 1/50\n - 0s - loss: 32.2682 - val_loss: 41.2561\nEpoch 2/50\n - 0s - loss: 32.2625 - val_loss: 41.1147\nEpoch 3/50\n - 0s - loss: 32.3235 - val_loss: 41.2895\nEpoch 4/50\n - 0s - loss: 32.3369 - val_loss: 41.1868\nEpoch 5/50\n - 0s - loss: 32.3491 - val_loss: 41.5161\nEpoch 6/50\n - 0s - loss: 32.3629 - val_loss: 41.3077\nEpoch 7/50\n - 0s - loss: 32.2714 - val_loss: 41.1699\nEpoch 8/50\n - 0s - loss: 32.2741 - val_loss: 40.9848\nEpoch 9/50\n - 0s - loss: 32.3500 - val_loss: 41.6109\nEpoch 10/50\n - 0s - loss: 32.4409 - val_loss: 41.3890\nEpoch 11/50\n - 0s - loss: 32.3106 - val_loss: 41.0065\nEpoch 12/50\n - 0s - loss: 32.3078 - val_loss: 41.0835\nEpoch 13/50\n - 0s - loss: 32.3636 - val_loss: 41.3577\nEpoch 14/50\n - 0s - loss: 32.2781 - val_loss: 41.0339\nEpoch 15/50\n - 0s - loss: 32.2884 - val_loss: 41.4055\nEpoch 16/50\n - 0s - loss: 32.2716 - val_loss: 41.2809\nEpoch 17/50\n - 0s - loss: 32.3326 - val_loss: 41.1569\nEpoch 18/50\n - 0s - loss: 32.2916 - val_loss: 41.0061\nEpoch 19/50\n - 0s - loss: 32.2605 - val_loss: 41.4064\nEpoch 20/50\n - 0s - loss: 32.3133 - val_loss: 41.2080\nEpoch 21/50\n - 0s - loss: 32.2973 - val_loss: 41.0535\nEpoch 22/50\n - 0s - loss: 32.3539 - val_loss: 41.4581\nEpoch 23/50\n - 0s - loss: 32.2777 - val_loss: 41.0247\nEpoch 24/50\n - 0s - loss: 32.3746 - val_loss: 41.3979\nEpoch 25/50\n - 0s - loss: 32.4684 - val_loss: 40.8859\nEpoch 26/50\n - 0s - loss: 32.4640 - val_loss: 41.5764\nEpoch 27/50\n - 0s - loss: 32.2755 - val_loss: 41.0745\nEpoch 28/50\n - 0s - loss: 32.2781 - val_loss: 41.3150\nEpoch 29/50\n - 0s - loss: 32.2611 - val_loss: 41.3347\nEpoch 30/50\n - 0s - loss: 32.2436 - val_loss: 41.1257\nEpoch 31/50\n - 0s - loss: 32.2477 - val_loss: 41.1609\nEpoch 32/50\n - 0s - loss: 32.2746 - val_loss: 41.2543\nEpoch 33/50\n - 0s - loss: 32.2817 - val_loss: 41.4580\nEpoch 34/50\n - 0s - loss: 32.3147 - val_loss: 41.0489\nEpoch 35/50\n - 0s - loss: 32.2237 - val_loss: 41.3562\nEpoch 36/50\n - 0s - loss: 32.3274 - val_loss: 41.3131\nEpoch 37/50\n - 0s - loss: 32.3190 - val_loss: 41.3766\nEpoch 38/50\n - 0s - loss: 32.2478 - val_loss: 41.0847\nEpoch 39/50\n - 0s - loss: 32.3315 - val_loss: 41.3219\nEpoch 40/50\n - 0s - loss: 32.3841 - val_loss: 41.0830\nEpoch 41/50\n - 0s - loss: 32.3036 - val_loss: 41.1387\nEpoch 42/50\n - 0s - loss: 32.2890 - val_loss: 41.3690\nEpoch 43/50\n - 0s - loss: 32.2987 - val_loss: 41.3797\nEpoch 44/50\n - 0s - loss: 32.3161 - val_loss: 41.3707\nEpoch 45/50\n - 0s - loss: 32.2868 - val_loss: 41.0033\nEpoch 46/50\n - 0s - loss: 32.3093 - val_loss: 41.1438\nEpoch 47/50\n - 0s - loss: 32.2569 - val_loss: 41.1756\nEpoch 48/50\n - 0s - loss: 32.3174 - val_loss: 41.2533\nEpoch 49/50\n - 0s - loss: 32.3716 - val_loss: 41.4901\nEpoch 50/50\n - 0s - loss: 32.2303 - val_loss: 41.0245\n"
                }
            ],
            "source": "for i in range(50):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2)\n    scores.append(model.evaluate(X_test, y_test, verbose=0))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### MSE for each iteration\n\nDisplaying the MSE for each iteration"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "MSE for iteration # [0] :  306.67\nMSE for iteration # [1] :  147.25\nMSE for iteration # [2] :  117.24\nMSE for iteration # [3] :  96.03\nMSE for iteration # [4] :  80.94\nMSE for iteration # [5] :  70.20\nMSE for iteration # [6] :  63.96\nMSE for iteration # [7] :  59.60\nMSE for iteration # [8] :  56.78\nMSE for iteration # [9] :  54.32\nMSE for iteration # [10] :  52.33\nMSE for iteration # [11] :  50.35\nMSE for iteration # [12] :  48.94\nMSE for iteration # [13] :  47.86\nMSE for iteration # [14] :  46.77\nMSE for iteration # [15] :  45.81\nMSE for iteration # [16] :  44.73\nMSE for iteration # [17] :  44.16\nMSE for iteration # [18] :  43.75\nMSE for iteration # [19] :  43.28\nMSE for iteration # [20] :  42.92\nMSE for iteration # [21] :  43.08\nMSE for iteration # [22] :  42.74\nMSE for iteration # [23] :  42.73\nMSE for iteration # [24] :  42.29\nMSE for iteration # [25] :  42.05\nMSE for iteration # [26] :  42.17\nMSE for iteration # [27] :  42.24\nMSE for iteration # [28] :  41.98\nMSE for iteration # [29] :  41.78\nMSE for iteration # [30] :  41.72\nMSE for iteration # [31] :  42.12\nMSE for iteration # [32] :  41.82\nMSE for iteration # [33] :  41.68\nMSE for iteration # [34] :  41.39\nMSE for iteration # [35] :  41.59\nMSE for iteration # [36] :  41.65\nMSE for iteration # [37] :  41.58\nMSE for iteration # [38] :  41.61\nMSE for iteration # [39] :  41.57\nMSE for iteration # [40] :  41.80\nMSE for iteration # [41] :  41.63\nMSE for iteration # [42] :  41.61\nMSE for iteration # [43] :  41.35\nMSE for iteration # [44] :  41.36\nMSE for iteration # [45] :  41.21\nMSE for iteration # [46] :  41.42\nMSE for iteration # [47] :  41.27\nMSE for iteration # [48] :  41.46\nMSE for iteration # [49] :  41.33\n"
                }
            ],
            "source": "for i in range(50):\n    print('MSE for iteration #',[i],': ',\"%.2f\" % scores[i])"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The Mean of the MSE is:  48.35\n"
                }
            ],
            "source": "print ( 'The Mean of the MSE is: ',\"%.2f\" % np.mean(scores))"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The Standard Deviation of the MSE is:  29.76\n"
                }
            ],
            "source": "print ( 'The Standard Deviation of the MSE is: ',\"%.2f\" % np.std(scores))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}